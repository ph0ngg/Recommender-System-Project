{

  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPYteKFdNqO0",
        "outputId": "24929a0c-40f6-4451-c5a6-42bab9c5f22b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting skorch\n",
            "  Downloading skorch-0.15.0-py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.3/239.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from skorch) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from skorch) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from skorch) (1.11.4)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from skorch) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.10/dist-packages (from skorch) (4.66.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.0->skorch) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.0->skorch) (3.2.0)\n",
            "Installing collected packages: skorch\n",
            "Successfully installed skorch-0.15.0\n"
          ]
        }
      ],
      "source": [
        "!pip install skorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQZgn3JNuFXM",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import itertools\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import patsy\n",
        "import time\n",
        "\n",
        "import sklearn\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import make_scorer, mean_squared_error\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch import optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from skorch import NeuralNet\n",
        "from skorch.helper import predefined_split, SliceDataset\n",
        "from skorch.callbacks import BatchScoring, Checkpoint, EarlyStopping, EpochScoring, LRScheduler, TensorBoard, ProgressBar\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import summary\n",
        "#%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYaQB02n1Md_",
        "outputId": "8b93db5a-10ab-465c-a681-723c28418722"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "# Torch parameters\n",
        "identifier = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device = torch.device(identifier)\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvYWqFf2uG9j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9b0b6f1-a71d-43e1-bfe7-5e56eebd4078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-04 11:41:43--  http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
            "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
            "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4924029 (4.7M) [application/zip]\n",
            "Saving to: ‘ml-100k.zip’\n",
            "\n",
            "ml-100k.zip         100%[===================>]   4.70M  16.5MB/s    in 0.3s    \n",
            "\n",
            "2024-01-04 11:41:44 (16.5 MB/s) - ‘ml-100k.zip’ saved [4924029/4924029]\n",
            "\n",
            "Archive:  ml-100k.zip\n",
            "   creating: ml-100k/\n",
            "  inflating: ml-100k/allbut.pl       \n",
            "  inflating: ml-100k/mku.sh          \n",
            "  inflating: ml-100k/README          \n",
            "  inflating: ml-100k/u.data          \n",
            "  inflating: ml-100k/u.genre         \n",
            "  inflating: ml-100k/u.info          \n",
            "  inflating: ml-100k/u.item          \n",
            "  inflating: ml-100k/u.occupation    \n",
            "  inflating: ml-100k/u.user          \n",
            "  inflating: ml-100k/u1.base         \n",
            "  inflating: ml-100k/u1.test         \n",
            "  inflating: ml-100k/u2.base         \n",
            "  inflating: ml-100k/u2.test         \n",
            "  inflating: ml-100k/u3.base         \n",
            "  inflating: ml-100k/u3.test         \n",
            "  inflating: ml-100k/u4.base         \n",
            "  inflating: ml-100k/u4.test         \n",
            "  inflating: ml-100k/u5.base         \n",
            "  inflating: ml-100k/u5.test         \n",
            "  inflating: ml-100k/ua.base         \n",
            "  inflating: ml-100k/ua.test         \n",
            "  inflating: ml-100k/ub.base         \n",
            "  inflating: ml-100k/ub.test         \n"
          ]
        }
      ],
      "source": [
        "if not os.path.exists('ml-100k'):\n",
        "    !wget http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
        "    !unzip -o ml-100k.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j24pjXi6tMs6"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RrnACEq_bN3"
      },
      "outputs": [],
      "source": [
        "genre_cols = [\n",
        "    \"genre_unknown\", \"Action\", \"Adventure\", \"Animation\", \"Children\", \"Comedy\",\n",
        "    \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\", \"FilmNoir\", \"Horror\",\n",
        "    \"Musical\", \"Mystery\", \"Romance\", \"SciFi\", \"Thriller\", \"War\", \"Western\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIFR-8xl-kQ8"
      },
      "outputs": [],
      "source": [
        "class rsdataset(Dataset):\n",
        "    def __init__(self, usersfile, moviesfile, ratingsfile, nrows=None):\n",
        "\n",
        "        # Read files\n",
        "        self.movies = pd.read_csv(moviesfile, sep='|', names=['MovieID', 'Title', 'date', 'video_rl_date', 'link']+genre_cols, engine='python', encoding='latin-1')\n",
        "        self.users = pd.read_csv(usersfile, sep='|', names=['UserID', 'Age', 'Gender', 'Occupation', 'Zipcode'], engine='python', encoding='latin-1')\n",
        "        self.ratings = pd.read_csv(ratingsfile, sep='\\t', names=['UserID', 'MovieID', 'Rating', 'Timestamp'], engine='python', nrows=nrows, encoding='latin-1')\n",
        "\n",
        "        df2 = self.movies[genre_cols]\n",
        "        df2['Genre'] = df2.apply(lambda row: ', '.join(row.index[row == 1]), axis=1)\n",
        "        self.movies['Genre'] = df2['Genre']\n",
        "        self.movies = self.movies.drop(genre_cols, axis = 1)\n",
        "        # self.movies['Genre'] = self.movies['Genre'].map(genre_dict)\n",
        "        bins = [0, 18, 25, 35, 45, 50, 56, 100]\n",
        "        labels = [1, 18, 25, 35, 45, 50, 56]\n",
        "        self.users['Age'] = pd.cut(self.users['Age'], bins=bins, labels=labels, right=False)\n",
        "        assert self.users['UserID'].nunique() >= self.ratings['UserID'].nunique(), 'UserID with unknown information'\n",
        "        assert self.movies['MovieID'].nunique() >= self.ratings['MovieID'].nunique(), 'Movies with unknown information'\n",
        "\n",
        "        self.users_emb_columns = []\n",
        "        self.users_ohe_columns = []\n",
        "        self.movies_emb_columns = []\n",
        "        self.movies_ohe_columns = []\n",
        "        self.interact_columns = []\n",
        "\n",
        "        self.movies = self.movies.drop(['date', 'video_rl_date', 'link'], axis= 1)\n",
        "        self.nusers = self.ratings['UserID'].nunique()\n",
        "        self.nmovies = self.ratings['MovieID'].nunique()\n",
        "\n",
        "        self.y_range = (self.ratings['Rating'].min(), self.ratings['Rating'].max())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        return (((self.users_emb[idx])),\n",
        "                ((self.users_ohe[idx])),\n",
        "                ((self.movies_emb[idx])),\n",
        "                ((self.movies_ohe[idx])),\n",
        "                ((self.interact[idx]))), (self.y[idx])\n",
        "\n",
        "    def to_tensor(self):\n",
        "        self.users_emb = torch.from_numpy(self.ratings[self.users_emb_columns].values)\n",
        "        self.users_ohe = torch.tensor(self.ratings[self.users_ohe_columns].values, dtype=torch.float)\n",
        "        self.movies_emb = torch.from_numpy(self.ratings[self.movies_emb_columns].values)\n",
        "        self.movies_ohe = torch.tensor(self.ratings[self.movies_ohe_columns].values, dtype=torch.float)\n",
        "        self.interact = torch.from_numpy(self.ratings[self.interact_columns].values)\n",
        "        self.y = torch.tensor(self.y.values, dtype=torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uINDlkiMB_Fo",
        "outputId": "82849101-5922-42cb-e87c-fe737bbcb7cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-f5a438cfccf0>:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df2['Genre'] = df2.apply(lambda row: ', '.join(row.index[row == 1]), axis=1)\n"
          ]
        }
      ],
      "source": [
        "train = rsdataset('ml-100k/u.user', 'ml-100k/u.item', 'ml-100k/u.data', nrows=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8ivFYVSM3nr"
      },
      "source": [
        "### Preprocessing of dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_w1QQER37El"
      },
      "outputs": [],
      "source": [
        "train.ratings = train.ratings.merge(train.movies, left_on='MovieID', right_on='MovieID')\n",
        "train.movies = train.ratings[train.movies.columns]\n",
        "\n",
        "train.ratings = train.ratings.merge(train.users, left_on='UserID', right_on='UserID')\n",
        "train.users = train.ratings[train.users.columns]\n",
        "\n",
        "train.y = train.ratings['Rating']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlQhqnXOYcmp"
      },
      "outputs": [],
      "source": [
        "# Label Encode users\n",
        "columns = ['UserID', 'Gender', 'Age', 'Occupation']\n",
        "train.ratings[columns] = train.ratings[columns].apply(preprocessing.LabelEncoder().fit_transform)\n",
        "train.users_emb_columns = train.users_emb_columns + columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZ3jSa_dRMLA"
      },
      "outputs": [],
      "source": [
        "# Label Encode movies\n",
        "columns = ['MovieID']\n",
        "train.ratings[columns] = train.ratings[columns].apply(preprocessing.LabelEncoder().fit_transform)\n",
        "train.movies_emb_columns = train.movies_emb_columns + columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2XcLbiz9lql"
      },
      "outputs": [],
      "source": [
        "# One Hot Encode users\n",
        "columns = ['Gender', 'Age', 'Occupation']\n",
        "ohe = preprocessing.OneHotEncoder(categories='auto', sparse=False, dtype='uint8')\n",
        "ohe.fit(train.ratings[columns])\n",
        "train.ratings = pd.concat([train.ratings, pd.DataFrame(data=ohe.transform(train.ratings[columns]), columns=ohe.get_feature_names_out(columns))], axis=1)\n",
        "train.users_ohe_columns = ohe.get_feature_names_out(columns)\n",
        "\n",
        "assert train.ratings[train.users_ohe_columns].max().max()<=1, 'Error with ohe columns'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfVq6sd5NqiO"
      },
      "outputs": [],
      "source": [
        "# One Hot Encode movies\n",
        "genres = [\"genre_unknown\", \"Action\", \"Adventure\", \"Animation\", \"Children\", \"Comedy\",\n",
        "    \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\", \"FilmNoir\", \"Horror\",\n",
        "    \"Musical\", \"Mystery\", \"Romance\", \"SciFi\", \"Thriller\", \"War\", \"Western\"]\n",
        "\n",
        "for genre in genres:\n",
        "    genre = genre.replace('-', '')\n",
        "    column = str(genre)\n",
        "    train.ratings[column] = train.ratings['Genre'].apply(lambda x: 1 if genre in x else 0)\n",
        "    train.movies_ohe_columns.append(column)\n",
        "\n",
        "assert train.ratings[train.movies_ohe_columns].max().max()<=1, 'Error with ohe columns'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTH7wNbZp_jA"
      },
      "outputs": [],
      "source": [
        "int_genres_gender = \"\"\n",
        "for genre in train.movies_ohe_columns:\n",
        "    int_genres_gender = int_genres_gender + '+' +genre + ':Gender'\n",
        "\n",
        "int_genres_age = \"\"\n",
        "for genre in train.movies_ohe_columns:\n",
        "    int_genres_age = int_genres_age + '+' + genre + ':Age'\n",
        "\n",
        "interact = patsy.dmatrix(\"0 + Gender:Age + Gender:Occupation + Age:Occupation\"+int_genres_gender+int_genres_age, data=train.ratings.astype('object'), return_type='dataframe').astype('int8')\n",
        "interact = interact.astype('uint8')\n",
        "train.ratings = pd.concat([train.ratings, interact], axis=1)\n",
        "train.interact_columns = interact.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "E92lXDy_Hpf2",
        "outputId": "7e169314-91ee-4284-e22e-4f5d21548cf0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Gender[0]:Age[0]  Gender[1]:Age[0]  Gender[0]:Age[1]  Gender[1]:Age[1]  \\\n",
              "0                     0                 0                 0                 0   \n",
              "1                     0                 0                 0                 0   \n",
              "2                     0                 0                 0                 0   \n",
              "3                     0                 0                 0                 0   \n",
              "4                     0                 0                 0                 0   \n",
              "...                 ...               ...               ...               ...   \n",
              "99995                 0                 0                 0                 0   \n",
              "99996                 0                 0                 0                 0   \n",
              "99997                 0                 0                 0                 0   \n",
              "99998                 0                 0                 0                 0   \n",
              "99999                 0                 0                 0                 0   \n",
              "\n",
              "       Gender[0]:Age[2]  Gender[1]:Age[2]  Gender[0]:Age[3]  Gender[1]:Age[3]  \\\n",
              "0                     0                 0                 0                 0   \n",
              "1                     0                 0                 0                 0   \n",
              "2                     0                 0                 0                 0   \n",
              "3                     0                 0                 0                 0   \n",
              "4                     0                 0                 0                 0   \n",
              "...                 ...               ...               ...               ...   \n",
              "99995                 0                 0                 0                 0   \n",
              "99996                 0                 0                 0                 0   \n",
              "99997                 0                 0                 0                 0   \n",
              "99998                 0                 0                 0                 0   \n",
              "99999                 0                 0                 0                 0   \n",
              "\n",
              "       Gender[0]:Age[4]  Gender[1]:Age[4]  ...  War[T.1]:Age[T.3]  \\\n",
              "0                     0                 1  ...                  0   \n",
              "1                     0                 1  ...                  0   \n",
              "2                     0                 1  ...                  0   \n",
              "3                     0                 1  ...                  0   \n",
              "4                     0                 1  ...                  0   \n",
              "...                 ...               ...  ...                ...   \n",
              "99995                 1                 0  ...                  0   \n",
              "99996                 1                 0  ...                  0   \n",
              "99997                 1                 0  ...                  0   \n",
              "99998                 1                 0  ...                  0   \n",
              "99999                 1                 0  ...                  0   \n",
              "\n",
              "       War[T.1]:Age[T.4]  War[T.1]:Age[T.5]  War[T.1]:Age[T.6]  \\\n",
              "0                      0                  0                  0   \n",
              "1                      0                  0                  0   \n",
              "2                      0                  0                  0   \n",
              "3                      0                  0                  0   \n",
              "4                      0                  0                  0   \n",
              "...                  ...                ...                ...   \n",
              "99995                  0                  0                  0   \n",
              "99996                  1                  0                  0   \n",
              "99997                  0                  0                  0   \n",
              "99998                  0                  0                  0   \n",
              "99999                  0                  0                  0   \n",
              "\n",
              "       Western[T.1]:Age[T.1]  Western[T.1]:Age[T.2]  Western[T.1]:Age[T.3]  \\\n",
              "0                          0                      0                      0   \n",
              "1                          0                      0                      0   \n",
              "2                          0                      0                      0   \n",
              "3                          0                      0                      0   \n",
              "4                          0                      0                      0   \n",
              "...                      ...                    ...                    ...   \n",
              "99995                      0                      0                      0   \n",
              "99996                      0                      0                      0   \n",
              "99997                      0                      0                      0   \n",
              "99998                      0                      0                      0   \n",
              "99999                      0                      0                      0   \n",
              "\n",
              "       Western[T.1]:Age[T.4]  Western[T.1]:Age[T.5]  Western[T.1]:Age[T.6]  \n",
              "0                          0                      0                      0  \n",
              "1                          0                      0                      0  \n",
              "2                          0                      0                      0  \n",
              "3                          0                      0                      0  \n",
              "4                          0                      0                      0  \n",
              "...                      ...                    ...                    ...  \n",
              "99995                      0                      0                      0  \n",
              "99996                      0                      0                      0  \n",
              "99997                      0                      0                      0  \n",
              "99998                      0                      0                      0  \n",
              "99999                      0                      0                      0  \n",
              "\n",
              "[100000 rows x 326 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-885dd3ec-f2b2-4d82-b05d-bc27a1213b85\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Gender[0]:Age[0]</th>\n",
              "      <th>Gender[1]:Age[0]</th>\n",
              "      <th>Gender[0]:Age[1]</th>\n",
              "      <th>Gender[1]:Age[1]</th>\n",
              "      <th>Gender[0]:Age[2]</th>\n",
              "      <th>Gender[1]:Age[2]</th>\n",
              "      <th>Gender[0]:Age[3]</th>\n",
              "      <th>Gender[1]:Age[3]</th>\n",
              "      <th>Gender[0]:Age[4]</th>\n",
              "      <th>Gender[1]:Age[4]</th>\n",
              "      <th>...</th>\n",
              "      <th>War[T.1]:Age[T.3]</th>\n",
              "      <th>War[T.1]:Age[T.4]</th>\n",
              "      <th>War[T.1]:Age[T.5]</th>\n",
              "      <th>War[T.1]:Age[T.6]</th>\n",
              "      <th>Western[T.1]:Age[T.1]</th>\n",
              "      <th>Western[T.1]:Age[T.2]</th>\n",
              "      <th>Western[T.1]:Age[T.3]</th>\n",
              "      <th>Western[T.1]:Age[T.4]</th>\n",
              "      <th>Western[T.1]:Age[T.5]</th>\n",
              "      <th>Western[T.1]:Age[T.6]</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99995</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99996</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99997</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99998</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99999</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100000 rows × 326 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-885dd3ec-f2b2-4d82-b05d-bc27a1213b85')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-885dd3ec-f2b2-4d82-b05d-bc27a1213b85 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-885dd3ec-f2b2-4d82-b05d-bc27a1213b85');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fd8b031c-9f32-4d11-8359-3b5447439e11\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fd8b031c-9f32-4d11-8359-3b5447439e11')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fd8b031c-9f32-4d11-8359-3b5447439e11 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_33bf3392-015d-4142-a085-84ab5bb1957c\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('interact')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_33bf3392-015d-4142-a085-84ab5bb1957c button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('interact');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "interact\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRSGsdEmMlPE"
      },
      "outputs": [],
      "source": [
        "# Drop unused columns\n",
        "train.movies.drop(['Title', 'Genre'], inplace=True, axis=1)\n",
        "train.ratings.drop(['Title', 'Genre', 'Zipcode'], inplace=True, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPTkxLKQMWhI"
      },
      "outputs": [],
      "source": [
        "train.to_tensor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm1qo9It3zsb"
      },
      "source": [
        "### DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNXPv9DAmHBB"
      },
      "outputs": [],
      "source": [
        "# Split\n",
        "train_size = int(0.8 * len(train))\n",
        "test_size = len(train) - train_size\n",
        "train_dataset, valid_dataset = torch.utils.data.random_split(train, [train_size, test_size])\n",
        "\n",
        "# Create dataloaders\n",
        "dataloaders = {}\n",
        "dataloaders['train'] = torch.utils.data.DataLoader(train_dataset, batch_size=4096, shuffle=True)\n",
        "dataloaders['valid'] = torch.utils.data.DataLoader(valid_dataset, batch_size=4096, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa-KNh5qkUh8"
      },
      "source": [
        "### Define Pytorch models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daekeS2zju4B",
        "outputId": "85ac6848-3d17-40b1-a821-cbe3de48fb6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deepnwide(\n",
            "  (emb_UserID): Embedding(943, 60)\n",
            "  (emb_Gender): Embedding(2, 60)\n",
            "  (emb_Age): Embedding(7, 60)\n",
            "  (emb_Occupation): Embedding(21, 60)\n",
            "  (emb_MovieID): Embedding(1682, 60)\n",
            "  (h1): Linear(in_features=300, out_features=100, bias=True)\n",
            "  (h2): Linear(in_features=100, out_features=100, bias=True)\n",
            "  (h3): Linear(in_features=100, out_features=100, bias=True)\n",
            "  (dropout1): Dropout(p=0.5, inplace=False)\n",
            "  (dropout2): Dropout(p=0.5, inplace=False)\n",
            "  (dropout3): Dropout(p=0.5, inplace=False)\n",
            "  (last_layer): Linear(in_features=445, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class deepnwide(nn.Module):\n",
        "\n",
        "    def __init__(self, users_emb, movies_emb, users_ohe, movies_ohe, interact, size_emb, y_range, dropout, linear_size= 500):\n",
        "        super().__init__()\n",
        "\n",
        "        self.name = 'deepnwide'\n",
        "        self.y_range = y_range\n",
        "\n",
        "        # wide part\n",
        "\n",
        "        # deep\n",
        "        self.emb_UserID = nn.Embedding(len(torch.unique(users_emb[:, 0])), size_emb)\n",
        "        self.emb_UserID.weight.data.uniform_(-.01, .01)\n",
        "        self.emb_Gender = nn.Embedding(len(torch.unique(users_emb[:, 1])), size_emb)\n",
        "        self.emb_Gender.weight.data.uniform_(-.01, .01)\n",
        "        self.emb_Age = nn.Embedding(len(torch.unique(users_emb[:, 2])), size_emb)\n",
        "        self.emb_Age.weight.data.uniform_(-.01, .01)\n",
        "        self.emb_Occupation = nn.Embedding(len(torch.unique(users_emb[:, 3])), size_emb)\n",
        "        self.emb_Occupation.weight.data.uniform_(-.01, .01)\n",
        "        self.emb_MovieID = nn.Embedding(len(torch.unique(movies_emb[:, 0])), size_emb)\n",
        "        self.emb_MovieID.weight.data.uniform_(-.01, .01)\n",
        "\n",
        "        # hidden layers\n",
        "        self.h1 = nn.Linear(5 * size_emb, linear_size)\n",
        "        self.h2 = nn.Linear(linear_size, linear_size)\n",
        "        self.h3 = nn.Linear(linear_size, linear_size)\n",
        "\n",
        "        # Dropout layers\n",
        "        self.dropout1 = nn.Dropout(p=dropout)\n",
        "        self.dropout2 = nn.Dropout(p=dropout)\n",
        "        self.dropout3 = nn.Dropout(p=dropout)\n",
        "\n",
        "        # final dense layer\n",
        "        self.last_layer = nn.Linear((interact.shape[1]) + (movies_ohe.shape[1]) + (linear_size), 1)\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Assign data\n",
        "        user_emb = X[0]\n",
        "        user_ohe = X[1]\n",
        "        movie_emb = X[2]\n",
        "        movie_ohe = X[3]\n",
        "        interact = X[4]\n",
        "\n",
        "        UserID = user_emb[:, 0]\n",
        "        Gender = user_emb[:, 1]\n",
        "        Age = user_emb[:, 2]\n",
        "        Occupation = user_emb[:, 3]\n",
        "        MovieID = movie_emb[:, 0]\n",
        "\n",
        "        UserID = self.emb_UserID(UserID)\n",
        "        Gender = self.emb_Gender(Gender)\n",
        "        Age = self.emb_Age(Age)\n",
        "        Occupation = self.emb_Occupation(Occupation)\n",
        "        MovieID = self.emb_MovieID(MovieID)\n",
        "\n",
        "        emb = torch.cat([UserID,\n",
        "                         Age,\n",
        "                         Gender,\n",
        "                         Occupation,\n",
        "                         MovieID],\n",
        "                         dim=1)\n",
        "\n",
        "        emb = F.relu(self.dropout1(self.h1(emb)))\n",
        "        emb = F.relu(self.dropout2(self.h2(emb)))\n",
        "        emb = F.relu(self.dropout3(self.h3(emb)))\n",
        "\n",
        "        result = self.last_layer(torch.cat([interact.float(), movie_ohe.float(), emb.float()], dim=1))\n",
        "\n",
        "        return (torch.sigmoid(result) * (self.y_range[1]-self.y_range[0]) + self.y_range[0]).squeeze()\n",
        "\n",
        "\n",
        "model = deepnwide(train.users_emb, train.movies_emb, train.users_ohe, train.movies_ohe, train.interact, 60, train.y_range, 0.5, 100)\n",
        "model.to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MftgRHZB3rFd",
        "outputId": "ba1dddc0-f4d9-4d17-aba3-f0b4f3c3d8de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "twoembeds(\n",
            "  (emb_UserID): Embedding(943, 15)\n",
            "  (emb_MovieID): Embedding(1682, 15)\n",
            "  (emb_UserID_b): Embedding(943, 1)\n",
            "  (emb_MovieID_b): Embedding(1682, 1)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class twoembeds(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, size_emb, y_range):\n",
        "        super().__init__()\n",
        "\n",
        "        # set name of model\n",
        "        self.name = 'twoembeds'\n",
        "        self.y_range = y_range\n",
        "\n",
        "        # User and movie embeddings\n",
        "        self.emb_UserID = nn.Embedding(train.nusers, size_emb)\n",
        "        self.emb_MovieID = nn.Embedding(train.nmovies, size_emb)\n",
        "        self.emb_UserID.weight.data.uniform_(-.01, .01)\n",
        "        self.emb_MovieID.weight.data.uniform_(-.01, .01)\n",
        "\n",
        "        # User and movie embeddings weights\n",
        "        self.emb_UserID_b = nn.Embedding(train.nusers, 1)\n",
        "        self.emb_MovieID_b = nn.Embedding(train.nmovies, 1)\n",
        "        self.emb_UserID_b.weight.data.uniform_(-.01, .01)\n",
        "        self.emb_MovieID_b.weight.data.uniform_(-.01, .01)\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "        user_emb = X[0]\n",
        "        user_ohe = X[1]\n",
        "        movie_emb = X[2]\n",
        "        movie_ohe = X[3]\n",
        "        interact = X[4]\n",
        "\n",
        "        UserID = user_emb[:, 0]\n",
        "        MovieID = movie_emb[:, 0]\n",
        "\n",
        "        user_emb = self.emb_UserID(UserID)\n",
        "        movie_emb = self.emb_MovieID(MovieID)\n",
        "\n",
        "        mult = (user_emb * movie_emb).sum(1)\n",
        "\n",
        "        # add bias\n",
        "        multb = mult + self.emb_UserID_b(UserID).squeeze() + self.emb_MovieID_b(MovieID).squeeze()\n",
        "\n",
        "        multb = multb.float()\n",
        "\n",
        "        return (torch.sigmoid(multb) * (self.y_range[1]-self.y_range[0]) + self.y_range[0]).squeeze()\n",
        "\n",
        "        return multb\n",
        "\n",
        "\n",
        "model = twoembeds(15, train.y_range)\n",
        "model.to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lzfGXgaNgJE",
        "outputId": "a783db3f-9eda-46d4-aed3-5eb02101b4bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ncf(\n",
            "  (gmf_embuserid): Embedding(943, 60)\n",
            "  (gmf_embgender): Embedding(2, 60)\n",
            "  (gmf_embage): Embedding(7, 60)\n",
            "  (gmf_embocc): Embedding(21, 60)\n",
            "  (gmf_embmovieid): Embedding(1682, 221)\n",
            "  (mlp_embuserid): Embedding(943, 60)\n",
            "  (mlp_embgender): Embedding(2, 60)\n",
            "  (mlp_embage): Embedding(7, 60)\n",
            "  (mlp_embocc): Embedding(21, 60)\n",
            "  (mlp_embmovieid): Embedding(1682, 60)\n",
            "  (h1): Linear(in_features=319, out_features=200, bias=True)\n",
            "  (h2): Linear(in_features=200, out_features=100, bias=True)\n",
            "  (dropout1): Dropout(p=0.5, inplace=False)\n",
            "  (dropout2): Dropout(p=0.5, inplace=False)\n",
            "  (last_layer): Linear(in_features=340, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class ncf(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, users_emb, movies_emb, users_ohe, movies_ohe, interact, size_emb, dropout, linear_size, y_range):\n",
        "        super().__init__()\n",
        "\n",
        "        # set name of model\n",
        "        self.name = 'ncf'\n",
        "        self.y_range = y_range\n",
        "\n",
        "        ### GMF part\n",
        "        # user embeddings\n",
        "        self.gmf_embuserid = nn.Embedding(len(torch.unique(users_emb[:, 0])), size_emb)\n",
        "        self.gmf_embuserid.weight.data.uniform_(-.01, .01)\n",
        "        self.gmf_embgender = nn.Embedding(len(torch.unique(users_emb[:, 1])), size_emb)\n",
        "        self.gmf_embgender.weight.data.uniform_(-.01, .01)\n",
        "        self.gmf_embage = nn.Embedding(len(torch.unique(users_emb[:, 2])), size_emb)\n",
        "        self.gmf_embage.weight.data.uniform_(-.01, .01)\n",
        "        self.gmf_embocc = nn.Embedding(len(torch.unique(users_emb[:, 3])), size_emb)\n",
        "        self.gmf_embocc.weight.data.uniform_(-.01, .01)\n",
        "        # movie embeddings\n",
        "        self.gmf_embmovieid = nn.Embedding(len(torch.unique(movies_emb[:, 0])), size_emb*4-len(train.movies_ohe_columns))\n",
        "        self.gmf_embmovieid.weight.data.uniform_(-.01, .01)\n",
        "\n",
        "\n",
        "        ### MLP part\n",
        "        # user embeddings\n",
        "        self.mlp_embuserid = nn.Embedding(len(torch.unique(users_emb[:, 0])), size_emb)\n",
        "        self.mlp_embuserid.weight.data.uniform_(-.01, .01)\n",
        "        self.mlp_embgender = nn.Embedding(len(torch.unique(users_emb[:, 1])), size_emb)\n",
        "        self.mlp_embgender.weight.data.uniform_(-.01, .01)\n",
        "        self.mlp_embage = nn.Embedding(len(torch.unique(users_emb[:, 2])), size_emb)\n",
        "        self.mlp_embage.weight.data.uniform_(-.01, .01)\n",
        "        self.mlp_embocc = nn.Embedding(len(torch.unique(users_emb[:, 3])), size_emb)\n",
        "        self.mlp_embocc.weight.data.uniform_(-.01, .01)\n",
        "        # movie embeddings\n",
        "        self.mlp_embmovieid = nn.Embedding(len(torch.unique(movies_emb[:, 0])), size_emb)\n",
        "        self.mlp_embmovieid.weight.data.uniform_(-.01, .01)\n",
        "        # hidden layers\n",
        "        self.h1 = nn.Linear(5*size_emb+len(train.movies_ohe_columns), linear_size)\n",
        "        self.h2 = nn.Linear(linear_size, int(linear_size/2))\n",
        "        #self.h3 = nn.Linear(linear_size, linear_size)\n",
        "        # Dropout layers\n",
        "        self.dropout1 = nn.Dropout(p=dropout)\n",
        "        self.dropout2 = nn.Dropout(p=dropout)\n",
        "        #self.dropout3 = nn.Dropout(p=dropout)\n",
        "\n",
        "        # final dense layer\n",
        "        self.last_layer = nn.Linear(size_emb*4+int(linear_size/2), 1)\n",
        "\n",
        "    def forward(self, X):\n",
        "        user_emb = X[0]\n",
        "        user_ohe = X[1]\n",
        "        movie_emb = X[2]\n",
        "        movie_ohe = X[3]\n",
        "        interact = X[4]\n",
        "\n",
        "        UserID = user_emb[:, 0]\n",
        "        Gender = user_emb[:, 1]\n",
        "        Age = user_emb[:, 2]\n",
        "        Occupation = user_emb[:, 3]\n",
        "        MovieID = movie_emb[:, 0]\n",
        "\n",
        "        # GMF part\n",
        "        gmf_embuserid = self.gmf_embuserid(UserID)\n",
        "        gmf_embgender = self.gmf_embgender(Gender)\n",
        "        gmf_embage = self.gmf_embage(Age)\n",
        "        gmf_embocc = self.gmf_embocc(Occupation)\n",
        "        gmf_embmovieid = self.gmf_embmovieid(MovieID)\n",
        "\n",
        "        gmf_user_vector = torch.cat([gmf_embuserid,\n",
        "                                    gmf_embgender,\n",
        "                                    gmf_embage,\n",
        "                                    gmf_embocc],\n",
        "                                    dim=1)\n",
        "\n",
        "        gmf_movie_vector = torch.cat([gmf_embmovieid, movie_ohe], 1)\n",
        "\n",
        "        gmf_vector = (gmf_user_vector * gmf_movie_vector)\n",
        "\n",
        "\n",
        "        # MLP part\n",
        "        mlp_embuserid = self.mlp_embuserid(UserID)\n",
        "        mlp_embgender = self.mlp_embgender(Gender)\n",
        "        mlp_embage = self.mlp_embage(Age)\n",
        "        mlp_embocc = self.mlp_embocc(Occupation)\n",
        "        mlp_movieid = self.mlp_embmovieid(MovieID)\n",
        "\n",
        "        mlp_vector = torch.cat([mlp_embuserid,\n",
        "                                mlp_embgender,\n",
        "                                mlp_embage,\n",
        "                                mlp_embocc,\n",
        "                                mlp_movieid,\n",
        "                                movie_ohe],\n",
        "                                dim=1)\n",
        "        mlp_vector = F.relu(self.dropout1(self.h1(mlp_vector)))\n",
        "        mlp_vector = F.relu(self.dropout2(self.h2(mlp_vector)))\n",
        "        #mlp_vector = F.relu(self.dropout3(self.h3(mlp_vector)))\n",
        "\n",
        "        # Fusion\n",
        "        result = torch.cat([gmf_vector, mlp_vector], dim=1)\n",
        "        result = self.last_layer(result)\n",
        "\n",
        "        #return (torch.sigmoid(result) * (5-1) + 1).squeeze\n",
        "        return (torch.sigmoid(result) * (self.y_range[1]-self.y_range[0]) + self.y_range[0]).squeeze()\n",
        "\n",
        "\n",
        "model = ncf(train.users_emb, train.movies_emb, train.users_ohe, train.movies_ohe, train.interact, 60, 0.5, 200, train.y_range)\n",
        "model.to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rBEpoBPd4w5"
      },
      "source": [
        "### Skorch callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9XsVtTYOqGq"
      },
      "outputs": [],
      "source": [
        "# Earlystopping callback\n",
        "earlystopping = EarlyStopping(monitor='valid_loss', patience=10, threshold=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ockpmqrz9KYv"
      },
      "outputs": [],
      "source": [
        "# RMSE callback\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "\n",
        "def rmseloss(y_true, y_pred):\n",
        "    #return f1_score(y_true, y_pred)\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    y_true_binary = (y_true >= 4).astype(int)\n",
        "    y_pred_binary = (y_pred >= 4).astype(int)\n",
        "\n",
        "    # Calculate Precision and Recall\n",
        "    precision_scoree = precision_score(y_true_binary, y_pred_binary)\n",
        "    return precision_scoree\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    y_true_binary = (y_true >= 4).astype(int)\n",
        "    y_pred_binary = (y_pred >= 4).astype(int)\n",
        "\n",
        "    # Calculate Precision and Recall\n",
        "    recall_scoree = recall_score(y_true_binary, y_pred_binary)\n",
        "    return recall_scoree\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    y_true_binary = (y_true >= 4).astype(int)\n",
        "    y_pred_binary = (y_pred >= 4).astype(int)\n",
        "\n",
        "    # Calculate F1 score\n",
        "    f1_scoree = f1_score(y_true_binary, y_pred_binary)\n",
        "    return f1_scoree\n",
        "\n",
        "rmse_scorer = make_scorer(rmseloss)\n",
        "precision_scorer = make_scorer(precision)\n",
        "recall_scorer = make_scorer(recall)\n",
        "f1_scorer = make_scorer(f1)\n",
        "\n",
        "epoch_rmse = EpochScoring(rmse_scorer, name='rmse_score', lower_is_better=True)\n",
        "epoch_precision = EpochScoring(precision_scorer, name='precision', lower_is_better= False)\n",
        "epoch_recall = EpochScoring(recall_scorer, name='recall', lower_is_better= False)\n",
        "epoch_f1 = EpochScoring(f1_scorer, name='f1', lower_is_better= False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p62gAk8FgaZS"
      },
      "outputs": [],
      "source": [
        "# Checkpoint callback\n",
        "checkpoint = Checkpoint(monitor='rmse_score_best', f_params='params.pt', f_optimizer='optimizer.pt', f_history='history.json', f_pickle='model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knAy1XQYjgze"
      },
      "outputs": [],
      "source": [
        "# Learning rate scheduler callback\n",
        "lr_scheduler = LRScheduler(policy=\"StepLR\", step_size=7, gamma=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeHN1BMLNgJF"
      },
      "source": [
        "### Neural Collaborative Filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WXJpYU6NgJF"
      },
      "source": [
        "#### Manually specify hyperparamers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7Lz3NnTNgJF"
      },
      "outputs": [],
      "source": [
        "#86.57\n",
        "ncfnet = NeuralNet(\n",
        "    ncf,\n",
        "    module__users_emb=train.users_emb,\n",
        "    module__movies_emb=train.movies_emb,\n",
        "    module__users_ohe=train.users_ohe,\n",
        "    module__movies_ohe=train.movies_ohe,\n",
        "    module__interact=train.interact,\n",
        "    module__size_emb=500,\n",
        "    module__dropout=0.5,\n",
        "    module__linear_size=1200,\n",
        "    module__y_range=train.y_range,#### Manually specify hyperparamers\n",
        "    max_epochs=50,\n",
        "    lr=0.001,\n",
        "    optimizer=torch.optim.Adam,\n",
        "    criterion=torch.nn.MSELoss,\n",
        "    device=device,\n",
        "    iterator_train__batch_size=1024,\n",
        "    iterator_train__num_workers=0,\n",
        "    iterator_train__shuffle=True,\n",
        "    iterator_valid__batch_size=1024,\n",
        "    train_split=predefined_split(valid_dataset),\n",
        "    callbacks=[\n",
        "               earlystopping,\n",
        "               epoch_rmse,\n",
        "               epoch_precision,\n",
        "               epoch_recall,\n",
        "               epoch_f1,\n",
        "               checkpoint,\n",
        "               lr_scheduler,\n",
        "               #TensorBoard(writer),\n",
        "               #progressbar\n",
        "               ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHSbndsbNgJG",
        "outputId": "811b0b20-4ebd-4178-906b-82d9c27b5afc",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss    cp      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ----  ------  ------\n",
            "      1  \u001b[36m0.4843\u001b[0m       \u001b[32m0.8280\u001b[0m    \u001b[35m0.3423\u001b[0m        \u001b[31m0.9446\u001b[0m        \u001b[94m1.0215\u001b[0m        \u001b[36m0.8923\u001b[0m     +  0.0010  3.7076\n",
            "      2  0.4783       \u001b[32m0.8512\u001b[0m    0.3326        \u001b[31m0.9252\u001b[0m        \u001b[94m0.8626\u001b[0m        \u001b[36m0.8559\u001b[0m     +  0.0010  4.0770\n",
            "      3  \u001b[36m0.5348\u001b[0m       0.8506    \u001b[35m0.3900\u001b[0m        \u001b[31m0.9142\u001b[0m        \u001b[94m0.7350\u001b[0m        \u001b[36m0.8357\u001b[0m     +  0.0010  4.0622\n",
            "      4  \u001b[36m0.5927\u001b[0m       0.8345    \u001b[35m0.4595\u001b[0m        \u001b[31m0.9130\u001b[0m        \u001b[94m0.5401\u001b[0m        \u001b[36m0.8336\u001b[0m     +  0.0010  4.2478\n",
            "      5  \u001b[36m0.5978\u001b[0m       0.8247    \u001b[35m0.4688\u001b[0m        0.9233        \u001b[94m0.3581\u001b[0m        0.8525        0.0010  4.1074\n",
            "      6  \u001b[36m0.6014\u001b[0m       0.8183    \u001b[35m0.4754\u001b[0m        0.9312        \u001b[94m0.2277\u001b[0m        0.8672        0.0010  4.0494\n",
            "      7  \u001b[36m0.6254\u001b[0m       0.8030    \u001b[35m0.5121\u001b[0m        0.9401        \u001b[94m0.1508\u001b[0m        0.8838        0.0010  3.7129\n",
            "      8  0.6015       0.8075    0.4792        0.9410        \u001b[94m0.1064\u001b[0m        0.8855        0.0001  3.5334\n",
            "      9  0.6019       0.8063    0.4802        0.9417        \u001b[94m0.1020\u001b[0m        0.8868        0.0001  3.4865\n",
            "     10  0.6038       0.8046    0.4833        0.9426        \u001b[94m0.0983\u001b[0m        0.8886        0.0001  4.3009\n",
            "     11  0.6071       0.8043    0.4876        0.9435        \u001b[94m0.0949\u001b[0m        0.8901        0.0001  3.5100\n",
            "     12  0.6000       0.8055    0.4781        0.9449        \u001b[94m0.0920\u001b[0m        0.8928        0.0001  3.5058\n",
            "     13  0.6075       0.8035    0.4884        0.9455        \u001b[94m0.0888\u001b[0m        0.8939        0.0001  4.0031\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<class 'skorch.net.NeuralNet'>[initialized](\n",
              "  module_=ncf(\n",
              "    (gmf_embuserid): Embedding(943, 500)\n",
              "    (gmf_embgender): Embedding(2, 500)\n",
              "    (gmf_embage): Embedding(7, 500)\n",
              "    (gmf_embocc): Embedding(21, 500)\n",
              "    (gmf_embmovieid): Embedding(1682, 1981)\n",
              "    (mlp_embuserid): Embedding(943, 500)\n",
              "    (mlp_embgender): Embedding(2, 500)\n",
              "    (mlp_embage): Embedding(7, 500)\n",
              "    (mlp_embocc): Embedding(21, 500)\n",
              "    (mlp_embmovieid): Embedding(1682, 500)\n",
              "    (h1): Linear(in_features=2519, out_features=1200, bias=True)\n",
              "    (h2): Linear(in_features=1200, out_features=600, bias=True)\n",
              "    (dropout1): Dropout(p=0.5, inplace=False)\n",
              "    (dropout2): Dropout(p=0.5, inplace=False)\n",
              "    (last_layer): Linear(in_features=2600, out_features=1, bias=True)\n",
              "  ),\n",
              ")"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ncfnet.fit(train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFA1Uuo5NgJG"
      },
      "source": [
        "#### GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhK3602PNgJG",
        "outputId": "5be812c5-e28b-418c-9612-38c635aaaad6",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
            "[CV 1/3; 1/6] START lr=0.01, module__dropout=0.3, module__linear_size=200, module__size_emb=30\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss    cp      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ----  ------  ------\n",
            "      1  \u001b[36m0.4183\u001b[0m       \u001b[32m0.8544\u001b[0m    \u001b[35m0.2770\u001b[0m        \u001b[31m0.9731\u001b[0m        \u001b[94m0.9965\u001b[0m        \u001b[36m0.9469\u001b[0m     +  0.0100  7.2017\n",
            "      2  \u001b[36m0.6149\u001b[0m       \u001b[32m0.9057\u001b[0m    \u001b[35m0.4654\u001b[0m        \u001b[31m0.8364\u001b[0m        \u001b[94m0.7648\u001b[0m        \u001b[36m0.6995\u001b[0m     +  0.0100  7.4854\n",
            "      3  \u001b[36m0.6850\u001b[0m       \u001b[32m0.9193\u001b[0m    \u001b[35m0.5458\u001b[0m        \u001b[31m0.7758\u001b[0m        \u001b[94m0.4897\u001b[0m        \u001b[36m0.6018\u001b[0m     +  0.0100  5.8225\n",
            "      4  \u001b[36m0.7077\u001b[0m       \u001b[32m0.9300\u001b[0m    \u001b[35m0.5711\u001b[0m        \u001b[31m0.7475\u001b[0m        \u001b[94m0.3499\u001b[0m        \u001b[36m0.5587\u001b[0m     +  0.0100  7.6369\n",
            "      5  \u001b[36m0.7415\u001b[0m       0.9204    \u001b[35m0.6208\u001b[0m        \u001b[31m0.7389\u001b[0m        \u001b[94m0.2852\u001b[0m        \u001b[36m0.5460\u001b[0m     +  0.0100  6.5246\n",
            "      6  \u001b[36m0.7726\u001b[0m       0.9131    \u001b[35m0.6696\u001b[0m        0.7407        \u001b[94m0.2482\u001b[0m        0.5487        0.0100  7.8110\n",
            "      7  0.6949       \u001b[32m0.9321\u001b[0m    0.5539        \u001b[31m0.7293\u001b[0m        \u001b[94m0.2252\u001b[0m        \u001b[36m0.5318\u001b[0m     +  0.0100  6.2929\n",
            "      8  0.7313       0.9297    0.6027        \u001b[31m0.7229\u001b[0m        \u001b[94m0.1689\u001b[0m        \u001b[36m0.5226\u001b[0m     +  0.0010  7.0441\n",
            "      9  0.7378       0.9293    0.6117        \u001b[31m0.7199\u001b[0m        \u001b[94m0.1593\u001b[0m        \u001b[36m0.5183\u001b[0m     +  0.0010  7.0973\n",
            "     10  0.7422       0.9295    0.6177        \u001b[31m0.7182\u001b[0m        \u001b[94m0.1530\u001b[0m        \u001b[36m0.5158\u001b[0m     +  0.0010  6.5296\n",
            "     11  0.7367       0.9313    0.6094        \u001b[31m0.7169\u001b[0m        \u001b[94m0.1487\u001b[0m        \u001b[36m0.5139\u001b[0m     +  0.0010  7.5951\n",
            "     12  0.7371       0.9304    0.6103        \u001b[31m0.7162\u001b[0m        \u001b[94m0.1450\u001b[0m        \u001b[36m0.5129\u001b[0m     +  0.0010  6.1223\n",
            "     13  0.7395       0.9299    0.6138        \u001b[31m0.7157\u001b[0m        \u001b[94m0.1420\u001b[0m        \u001b[36m0.5122\u001b[0m     +  0.0010  7.0968\n",
            "     14  0.7393       0.9309    0.6131        \u001b[31m0.7154\u001b[0m        \u001b[94m0.1395\u001b[0m        \u001b[36m0.5117\u001b[0m     +  0.0010  6.9996\n",
            "     15  0.7423       0.9301    0.6176        \u001b[31m0.7153\u001b[0m        \u001b[94m0.1340\u001b[0m        \u001b[36m0.5117\u001b[0m     +  0.0001  6.7974\n",
            "     16  0.7421       0.9307    0.6171        \u001b[31m0.7153\u001b[0m        \u001b[94m0.1338\u001b[0m        \u001b[36m0.5116\u001b[0m     +  0.0001  7.1361\n",
            "     17  0.7409       0.9309    0.6153        \u001b[31m0.7152\u001b[0m        \u001b[94m0.1336\u001b[0m        \u001b[36m0.5114\u001b[0m     +  0.0001  6.4703\n",
            "     18  0.7433       0.9308    0.6187        \u001b[31m0.7151\u001b[0m        \u001b[94m0.1334\u001b[0m        \u001b[36m0.5114\u001b[0m     +  0.0001  7.5812\n",
            "     19  0.7446       0.9308    0.6205        0.7152        \u001b[94m0.1331\u001b[0m        0.5114        0.0001  6.7310\n",
            "     20  0.7422       0.9307    0.6171        \u001b[31m0.7151\u001b[0m        \u001b[94m0.1327\u001b[0m        \u001b[36m0.5113\u001b[0m     +  0.0001  7.4109\n",
            "     21  0.7440       0.9309    0.6197        \u001b[31m0.7150\u001b[0m        \u001b[94m0.1327\u001b[0m        \u001b[36m0.5113\u001b[0m     +  0.0001  7.3445\n",
            "     22  0.7436       0.9308    0.6191        \u001b[31m0.7150\u001b[0m        \u001b[94m0.1321\u001b[0m        \u001b[36m0.5113\u001b[0m     +  0.0000  6.1223\n",
            "     23  0.7431       0.9309    0.6184        \u001b[31m0.7150\u001b[0m        \u001b[94m0.1320\u001b[0m        \u001b[36m0.5112\u001b[0m     +  0.0000  7.6557\n",
            "     24  0.7426       0.9309    0.6176        \u001b[31m0.7150\u001b[0m        0.1322        \u001b[36m0.5112\u001b[0m     +  0.0000  6.0913\n",
            "     25  0.7425       0.9310    0.6174        \u001b[31m0.7150\u001b[0m        0.1321        \u001b[36m0.5112\u001b[0m     +  0.0000  7.6772\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 1/3; 1/6] END lr=0.01, module__dropout=0.3, module__linear_size=200, module__size_emb=30;, score=-1.256 total time= 3.4min\n",
            "[CV 2/3; 1/6] START lr=0.01, module__dropout=0.3, module__linear_size=200, module__size_emb=30\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss    cp      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ----  ------  ------\n",
            "      1  \u001b[36m0.4908\u001b[0m       \u001b[32m0.8464\u001b[0m    \u001b[35m0.3456\u001b[0m        \u001b[31m0.9431\u001b[0m        \u001b[94m1.0231\u001b[0m        \u001b[36m0.8894\u001b[0m     +  0.0100  6.1477\n",
            "      2  \u001b[36m0.6052\u001b[0m       \u001b[32m0.9056\u001b[0m    \u001b[35m0.4544\u001b[0m        \u001b[31m0.8120\u001b[0m        \u001b[94m0.7711\u001b[0m        \u001b[36m0.6593\u001b[0m     +  0.0100  7.7585\n",
            "      3  \u001b[36m0.6567\u001b[0m       \u001b[32m0.9255\u001b[0m    \u001b[35m0.5088\u001b[0m        \u001b[31m0.7371\u001b[0m        \u001b[94m0.4927\u001b[0m        \u001b[36m0.5434\u001b[0m     +  0.0100  6.2341\n",
            "      4  \u001b[36m0.6902\u001b[0m       \u001b[32m0.9290\u001b[0m    \u001b[35m0.5491\u001b[0m        \u001b[31m0.7096\u001b[0m        \u001b[94m0.3451\u001b[0m        \u001b[36m0.5035\u001b[0m     +  0.0100  7.4117\n",
            "      5  \u001b[36m0.7036\u001b[0m       \u001b[32m0.9300\u001b[0m    \u001b[35m0.5659\u001b[0m        \u001b[31m0.6961\u001b[0m        \u001b[94m0.2745\u001b[0m        \u001b[36m0.4846\u001b[0m     +  0.0100  8.1309\n",
            "      6  \u001b[36m0.7424\u001b[0m       0.9231    \u001b[35m0.6208\u001b[0m        \u001b[31m0.6918\u001b[0m        \u001b[94m0.2364\u001b[0m        \u001b[36m0.4786\u001b[0m     +  0.0100  6.7484\n",
            "      7  \u001b[36m0.7667\u001b[0m       0.9226    \u001b[35m0.6560\u001b[0m        \u001b[31m0.6899\u001b[0m        \u001b[94m0.2109\u001b[0m        \u001b[36m0.4759\u001b[0m     +  0.0100  7.3711\n",
            "      8  0.7206       \u001b[32m0.9312\u001b[0m    0.5876        \u001b[31m0.6823\u001b[0m        \u001b[94m0.1601\u001b[0m        \u001b[36m0.4656\u001b[0m     +  0.0010  6.7857\n",
            "      9  0.7261       \u001b[32m0.9318\u001b[0m    0.5948        \u001b[31m0.6798\u001b[0m        \u001b[94m0.1485\u001b[0m        \u001b[36m0.4621\u001b[0m     +  0.0010  7.2777\n",
            "     10  0.7273       \u001b[32m0.9319\u001b[0m    0.5964        \u001b[31m0.6783\u001b[0m        \u001b[94m0.1429\u001b[0m        \u001b[36m0.4601\u001b[0m     +  0.0010  7.5072\n",
            "     11  0.7335       0.9314    0.6050        \u001b[31m0.6774\u001b[0m        \u001b[94m0.1379\u001b[0m        \u001b[36m0.4589\u001b[0m     +  0.0010  6.6332\n",
            "     12  0.7329       0.9310    0.6043        \u001b[31m0.6769\u001b[0m        \u001b[94m0.1340\u001b[0m        \u001b[36m0.4583\u001b[0m     +  0.0010  7.3048\n",
            "     13  0.7308       0.9319    0.6011        \u001b[31m0.6767\u001b[0m        \u001b[94m0.1312\u001b[0m        \u001b[36m0.4579\u001b[0m     +  0.0010  6.7105\n",
            "     14  0.7346       0.9310    0.6067        \u001b[31m0.6758\u001b[0m        \u001b[94m0.1290\u001b[0m        \u001b[36m0.4567\u001b[0m     +  0.0010  8.4867\n",
            "     15  0.7373       0.9308    0.6105        0.6759        \u001b[94m0.1238\u001b[0m        0.4568        0.0001  7.8045\n",
            "     16  0.7373       0.9307    0.6105        0.6758        \u001b[94m0.1232\u001b[0m        0.4568        0.0001  6.3427\n",
            "     17  0.7345       0.9305    0.6067        0.6760        \u001b[94m0.1230\u001b[0m        0.4569        0.0001  7.8117\n",
            "     18  0.7330       0.9304    0.6047        0.6760        \u001b[94m0.1229\u001b[0m        0.4570        0.0001  6.7687\n",
            "     19  0.7361       0.9305    0.6089        0.6759        \u001b[94m0.1224\u001b[0m        0.4569        0.0001  7.3664\n",
            "     20  0.7359       0.9307    0.6086        0.6759        \u001b[94m0.1221\u001b[0m        0.4568        0.0001  6.6008\n",
            "     21  0.7366       0.9304    0.6097        0.6759        0.1222        0.4568        0.0001  7.1651\n",
            "     22  0.7366       0.9304    0.6097        0.6759        \u001b[94m0.1217\u001b[0m        0.4568        0.0000  6.9145\n",
            "     23  0.7363       0.9305    0.6092        0.6759        \u001b[94m0.1216\u001b[0m        0.4569        0.0000  7.0902\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 2/3; 1/6] END lr=0.01, module__dropout=0.3, module__linear_size=200, module__size_emb=30;, score=-1.122 total time= 3.2min\n",
            "[CV 3/3; 1/6] START lr=0.01, module__dropout=0.3, module__linear_size=200, module__size_emb=30\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss    cp      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ----  ------  ------\n",
            "      1  \u001b[36m0.5074\u001b[0m       \u001b[32m0.8387\u001b[0m    \u001b[35m0.3637\u001b[0m        \u001b[31m0.9423\u001b[0m        \u001b[94m0.9882\u001b[0m        \u001b[36m0.8880\u001b[0m     +  0.0100  6.7775\n",
            "      2  0.4812       \u001b[32m0.9404\u001b[0m    0.3233        \u001b[31m0.8299\u001b[0m        \u001b[94m0.7439\u001b[0m        \u001b[36m0.6887\u001b[0m     +  0.0100  7.2786\n",
            "      3  \u001b[36m0.6507\u001b[0m       0.9256    \u001b[35m0.5017\u001b[0m        \u001b[31m0.7541\u001b[0m        \u001b[94m0.4878\u001b[0m        \u001b[36m0.5687\u001b[0m     +  0.0100  6.3818\n",
            "      4  \u001b[36m0.6755\u001b[0m       0.9310    \u001b[35m0.5301\u001b[0m        \u001b[31m0.7241\u001b[0m        \u001b[94m0.3457\u001b[0m        \u001b[36m0.5244\u001b[0m     +  0.0100  7.1291\n",
            "      5  \u001b[36m0.6792\u001b[0m       0.9313    \u001b[35m0.5345\u001b[0m        \u001b[31m0.7160\u001b[0m        \u001b[94m0.2817\u001b[0m        \u001b[36m0.5126\u001b[0m     +  0.0100  7.0738\n",
            "      6  \u001b[36m0.7257\u001b[0m       0.9236    \u001b[35m0.5977\u001b[0m        \u001b[31m0.7103\u001b[0m        \u001b[94m0.2441\u001b[0m        \u001b[36m0.5045\u001b[0m     +  0.0100  6.5027\n",
            "      7  0.6830       0.9328    0.5387        \u001b[31m0.7080\u001b[0m        \u001b[94m0.2208\u001b[0m        \u001b[36m0.5012\u001b[0m     +  0.0100  7.8090\n",
            "      8  0.7122       0.9296    0.5772        \u001b[31m0.7022\u001b[0m        \u001b[94m0.1670\u001b[0m        \u001b[36m0.4931\u001b[0m     +  0.0010  6.7048\n",
            "      9  0.7102       0.9305    0.5742        \u001b[31m0.7002\u001b[0m        \u001b[94m0.1575\u001b[0m        \u001b[36m0.4902\u001b[0m     +  0.0010  8.0953\n",
            "     10  0.7144       0.9313    0.5795        \u001b[31m0.6979\u001b[0m        \u001b[94m0.1513\u001b[0m        \u001b[36m0.4870\u001b[0m     +  0.0010  6.8571\n",
            "     11  0.7129       0.9320    0.5773        \u001b[31m0.6976\u001b[0m        \u001b[94m0.1469\u001b[0m        \u001b[36m0.4866\u001b[0m     +  0.0010  6.8325\n",
            "     12  0.7177       0.9308    0.5840        \u001b[31m0.6967\u001b[0m        \u001b[94m0.1439\u001b[0m        \u001b[36m0.4854\u001b[0m     +  0.0010  7.6861\n",
            "     13  0.7167       0.9309    0.5826        \u001b[31m0.6964\u001b[0m        \u001b[94m0.1408\u001b[0m        \u001b[36m0.4850\u001b[0m     +  0.0010  6.2113\n",
            "     14  0.7210       0.9299    0.5888        \u001b[31m0.6956\u001b[0m        \u001b[94m0.1384\u001b[0m        \u001b[36m0.4839\u001b[0m     +  0.0010  7.6825\n",
            "     15  0.7218       0.9301    0.5897        0.6956        \u001b[94m0.1330\u001b[0m        0.4839        0.0001  6.2689\n",
            "     16  0.7221       0.9302    0.5902        0.6956        \u001b[94m0.1329\u001b[0m        0.4839        0.0001  7.7802\n",
            "     17  0.7215       0.9298    0.5894        0.6957        \u001b[94m0.1325\u001b[0m        0.4840        0.0001  6.1626\n",
            "     18  0.7229       0.9299    0.5912        0.6957        \u001b[94m0.1322\u001b[0m        0.4839        0.0001  7.5487\n",
            "     19  0.7214       0.9297    0.5894        0.6957        \u001b[94m0.1320\u001b[0m        0.4839        0.0001  6.0265\n",
            "     20  0.7205       0.9300    0.5880        0.6956        \u001b[94m0.1317\u001b[0m        0.4839        0.0001  7.6343\n",
            "     21  0.7256       0.9300    0.5949        \u001b[31m0.6955\u001b[0m        \u001b[94m0.1312\u001b[0m        \u001b[36m0.4837\u001b[0m     +  0.0001  6.3263\n",
            "     22  0.7247       0.9301    0.5936        0.6955        \u001b[94m0.1310\u001b[0m        0.4838        0.0000  7.1639\n",
            "     23  0.7240       0.9300    0.5927        0.6955        \u001b[94m0.1309\u001b[0m        0.4838        0.0000  7.3281\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 3/3; 1/6] END lr=0.01, module__dropout=0.3, module__linear_size=200, module__size_emb=30;, score=-1.186 total time= 3.0min\n",
            "[CV 1/3; 2/6] START lr=0.01, module__dropout=0.3, module__linear_size=200, module__size_emb=60\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss    cp      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ----  ------  ------\n",
            "      1  \u001b[36m0.4658\u001b[0m       \u001b[32m0.8441\u001b[0m    \u001b[35m0.3217\u001b[0m        \u001b[31m0.9634\u001b[0m        \u001b[94m0.9957\u001b[0m        \u001b[36m0.9282\u001b[0m     +  0.0100  8.2805\n",
            "      2  \u001b[36m0.6840\u001b[0m       \u001b[32m0.9148\u001b[0m    \u001b[35m0.5462\u001b[0m        \u001b[31m0.7919\u001b[0m        \u001b[94m0.7206\u001b[0m        \u001b[36m0.6271\u001b[0m     +  0.0100  7.3368\n",
            "      3  \u001b[36m0.7168\u001b[0m       \u001b[32m0.9426\u001b[0m    \u001b[35m0.5782\u001b[0m        \u001b[31m0.7231\u001b[0m        \u001b[94m0.3746\u001b[0m        \u001b[36m0.5229\u001b[0m     +  0.0100  7.4032\n",
            "      4  \u001b[36m0.7553\u001b[0m       \u001b[32m0.9438\u001b[0m    \u001b[35m0.6295\u001b[0m        \u001b[31m0.7018\u001b[0m        \u001b[94m0.2268\u001b[0m        \u001b[36m0.4925\u001b[0m     +  0.0100  8.3514\n",
            "      5  0.7536       \u001b[32m0.9483\u001b[0m    0.6253        \u001b[31m0.6897\u001b[0m        \u001b[94m0.1658\u001b[0m        \u001b[36m0.4756\u001b[0m     +  0.0100  6.8952\n",
            "      6  0.7335       \u001b[32m0.9507\u001b[0m    0.5970        \u001b[31m0.6851\u001b[0m        \u001b[94m0.1351\u001b[0m        \u001b[36m0.4694\u001b[0m     +  0.0100  8.2574\n",
            "      7  0.7546       0.9481    0.6266        \u001b[31m0.6836\u001b[0m        \u001b[94m0.1184\u001b[0m        \u001b[36m0.4674\u001b[0m     +  0.0100  14.4077\n",
            "      8  0.7470       0.9506    0.6152        \u001b[31m0.6761\u001b[0m        \u001b[94m0.0884\u001b[0m        \u001b[36m0.4572\u001b[0m     +  0.0010  8.2223\n",
            "      9  0.7482       \u001b[32m0.9513\u001b[0m    0.6166        \u001b[31m0.6718\u001b[0m        \u001b[94m0.0762\u001b[0m        \u001b[36m0.4513\u001b[0m     +  0.0010  6.8578\n",
            "     10  0.7494       \u001b[32m0.9515\u001b[0m    0.6181        \u001b[31m0.6693\u001b[0m        \u001b[94m0.0681\u001b[0m        \u001b[36m0.4480\u001b[0m     +  0.0010  8.2549\n",
            "     11  0.7480       0.9509    0.6164        \u001b[31m0.6678\u001b[0m        \u001b[94m0.0626\u001b[0m        \u001b[36m0.4459\u001b[0m     +  0.0010  10.3094\n",
            "     12  \u001b[36m0.7580\u001b[0m       \u001b[32m0.9517\u001b[0m    \u001b[35m0.6298\u001b[0m        \u001b[31m0.6668\u001b[0m        \u001b[94m0.0587\u001b[0m        \u001b[36m0.4446\u001b[0m     +  0.0010  11.9405\n",
            "     13  \u001b[36m0.7640\u001b[0m       0.9504    \u001b[35m0.6387\u001b[0m        \u001b[31m0.6662\u001b[0m        \u001b[94m0.0558\u001b[0m        \u001b[36m0.4439\u001b[0m     +  0.0010  7.8221\n",
            "     14  0.7604       0.9512    0.6334        \u001b[31m0.6654\u001b[0m        \u001b[94m0.0534\u001b[0m        \u001b[36m0.4427\u001b[0m     +  0.0010  10.0196\n",
            "     15  0.7590       0.9513    0.6313        \u001b[31m0.6653\u001b[0m        \u001b[94m0.0505\u001b[0m        \u001b[36m0.4427\u001b[0m     +  0.0001  8.3622\n",
            "     16  0.7606       0.9512    0.6337        \u001b[31m0.6653\u001b[0m        \u001b[94m0.0502\u001b[0m        \u001b[36m0.4426\u001b[0m     +  0.0001  8.0867\n",
            "     17  0.7624       0.9511    0.6361        \u001b[31m0.6653\u001b[0m        \u001b[94m0.0500\u001b[0m        \u001b[36m0.4426\u001b[0m     +  0.0001  7.5914\n",
            "     18  0.7594       0.9514    0.6319        \u001b[31m0.6652\u001b[0m        \u001b[94m0.0499\u001b[0m        \u001b[36m0.4425\u001b[0m     +  0.0001  8.3439\n",
            "     19  0.7623       0.9510    0.6361        0.6652        \u001b[94m0.0497\u001b[0m        0.4425        0.0001  9.6265\n",
            "     20  0.7604       0.9512    0.6334        \u001b[31m0.6651\u001b[0m        \u001b[94m0.0493\u001b[0m        \u001b[36m0.4424\u001b[0m     +  0.0001  7.5333\n",
            "     21  0.7601       0.9513    0.6329        \u001b[31m0.6650\u001b[0m        \u001b[94m0.0491\u001b[0m        \u001b[36m0.4422\u001b[0m     +  0.0001  7.9030\n",
            "     22  0.7604       0.9513    0.6333        0.6650        \u001b[94m0.0488\u001b[0m        0.4422        0.0000  7.3179\n",
            "     23  0.7607       0.9511    0.6338        0.6650        \u001b[94m0.0487\u001b[0m        0.4422        0.0000  7.8132\n",
            "     24  0.7609       0.9511    0.6341        0.6650        0.0489        0.4423        0.0000  8.2360\n",
            "     25  0.7611       0.9512    0.6344        0.6650        \u001b[94m0.0487\u001b[0m        0.4422        0.0000  6.7063\n",
            "     26  0.7610       0.9510    0.6343        \u001b[31m0.6650\u001b[0m        0.0487        \u001b[36m0.4422\u001b[0m     +  0.0000  8.3019\n",
            "     27  0.7620       0.9511    0.6356        0.6650        0.0488        0.4423        0.0000  6.9213\n",
            "     28  0.7622       0.9511    0.6358        0.6650        0.0487        0.4422        0.0000  9.3915\n",
            "     29  0.7622       0.9511    0.6359        0.6650        \u001b[94m0.0486\u001b[0m        0.4422        0.0000  7.6091\n",
            "     30  0.7621       0.9511    0.6357        0.6650        0.0487        0.4422        0.0000  7.1938\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 1/3; 2/6] END lr=0.01, module__dropout=0.3, module__linear_size=200, module__size_emb=60;, score=-1.216 total time= 4.8min\n",
            "[CV 2/3; 2/6] START lr=0.01, module__dropout=0.3, module__linear_size=200, module__size_emb=60\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss    cp      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ----  ------  ------\n",
            "      1  \u001b[36m0.5054\u001b[0m       \u001b[32m0.8575\u001b[0m    \u001b[35m0.3583\u001b[0m        \u001b[31m0.9319\u001b[0m        \u001b[94m1.0359\u001b[0m        \u001b[36m0.8684\u001b[0m     +  0.0100  7.7827\n",
            "      2  \u001b[36m0.6899\u001b[0m       \u001b[32m0.9141\u001b[0m    \u001b[35m0.5540\u001b[0m        \u001b[31m0.7553\u001b[0m        \u001b[94m0.7006\u001b[0m        \u001b[36m0.5705\u001b[0m     +  0.0100  7.3671\n",
            "      3  \u001b[36m0.7014\u001b[0m       \u001b[32m0.9449\u001b[0m    \u001b[35m0.5577\u001b[0m        \u001b[31m0.6854\u001b[0m        \u001b[94m0.3558\u001b[0m        \u001b[36m0.4697\u001b[0m     +  0.0100  7.8888\n",
            "      4  \u001b[36m0.7078\u001b[0m       \u001b[32m0.9524\u001b[0m    \u001b[35m0.5632\u001b[0m        \u001b[31m0.6596\u001b[0m        \u001b[94m0.2128\u001b[0m        \u001b[36m0.4350\u001b[0m     +  0.0100  8.0748\n",
            "      5  \u001b[36m0.7429\u001b[0m       0.9501    \u001b[35m0.6099\u001b[0m        \u001b[31m0.6497\u001b[0m        \u001b[94m0.1540\u001b[0m        \u001b[36m0.4221\u001b[0m     +  0.0100  7.2678\n",
            "      6  0.7280       0.9520    0.5894        \u001b[31m0.6460\u001b[0m        \u001b[94m0.1257\u001b[0m        \u001b[36m0.4173\u001b[0m     +  0.0100  7.7064\n",
            "      7  \u001b[36m0.7621\u001b[0m       0.9494    \u001b[35m0.6366\u001b[0m        \u001b[31m0.6428\u001b[0m        \u001b[94m0.1092\u001b[0m        \u001b[36m0.4132\u001b[0m     +  0.0100  7.5976\n",
            "      8  0.7266       \u001b[32m0.9524\u001b[0m    0.5873        \u001b[31m0.6359\u001b[0m        \u001b[94m0.0839\u001b[0m        \u001b[36m0.4043\u001b[0m     +  0.0010  7.2134\n",
            "      9  0.7300       0.9524    0.5919        \u001b[31m0.6315\u001b[0m        \u001b[94m0.0707\u001b[0m        \u001b[36m0.3988\u001b[0m     +  0.0010  8.1202\n",
            "     10  0.7401       0.9514    0.6056        \u001b[31m0.6289\u001b[0m        \u001b[94m0.0627\u001b[0m        \u001b[36m0.3955\u001b[0m     +  0.0010  6.8225\n",
            "     11  0.7436       0.9518    0.6101        \u001b[31m0.6273\u001b[0m        \u001b[94m0.0571\u001b[0m        \u001b[36m0.3935\u001b[0m     +  0.0010  8.6851\n",
            "     12  0.7410       0.9512    0.6069        \u001b[31m0.6266\u001b[0m        \u001b[94m0.0531\u001b[0m        \u001b[36m0.3927\u001b[0m     +  0.0010  8.3373\n",
            "     13  0.7483       0.9513    0.6167        \u001b[31m0.6259\u001b[0m        \u001b[94m0.0504\u001b[0m        \u001b[36m0.3918\u001b[0m     +  0.0010  7.3467\n",
            "     14  0.7454       0.9510    0.6129        \u001b[31m0.6257\u001b[0m        \u001b[94m0.0481\u001b[0m        \u001b[36m0.3915\u001b[0m     +  0.0010  7.8572\n",
            "     15  0.7487       0.9512    0.6173        \u001b[31m0.6255\u001b[0m        \u001b[94m0.0451\u001b[0m        \u001b[36m0.3912\u001b[0m     +  0.0001  8.0438\n",
            "     16  0.7501       0.9515    0.6191        \u001b[31m0.6254\u001b[0m        \u001b[94m0.0450\u001b[0m        \u001b[36m0.3911\u001b[0m     +  0.0001  6.8469\n",
            "     17  0.7505       0.9515    0.6197        \u001b[31m0.6253\u001b[0m        \u001b[94m0.0447\u001b[0m        \u001b[36m0.3911\u001b[0m     +  0.0001  8.2402\n",
            "     18  0.7517       0.9516    0.6212        \u001b[31m0.6253\u001b[0m        0.0447        \u001b[36m0.3910\u001b[0m     +  0.0001  7.2273\n",
            "     19  0.7509       0.9515    0.6201        0.6253        \u001b[94m0.0445\u001b[0m        0.3910        0.0001  7.6578\n",
            "     20  0.7532       0.9509    0.6236        \u001b[31m0.6252\u001b[0m        \u001b[94m0.0442\u001b[0m        \u001b[36m0.3909\u001b[0m     +  0.0001  7.4980\n",
            "     21  0.7502       0.9512    0.6193        0.6253        \u001b[94m0.0440\u001b[0m        0.3909        0.0001  5.8309\n",
            "     22  0.7512       0.9512    0.6208        0.6252        \u001b[94m0.0436\u001b[0m        0.3909        0.0000  7.1924\n",
            "     23  0.7514       0.9512    0.6210        \u001b[31m0.6252\u001b[0m        0.0437        \u001b[36m0.3909\u001b[0m     +  0.0000  6.2282\n",
            "     24  0.7516       0.9512    0.6212        \u001b[31m0.6252\u001b[0m        \u001b[94m0.0436\u001b[0m        \u001b[36m0.3909\u001b[0m     +  0.0000  7.8307\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 2/3; 2/6] END lr=0.01, module__dropout=0.3, module__linear_size=200, module__size_emb=60;, score=-1.084 total time= 3.5min\n",
            "[CV 3/3; 2/6] START lr=0.01, module__dropout=0.3, module__linear_size=200, module__size_emb=60\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss    cp      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ----  ------  ------\n",
            "      1  \u001b[36m0.3369\u001b[0m       \u001b[32m0.8939\u001b[0m    \u001b[35m0.2076\u001b[0m        \u001b[31m0.9456\u001b[0m        \u001b[94m0.9966\u001b[0m        \u001b[36m0.8941\u001b[0m     +  0.0100  6.3317\n",
            "      2  \u001b[36m0.6051\u001b[0m       \u001b[32m0.9319\u001b[0m    \u001b[35m0.4480\u001b[0m        \u001b[31m0.7737\u001b[0m        \u001b[94m0.7010\u001b[0m        \u001b[36m0.5987\u001b[0m     +  0.0100  6.7959\n",
            "      3  \u001b[36m0.6759\u001b[0m       \u001b[32m0.9494\u001b[0m    \u001b[35m0.5247\u001b[0m        \u001b[31m0.7047\u001b[0m        \u001b[94m0.3727\u001b[0m        \u001b[36m0.4965\u001b[0m     +  0.0100  5.6509\n",
            "      4  \u001b[36m0.7225\u001b[0m       0.9422    \u001b[35m0.5858\u001b[0m        \u001b[31m0.6810\u001b[0m        \u001b[94m0.2257\u001b[0m        \u001b[36m0.4637\u001b[0m     +  0.0100  7.2168\n",
            "      5  \u001b[36m0.7407\u001b[0m       0.9443    \u001b[35m0.6093\u001b[0m        \u001b[31m0.6705\u001b[0m        \u001b[94m0.1646\u001b[0m        \u001b[36m0.4496\u001b[0m     +  0.0100  5.7169\n",
            "      6  0.6990       0.9458    0.5543        \u001b[31m0.6643\u001b[0m        \u001b[94m0.1366\u001b[0m        \u001b[36m0.4414\u001b[0m     +  0.0100  7.3966\n",
            "      7  0.7256       0.9463    0.5884        \u001b[31m0.6597\u001b[0m        \u001b[94m0.1181\u001b[0m        \u001b[36m0.4353\u001b[0m     +  0.0100  6.4000\n",
            "      8  0.7217       0.9473    0.5829        \u001b[31m0.6544\u001b[0m        \u001b[94m0.0881\u001b[0m        \u001b[36m0.4283\u001b[0m     +  0.0010  6.7489\n",
            "      9  0.7248       0.9481    0.5866        \u001b[31m0.6512\u001b[0m        \u001b[94m0.0759\u001b[0m        \u001b[36m0.4241\u001b[0m     +  0.0010  6.4221\n",
            "     10  0.7209       0.9483    0.5814        \u001b[31m0.6490\u001b[0m        \u001b[94m0.0680\u001b[0m        \u001b[36m0.4213\u001b[0m     +  0.0010  7.2037\n",
            "     11  0.7272       0.9489    0.5894        \u001b[31m0.6475\u001b[0m        \u001b[94m0.0627\u001b[0m        \u001b[36m0.4193\u001b[0m     +  0.0010  7.0761\n",
            "     12  0.7347       0.9487    0.5995        \u001b[31m0.6469\u001b[0m        \u001b[94m0.0589\u001b[0m        \u001b[36m0.4185\u001b[0m     +  0.0010  6.1455\n",
            "     13  0.7326       0.9484    0.5968        \u001b[31m0.6463\u001b[0m        \u001b[94m0.0563\u001b[0m        \u001b[36m0.4178\u001b[0m     +  0.0010  7.3442\n",
            "     14  \u001b[36m0.7421\u001b[0m       0.9486    \u001b[35m0.6095\u001b[0m        \u001b[31m0.6456\u001b[0m        \u001b[94m0.0540\u001b[0m        \u001b[36m0.4168\u001b[0m     +  0.0010  5.9546\n",
            "     15  0.7413       0.9486    0.6084        0.6457        \u001b[94m0.0510\u001b[0m        0.4169        0.0001  7.3297\n",
            "     16  0.7407       0.9484    0.6076        0.6457        \u001b[94m0.0505\u001b[0m        0.4169        0.0001  6.2540\n",
            "     17  \u001b[36m0.7434\u001b[0m       0.9485    \u001b[35m0.6112\u001b[0m        0.6457        0.0506        0.4169        0.0001  7.0960\n",
            "     18  0.7400       0.9482    0.6068        0.6457        \u001b[94m0.0502\u001b[0m        0.4170        0.0001  6.2665\n",
            "     19  0.7429       0.9484    0.6106        0.6456        \u001b[94m0.0501\u001b[0m        0.4168        0.0001  6.9525\n",
            "     20  0.7417       0.9483    0.6090        0.6456        \u001b[94m0.0499\u001b[0m        0.4169        0.0001  6.1947\n",
            "     21  0.7404       0.9483    0.6072        0.6456        \u001b[94m0.0495\u001b[0m        0.4168        0.0001  7.0231\n",
            "     22  0.7412       0.9482    0.6084        \u001b[31m0.6456\u001b[0m        \u001b[94m0.0493\u001b[0m        \u001b[36m0.4168\u001b[0m     +  0.0000  6.2875\n",
            "     23  0.7418       0.9483    0.6091        \u001b[31m0.6456\u001b[0m        \u001b[94m0.0492\u001b[0m        \u001b[36m0.4168\u001b[0m     +  0.0000  7.0088\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 3/3; 2/6] END lr=0.01, module__dropout=0.3, module__linear_size=200, module__size_emb=60;, score=-1.154 total time= 2.9min\n",
            "[CV 1/3; 3/6] START lr=0.01, module__dropout=0.3, module__linear_size=200, module__size_emb=120\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss    cp      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ----  ------  ------\n",
            "      1  \u001b[36m0.5012\u001b[0m       \u001b[32m0.8462\u001b[0m    \u001b[35m0.3560\u001b[0m        \u001b[31m0.9560\u001b[0m        \u001b[94m0.9923\u001b[0m        \u001b[36m0.9139\u001b[0m     +  0.0100  8.0546\n",
            "      2  \u001b[36m0.6401\u001b[0m       \u001b[32m0.9559\u001b[0m    \u001b[35m0.4811\u001b[0m        \u001b[31m0.7530\u001b[0m        \u001b[94m0.6596\u001b[0m        \u001b[36m0.5671\u001b[0m     +  0.0100  9.0608\n",
            "      3  \u001b[36m0.7506\u001b[0m       0.9514    \u001b[35m0.6199\u001b[0m        \u001b[31m0.6951\u001b[0m        \u001b[94m0.2699\u001b[0m        \u001b[36m0.4831\u001b[0m     +  0.0100  8.3385\n",
            "      4  \u001b[36m0.7575\u001b[0m       \u001b[32m0.9591\u001b[0m    \u001b[35m0.6260\u001b[0m        \u001b[31m0.6761\u001b[0m        \u001b[94m0.1466\u001b[0m        \u001b[36m0.4571\u001b[0m     +  0.0100  7.3287\n",
            "      5  0.7434       0.9546    0.6088        \u001b[31m0.6683\u001b[0m        \u001b[94m0.1038\u001b[0m        \u001b[36m0.4466\u001b[0m     +  0.0100  8.8627\n",
            "      6  0.7449       0.9546    0.6107        \u001b[31m0.6654\u001b[0m        \u001b[94m0.0871\u001b[0m        \u001b[36m0.4428\u001b[0m     +  0.0100  7.1928\n",
            "      7  0.7344       0.9576    0.5956        \u001b[31m0.6635\u001b[0m        \u001b[94m0.0794\u001b[0m        \u001b[36m0.4402\u001b[0m     +  0.0100  8.1753\n",
            "      8  0.7436       0.9587    0.6073        \u001b[31m0.6546\u001b[0m        \u001b[94m0.0654\u001b[0m        \u001b[36m0.4285\u001b[0m     +  0.0010  8.7803\n",
            "      9  \u001b[36m0.7576\u001b[0m       0.9589    \u001b[35m0.6262\u001b[0m        \u001b[31m0.6493\u001b[0m        \u001b[94m0.0493\u001b[0m        \u001b[36m0.4216\u001b[0m     +  0.0010  7.8591\n",
            "     10  0.7533       \u001b[32m0.9596\u001b[0m    0.6199        \u001b[31m0.6456\u001b[0m        \u001b[94m0.0395\u001b[0m        \u001b[36m0.4168\u001b[0m     +  0.0010  7.2599\n",
            "     11  \u001b[36m0.7648\u001b[0m       0.9595    \u001b[35m0.6358\u001b[0m        \u001b[31m0.6436\u001b[0m        \u001b[94m0.0332\u001b[0m        \u001b[36m0.4142\u001b[0m     +  0.0010  7.6536\n",
            "     12  \u001b[36m0.7784\u001b[0m       0.9595    \u001b[35m0.6548\u001b[0m        \u001b[31m0.6423\u001b[0m        \u001b[94m0.0288\u001b[0m        \u001b[36m0.4125\u001b[0m     +  0.0010  7.0302\n",
            "     13  \u001b[36m0.7816\u001b[0m       \u001b[32m0.9603\u001b[0m    \u001b[35m0.6589\u001b[0m        \u001b[31m0.6413\u001b[0m        \u001b[94m0.0259\u001b[0m        \u001b[36m0.4113\u001b[0m     +  0.0010  8.7434\n",
            "     14  \u001b[36m0.7839\u001b[0m       0.9591    \u001b[35m0.6628\u001b[0m        \u001b[31m0.6405\u001b[0m        \u001b[94m0.0238\u001b[0m        \u001b[36m0.4102\u001b[0m     +  0.0010  8.8292\n",
            "     15  \u001b[36m0.7855\u001b[0m       0.9591    \u001b[35m0.6651\u001b[0m        \u001b[31m0.6405\u001b[0m        \u001b[94m0.0218\u001b[0m        \u001b[36m0.4102\u001b[0m     +  0.0001  7.4446\n",
            "     16  \u001b[36m0.7857\u001b[0m       0.9593    \u001b[35m0.6653\u001b[0m        \u001b[31m0.6404\u001b[0m        \u001b[94m0.0216\u001b[0m        \u001b[36m0.4101\u001b[0m     +  0.0001  8.6064\n",
            "     17  \u001b[36m0.7884\u001b[0m       0.9594    \u001b[35m0.6691\u001b[0m        \u001b[31m0.6404\u001b[0m        \u001b[94m0.0215\u001b[0m        \u001b[36m0.4101\u001b[0m     +  0.0001  8.1106\n",
            "     18  \u001b[36m0.7895\u001b[0m       0.9596    \u001b[35m0.6707\u001b[0m        \u001b[31m0.6403\u001b[0m        \u001b[94m0.0213\u001b[0m        \u001b[36m0.4100\u001b[0m     +  0.0001  7.4965\n",
            "     19  0.7889       0.9595    0.6698        \u001b[31m0.6402\u001b[0m        \u001b[94m0.0212\u001b[0m        \u001b[36m0.4099\u001b[0m     +  0.0001  7.7564\n",
            "     20  0.7874       0.9593    0.6677        \u001b[31m0.6401\u001b[0m        \u001b[94m0.0209\u001b[0m        \u001b[36m0.4098\u001b[0m     +  0.0001  8.2537\n",
            "     21  0.7886       0.9594    0.6694        \u001b[31m0.6401\u001b[0m        \u001b[94m0.0208\u001b[0m        \u001b[36m0.4097\u001b[0m     +  0.0001  7.5754\n",
            "     22  0.7887       0.9594    0.6696        \u001b[31m0.6401\u001b[0m        \u001b[94m0.0206\u001b[0m        \u001b[36m0.4097\u001b[0m     +  0.0000  8.0765\n",
            "     23  0.7894       0.9593    0.6706        0.6401        \u001b[94m0.0206\u001b[0m        0.4097        0.0000  8.9580\n",
            "     24  0.7887       0.9593    0.6696        \u001b[31m0.6401\u001b[0m        \u001b[94m0.0206\u001b[0m        \u001b[36m0.4097\u001b[0m     +  0.0000  8.1009\n",
            "     25  0.7889       0.9593    0.6699        \u001b[31m0.6401\u001b[0m        \u001b[94m0.0205\u001b[0m        \u001b[36m0.4097\u001b[0m     +  0.0000  10.5691\n",
            "     26  0.7892       0.9593    0.6703        \u001b[31m0.6401\u001b[0m        0.0206        \u001b[36m0.4097\u001b[0m     +  0.0000  7.7656\n",
            "     27  \u001b[36m0.7897\u001b[0m       0.9594    \u001b[35m0.6710\u001b[0m        0.6401        \u001b[94m0.0205\u001b[0m        0.4097        0.0000  7.4612\n",
            "     28  \u001b[36m0.7899\u001b[0m       0.9593    \u001b[35m0.6714\u001b[0m        \u001b[31m0.6401\u001b[0m        \u001b[94m0.0205\u001b[0m        \u001b[36m0.4097\u001b[0m     +  0.0000  8.4488\n",
            "     29  0.7899       0.9593    0.6714        \u001b[31m0.6401\u001b[0m        0.0205        \u001b[36m0.4097\u001b[0m     +  0.0000  8.2125\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 1/3; 3/6] END lr=0.01, module__dropout=0.3, module__linear_size=200, module__size_emb=120;, score=-1.176 total time= 4.6min\n",
            "[CV 2/3; 3/6] START lr=0.01, module__dropout=0.3, module__linear_size=200, module__size_emb=120\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss    cp      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ----  ------  ------\n",
            "      1  \u001b[36m0.4187\u001b[0m       \u001b[32m0.8855\u001b[0m    \u001b[35m0.2742\u001b[0m        \u001b[31m0.9279\u001b[0m        \u001b[94m1.0358\u001b[0m        \u001b[36m0.8610\u001b[0m     +  0.0100  7.4310\n",
            "      2  \u001b[36m0.6590\u001b[0m       \u001b[32m0.9582\u001b[0m    \u001b[35m0.5022\u001b[0m        \u001b[31m0.7146\u001b[0m        \u001b[94m0.6497\u001b[0m        \u001b[36m0.5106\u001b[0m     +  0.0100  9.0427\n",
            "      3  \u001b[36m0.7415\u001b[0m       0.9524    \u001b[35m0.6070\u001b[0m        \u001b[31m0.6550\u001b[0m        \u001b[94m0.2576\u001b[0m        \u001b[36m0.4291\u001b[0m     +  0.0100  7.9697\n",
            "      4  0.7329       0.9571    0.5938        \u001b[31m0.6368\u001b[0m        \u001b[94m0.1395\u001b[0m        \u001b[36m0.4056\u001b[0m     +  0.0100  7.2473\n",
            "      5  0.7357       0.9571    0.5976        \u001b[31m0.6293\u001b[0m        \u001b[94m0.1009\u001b[0m        \u001b[36m0.3961\u001b[0m     +  0.0100  8.5834\n",
            "      6  \u001b[36m0.7511\u001b[0m       0.9545    \u001b[35m0.6192\u001b[0m        \u001b[31m0.6285\u001b[0m        \u001b[94m0.0866\u001b[0m        \u001b[36m0.3950\u001b[0m     +  0.0100  7.2877\n",
            "      7  0.7159       0.9571    0.5718        \u001b[31m0.6244\u001b[0m        \u001b[94m0.0792\u001b[0m        \u001b[36m0.3899\u001b[0m     +  0.0100  8.3338\n",
            "      8  0.7348       \u001b[32m0.9586\u001b[0m    0.5957        \u001b[31m0.6155\u001b[0m        \u001b[94m0.0651\u001b[0m        \u001b[36m0.3788\u001b[0m     +  0.0010  8.2647\n",
            "      9  0.7298       \u001b[32m0.9590\u001b[0m    0.5891        \u001b[31m0.6105\u001b[0m        \u001b[94m0.0491\u001b[0m        \u001b[36m0.3728\u001b[0m     +  0.0010  7.0924\n",
            "     10  0.7409       \u001b[32m0.9594\u001b[0m    0.6035        \u001b[31m0.6072\u001b[0m        \u001b[94m0.0392\u001b[0m        \u001b[36m0.3686\u001b[0m     +  0.0010  7.8330\n",
            "     11  0.7429       \u001b[32m0.9600\u001b[0m    0.6060        \u001b[31m0.6053\u001b[0m        \u001b[94m0.0328\u001b[0m        \u001b[36m0.3663\u001b[0m     +  0.0010  8.1170\n",
            "     12  \u001b[36m0.7640\u001b[0m       0.9592    \u001b[35m0.6348\u001b[0m        \u001b[31m0.6038\u001b[0m        \u001b[94m0.0284\u001b[0m        \u001b[36m0.3645\u001b[0m     +  0.0010  6.8195\n",
            "     13  \u001b[36m0.7687\u001b[0m       0.9591    \u001b[35m0.6413\u001b[0m        \u001b[31m0.6030\u001b[0m        \u001b[94m0.0254\u001b[0m        \u001b[36m0.3637\u001b[0m     +  0.0010  8.2519\n",
            "     14  \u001b[36m0.7697\u001b[0m       0.9596    \u001b[35m0.6426\u001b[0m        \u001b[31m0.6022\u001b[0m        \u001b[94m0.0233\u001b[0m        \u001b[36m0.3626\u001b[0m     +  0.0010  7.6975\n",
            "     15  \u001b[36m0.7702\u001b[0m       0.9594    \u001b[35m0.6433\u001b[0m        0.6022        \u001b[94m0.0213\u001b[0m        0.3627        0.0001  7.4511\n",
            "     16  \u001b[36m0.7725\u001b[0m       0.9593    \u001b[35m0.6467\u001b[0m        \u001b[31m0.6022\u001b[0m        \u001b[94m0.0211\u001b[0m        \u001b[36m0.3626\u001b[0m     +  0.0001  8.6235\n",
            "     17  0.7719       0.9594    0.6458        \u001b[31m0.6021\u001b[0m        \u001b[94m0.0209\u001b[0m        \u001b[36m0.3626\u001b[0m     +  0.0001  7.5045\n",
            "     18  \u001b[36m0.7727\u001b[0m       0.9592    \u001b[35m0.6469\u001b[0m        \u001b[31m0.6021\u001b[0m        \u001b[94m0.0208\u001b[0m        \u001b[36m0.3625\u001b[0m     +  0.0001  8.3721\n",
            "     19  \u001b[36m0.7743\u001b[0m       0.9592    \u001b[35m0.6492\u001b[0m        \u001b[31m0.6020\u001b[0m        \u001b[94m0.0206\u001b[0m        \u001b[36m0.3625\u001b[0m     +  0.0001  8.3544\n",
            "     20  0.7728       0.9588    0.6472        \u001b[31m0.6020\u001b[0m        \u001b[94m0.0204\u001b[0m        \u001b[36m0.3624\u001b[0m     +  0.0001  7.5525\n",
            "     21  0.7742       0.9589    0.6492        \u001b[31m0.6019\u001b[0m        \u001b[94m0.0201\u001b[0m        \u001b[36m0.3623\u001b[0m     +  0.0001  8.1243\n",
            "     22  \u001b[36m0.7745\u001b[0m       0.9588    \u001b[35m0.6495\u001b[0m        \u001b[31m0.6019\u001b[0m        \u001b[94m0.0200\u001b[0m        \u001b[36m0.3623\u001b[0m     +  0.0000  9.0162\n",
            "     23  \u001b[36m0.7745\u001b[0m       0.9588    \u001b[35m0.6496\u001b[0m        \u001b[31m0.6019\u001b[0m        \u001b[94m0.0200\u001b[0m        \u001b[36m0.3623\u001b[0m     +  0.0000  7.1653\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 2/3; 3/6] END lr=0.01, module__dropout=0.3, module__linear_size=200, module__size_emb=120;, score=-1.044 total time= 3.5min\n",
            "[CV 3/3; 3/6] START lr=0.01, module__dropout=0.3, module__linear_size=200, module__size_emb=120\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss    cp      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ----  ------  ------\n",
            "      1  \u001b[36m0.5323\u001b[0m       \u001b[32m0.8481\u001b[0m    \u001b[35m0.3879\u001b[0m        \u001b[31m0.9236\u001b[0m        \u001b[94m1.0134\u001b[0m        \u001b[36m0.8530\u001b[0m     +  0.0100  7.7321\n",
            "      2  \u001b[36m0.6929\u001b[0m       \u001b[32m0.9278\u001b[0m    \u001b[35m0.5530\u001b[0m        \u001b[31m0.7303\u001b[0m        \u001b[94m0.6476\u001b[0m        \u001b[36m0.5333\u001b[0m     +  0.0100  6.8464\n",
            "      3  0.6807       \u001b[32m0.9584\u001b[0m    0.5278        \u001b[31m0.6761\u001b[0m        \u001b[94m0.2713\u001b[0m        \u001b[36m0.4571\u001b[0m     +  0.0100  8.1412\n",
            "      4  \u001b[36m0.7300\u001b[0m       0.9564    \u001b[35m0.5903\u001b[0m        \u001b[31m0.6532\u001b[0m        \u001b[94m0.1474\u001b[0m        \u001b[36m0.4267\u001b[0m     +  0.0100  8.3125\n",
            "      5  \u001b[36m0.7421\u001b[0m       0.9537    \u001b[35m0.6073\u001b[0m        \u001b[31m0.6476\u001b[0m        \u001b[94m0.1056\u001b[0m        \u001b[36m0.4194\u001b[0m     +  0.0100  7.1868\n",
            "      6  0.7262       0.9553    0.5857        \u001b[31m0.6442\u001b[0m        \u001b[94m0.0890\u001b[0m        \u001b[36m0.4150\u001b[0m     +  0.0100  7.7074\n",
            "      7  0.7231       0.9551    0.5818        \u001b[31m0.6398\u001b[0m        \u001b[94m0.0807\u001b[0m        \u001b[36m0.4094\u001b[0m     +  0.0100  8.6550\n",
            "      8  0.7228       0.9559    0.5811        \u001b[31m0.6327\u001b[0m        \u001b[94m0.0648\u001b[0m        \u001b[36m0.4003\u001b[0m     +  0.0010  7.1384\n",
            "      9  0.7254       0.9567    0.5841        \u001b[31m0.6274\u001b[0m        \u001b[94m0.0499\u001b[0m        \u001b[36m0.3936\u001b[0m     +  0.0010  8.5905\n",
            "     10  0.7412       0.9570    0.6049        \u001b[31m0.6245\u001b[0m        \u001b[94m0.0405\u001b[0m        \u001b[36m0.3900\u001b[0m     +  0.0010  7.4638\n",
            "     11  \u001b[36m0.7466\u001b[0m       0.9571    \u001b[35m0.6120\u001b[0m        \u001b[31m0.6228\u001b[0m        \u001b[94m0.0346\u001b[0m        \u001b[36m0.3879\u001b[0m     +  0.0010  8.2579\n",
            "     12  \u001b[36m0.7539\u001b[0m       0.9576    \u001b[35m0.6217\u001b[0m        \u001b[31m0.6213\u001b[0m        \u001b[94m0.0303\u001b[0m        \u001b[36m0.3860\u001b[0m     +  0.0010  9.4617\n",
            "     13  \u001b[36m0.7600\u001b[0m       0.9576    \u001b[35m0.6301\u001b[0m        \u001b[31m0.6204\u001b[0m        \u001b[94m0.0275\u001b[0m        \u001b[36m0.3849\u001b[0m     +  0.0010  9.1040\n",
            "     14  \u001b[36m0.7688\u001b[0m       0.9579    \u001b[35m0.6421\u001b[0m        \u001b[31m0.6198\u001b[0m        \u001b[94m0.0253\u001b[0m        \u001b[36m0.3841\u001b[0m     +  0.0010  8.6802\n",
            "     15  0.7659       0.9576    0.6382        0.6198        \u001b[94m0.0234\u001b[0m        0.3842        0.0001  8.4611\n",
            "     16  0.7641       0.9575    0.6357        0.6198        \u001b[94m0.0232\u001b[0m        0.3841        0.0001  9.2908\n",
            "     17  0.7679       0.9574    0.6411        \u001b[31m0.6197\u001b[0m        \u001b[94m0.0230\u001b[0m        \u001b[36m0.3840\u001b[0m     +  0.0001  8.2980\n",
            "     18  \u001b[36m0.7694\u001b[0m       0.9573    \u001b[35m0.6431\u001b[0m        \u001b[31m0.6197\u001b[0m        \u001b[94m0.0229\u001b[0m        \u001b[36m0.3840\u001b[0m     +  0.0001  8.9426\n",
            "     19  0.7674       0.9574    0.6403        \u001b[31m0.6196\u001b[0m        \u001b[94m0.0227\u001b[0m        \u001b[36m0.3839\u001b[0m     +  0.0001  9.0322\n",
            "     20  \u001b[36m0.7719\u001b[0m       0.9575    \u001b[35m0.6466\u001b[0m        \u001b[31m0.6196\u001b[0m        \u001b[94m0.0225\u001b[0m        \u001b[36m0.3839\u001b[0m     +  0.0001  9.3989\n",
            "     21  0.7644       0.9572    0.6362        \u001b[31m0.6196\u001b[0m        \u001b[94m0.0224\u001b[0m        \u001b[36m0.3839\u001b[0m     +  0.0001  7.9202\n",
            "     22  0.7670       0.9572    0.6399        \u001b[31m0.6195\u001b[0m        \u001b[94m0.0222\u001b[0m        \u001b[36m0.3838\u001b[0m     +  0.0000  9.2398\n",
            "     23  0.7692       0.9574    0.6428        \u001b[31m0.6195\u001b[0m        \u001b[94m0.0221\u001b[0m        \u001b[36m0.3838\u001b[0m     +  0.0000  9.0350\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 3/3; 3/6] END lr=0.01, module__dropout=0.3, module__linear_size=200, module__size_emb=120;, score=-1.104 total time= 3.7min\n",
            "[CV 1/3; 4/6] START lr=0.01, module__dropout=0.3, module__linear_size=400, module__size_emb=30\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss    cp      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ----  ------  ------\n",
            "      1  \u001b[36m0.5306\u001b[0m       \u001b[32m0.8271\u001b[0m    \u001b[35m0.3906\u001b[0m        \u001b[31m0.9752\u001b[0m        \u001b[94m0.9954\u001b[0m        \u001b[36m0.9510\u001b[0m     +  0.0100  7.5170\n",
            "      2  \u001b[36m0.5839\u001b[0m       \u001b[32m0.9132\u001b[0m    \u001b[35m0.4292\u001b[0m        \u001b[31m0.8419\u001b[0m        \u001b[94m0.7717\u001b[0m        \u001b[36m0.7088\u001b[0m     +  0.0100  8.4228\n",
            "      3  \u001b[36m0.6757\u001b[0m       \u001b[32m0.9188\u001b[0m    \u001b[35m0.5343\u001b[0m        \u001b[31m0.7750\u001b[0m        \u001b[94m0.4933\u001b[0m        \u001b[36m0.6006\u001b[0m     +  0.0100  8.2682\n",
            "      4  \u001b[36m0.7314\u001b[0m       0.9188    \u001b[35m0.6075\u001b[0m        \u001b[31m0.7552\u001b[0m        \u001b[94m0.3517\u001b[0m        \u001b[36m0.5703\u001b[0m     +  0.0100  7.4027\n",
            "      5  0.7288       \u001b[32m0.9234\u001b[0m    0.6020        \u001b[31m0.7387\u001b[0m        \u001b[94m0.2870\u001b[0m        \u001b[36m0.5457\u001b[0m     +  0.0100  7.9063\n",
            "      6  0.7202       \u001b[32m0.9322\u001b[0m    0.5867        \u001b[31m0.7299\u001b[0m        \u001b[94m0.2484\u001b[0m        \u001b[36m0.5328\u001b[0m     +  0.0100  8.6668\n",
            "      7  \u001b[36m0.7639\u001b[0m       0.9243    \u001b[35m0.6510\u001b[0m        0.7336        \u001b[94m0.2252\u001b[0m        0.5382        0.0100  7.0725\n",
            "      8  0.7293       0.9302    0.5997        \u001b[31m0.7238\u001b[0m        \u001b[94m0.1698\u001b[0m        \u001b[36m0.5238\u001b[0m     +  0.0010  8.5267\n",
            "      9  0.7299       0.9304    0.6005        \u001b[31m0.7208\u001b[0m        \u001b[94m0.1593\u001b[0m        \u001b[36m0.5196\u001b[0m     +  0.0010  7.9534\n",
            "     10  0.7384       0.9307    0.6120        \u001b[31m0.7190\u001b[0m        \u001b[94m0.1531\u001b[0m        \u001b[36m0.5170\u001b[0m     +  0.0010  7.2995\n",
            "     11  0.7252       \u001b[32m0.9322\u001b[0m    0.5934        \u001b[31m0.7174\u001b[0m        \u001b[94m0.1482\u001b[0m        \u001b[36m0.5147\u001b[0m     +  0.0010  8.7677\n",
            "     12  0.7358       \u001b[32m0.9323\u001b[0m    0.6077        \u001b[31m0.7168\u001b[0m        \u001b[94m0.1440\u001b[0m        \u001b[36m0.5139\u001b[0m     +  0.0010  7.3483\n",
            "     13  0.7477       0.9312    0.6245        \u001b[31m0.7164\u001b[0m        \u001b[94m0.1409\u001b[0m        \u001b[36m0.5132\u001b[0m     +  0.0010  8.3357\n",
            "     14  0.7293       \u001b[32m0.9325\u001b[0m    0.5987        \u001b[31m0.7152\u001b[0m        \u001b[94m0.1385\u001b[0m        \u001b[36m0.5115\u001b[0m     +  0.0010  14.0739\n",
            "     15  0.7406       0.9322    0.6144        \u001b[31m0.7152\u001b[0m        \u001b[94m0.1325\u001b[0m        \u001b[36m0.5115\u001b[0m     +  0.0001  8.7952\n",
            "     16  0.7408       0.9322    0.6145        \u001b[31m0.7151\u001b[0m        \u001b[94m0.1325\u001b[0m        \u001b[36m0.5114\u001b[0m     +  0.0001  7.2559\n",
            "     17  0.7431       0.9317    0.6180        \u001b[31m0.7151\u001b[0m        \u001b[94m0.1322\u001b[0m        \u001b[36m0.5113\u001b[0m     +  0.0001  8.6197\n",
            "     18  0.7407       0.9320    0.6145        \u001b[31m0.7150\u001b[0m        \u001b[94m0.1317\u001b[0m        \u001b[36m0.5112\u001b[0m     +  0.0001  8.2165\n",
            "     19  0.7413       0.9319    0.6154        \u001b[31m0.7149\u001b[0m        \u001b[94m0.1313\u001b[0m        \u001b[36m0.5111\u001b[0m     +  0.0001  7.6065\n",
            "     20  0.7427       0.9319    0.6173        \u001b[31m0.7149\u001b[0m        \u001b[94m0.1311\u001b[0m        \u001b[36m0.5111\u001b[0m     +  0.0001  8.2821\n",
            "     21  0.7424       0.9320    0.6170        \u001b[31m0.7149\u001b[0m        \u001b[94m0.1307\u001b[0m        \u001b[36m0.5110\u001b[0m     +  0.0001  8.6781\n",
            "     22  0.7424       0.9320    0.6169        \u001b[31m0.7148\u001b[0m        \u001b[94m0.1303\u001b[0m        \u001b[36m0.5110\u001b[0m     +  0.0000  7.0966\n",
            "     23  0.7422       0.9319    0.6166        \u001b[31m0.7148\u001b[0m        \u001b[94m0.1301\u001b[0m        \u001b[36m0.5110\u001b[0m     +  0.0000  8.6156\n",
            "     24  0.7423       0.9320    0.6168        \u001b[31m0.7148\u001b[0m        0.1302        \u001b[36m0.5110\u001b[0m     +  0.0000  7.9441\n",
            "     25  0.7423       0.9321    0.6167        \u001b[31m0.7148\u001b[0m        \u001b[94m0.1301\u001b[0m        \u001b[36m0.5110\u001b[0m     +  0.0000  7.6041\n",
            "     26  0.7421       0.9321    0.6165        \u001b[31m0.7148\u001b[0m        \u001b[94m0.1300\u001b[0m        \u001b[36m0.5110\u001b[0m     +  0.0000  8.1809\n",
            "     27  0.7423       0.9321    0.6167        \u001b[31m0.7148\u001b[0m        \u001b[94m0.1299\u001b[0m        \u001b[36m0.5110\u001b[0m     +  0.0000  8.9010\n",
            "     28  0.7423       0.9321    0.6168        \u001b[31m0.7148\u001b[0m        0.1299        \u001b[36m0.5109\u001b[0m     +  0.0000  7.3863\n",
            "     29  0.7424       0.9321    0.6169        \u001b[31m0.7148\u001b[0m        0.1300        \u001b[36m0.5109\u001b[0m     +  0.0000  8.7278\n",
            "     30  0.7423       0.9321    0.6168        \u001b[31m0.7148\u001b[0m        0.1300        \u001b[36m0.5109\u001b[0m     +  0.0000  8.2442\n",
            "     31  0.7423       0.9321    0.6168        \u001b[31m0.7148\u001b[0m        \u001b[94m0.1298\u001b[0m        \u001b[36m0.5109\u001b[0m     +  0.0000  7.0907\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 1/3; 4/6] END lr=0.01, module__dropout=0.3, module__linear_size=400, module__size_emb=30;, score=-1.265 total time= 4.9min\n",
            "[CV 2/3; 4/6] START lr=0.01, module__dropout=0.3, module__linear_size=400, module__size_emb=30\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss    cp      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ----  ------  ------\n",
            "      1  \u001b[36m0.3954\u001b[0m       \u001b[32m0.8722\u001b[0m    \u001b[35m0.2557\u001b[0m        \u001b[31m0.9451\u001b[0m        \u001b[94m1.0120\u001b[0m        \u001b[36m0.8933\u001b[0m     +  0.0100  8.6335\n",
            "      2  \u001b[36m0.5612\u001b[0m       \u001b[32m0.9235\u001b[0m    \u001b[35m0.4031\u001b[0m        \u001b[31m0.8086\u001b[0m        \u001b[94m0.7712\u001b[0m        \u001b[36m0.6538\u001b[0m     +  0.0100  7.0008\n",
            "      3  \u001b[36m0.7140\u001b[0m       0.9127    \u001b[35m0.5863\u001b[0m        \u001b[31m0.7361\u001b[0m        \u001b[94m0.4818\u001b[0m        \u001b[36m0.5418\u001b[0m     +  0.0100  8.3264\n",
            "      4  \u001b[36m0.7192\u001b[0m       \u001b[32m0.9243\u001b[0m    \u001b[35m0.5886\u001b[0m        \u001b[31m0.7069\u001b[0m        \u001b[94m0.3330\u001b[0m        \u001b[36m0.4997\u001b[0m     +  0.0100  8.3092\n",
            "      5  0.6650       \u001b[32m0.9385\u001b[0m    0.5150        \u001b[31m0.6997\u001b[0m        \u001b[94m0.2669\u001b[0m        \u001b[36m0.4896\u001b[0m     +  0.0100  7.6189\n",
            "      6  \u001b[36m0.7202\u001b[0m       0.9268    \u001b[35m0.5889\u001b[0m        \u001b[31m0.6898\u001b[0m        \u001b[94m0.2292\u001b[0m        \u001b[36m0.4758\u001b[0m     +  0.0100  8.1621\n",
            "      7  \u001b[36m0.7266\u001b[0m       0.9282    \u001b[35m0.5970\u001b[0m        \u001b[31m0.6847\u001b[0m        \u001b[94m0.2057\u001b[0m        \u001b[36m0.4688\u001b[0m     +  0.0100  8.5804\n",
            "      8  0.7221       0.9281    0.5910        \u001b[31m0.6809\u001b[0m        \u001b[94m0.1540\u001b[0m        \u001b[36m0.4636\u001b[0m     +  0.0010  7.4174\n",
            "      9  0.7263       0.9300    0.5958        \u001b[31m0.6782\u001b[0m        \u001b[94m0.1443\u001b[0m        \u001b[36m0.4600\u001b[0m     +  0.0010  8.2129\n",
            "     10  \u001b[36m0.7361\u001b[0m       0.9285    \u001b[35m0.6097\u001b[0m        \u001b[31m0.6769\u001b[0m        \u001b[94m0.1382\u001b[0m        \u001b[36m0.4582\u001b[0m     +  0.0010  8.1525\n",
            "     11  \u001b[36m0.7383\u001b[0m       0.9299    \u001b[35m0.6121\u001b[0m        \u001b[31m0.6758\u001b[0m        \u001b[94m0.1336\u001b[0m        \u001b[36m0.4567\u001b[0m     +  0.0010  7.1490\n",
            "     12  0.7333       0.9310    0.6049        \u001b[31m0.6751\u001b[0m        \u001b[94m0.1299\u001b[0m        \u001b[36m0.4557\u001b[0m     +  0.0010  8.9571\n",
            "     13  0.7310       0.9309    0.6018        \u001b[31m0.6750\u001b[0m        \u001b[94m0.1270\u001b[0m        \u001b[36m0.4556\u001b[0m     +  0.0010  7.8597\n",
            "     14  \u001b[36m0.7446\u001b[0m       0.9295    \u001b[35m0.6210\u001b[0m        \u001b[31m0.6747\u001b[0m        \u001b[94m0.1247\u001b[0m        \u001b[36m0.4552\u001b[0m     +  0.0010  7.2997\n",
            "     15  0.7359       0.9302    0.6088        \u001b[31m0.6746\u001b[0m        \u001b[94m0.1196\u001b[0m        \u001b[36m0.4551\u001b[0m     +  0.0001  8.3626\n",
            "     16  0.7352       0.9305    0.6077        \u001b[31m0.6746\u001b[0m        \u001b[94m0.1195\u001b[0m        \u001b[36m0.4551\u001b[0m     +  0.0001  7.7027\n",
            "     17  0.7373       0.9302    0.6106        \u001b[31m0.6745\u001b[0m        \u001b[94m0.1189\u001b[0m        \u001b[36m0.4550\u001b[0m     +  0.0001  7.7006\n",
            "     18  0.7386       0.9305    0.6124        \u001b[31m0.6744\u001b[0m        \u001b[94m0.1188\u001b[0m        \u001b[36m0.4549\u001b[0m     +  0.0001  8.6407\n",
            "     19  0.7363       0.9308    0.6090        \u001b[31m0.6744\u001b[0m        \u001b[94m0.1186\u001b[0m        \u001b[36m0.4549\u001b[0m     +  0.0001  7.2591\n",
            "     20  0.7331       0.9308    0.6047        0.6746        0.1186        0.4550        0.0001  8.1053\n",
            "     21  0.7354       0.9308    0.6078        \u001b[31m0.6744\u001b[0m        \u001b[94m0.1182\u001b[0m        \u001b[36m0.4548\u001b[0m     +  0.0001  8.6738\n",
            "     22  0.7359       0.9307    0.6085        \u001b[31m0.6744\u001b[0m        \u001b[94m0.1175\u001b[0m        \u001b[36m0.4548\u001b[0m     +  0.0000  7.0315\n",
            "     23  0.7366       0.9308    0.6094        \u001b[31m0.6744\u001b[0m        0.1175        \u001b[36m0.4548\u001b[0m     +  0.0000  9.1724\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 2/3; 4/6] END lr=0.01, module__dropout=0.3, module__linear_size=400, module__size_emb=30;, score=-1.127 total time= 3.6min\n",
            "[CV 3/3; 4/6] START lr=0.01, module__dropout=0.3, module__linear_size=400, module__size_emb=30\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss    cp      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ----  ------  ------\n",
            "      1  \u001b[36m0.5508\u001b[0m       \u001b[32m0.8266\u001b[0m    \u001b[35m0.4130\u001b[0m        \u001b[31m0.9461\u001b[0m        \u001b[94m0.9914\u001b[0m        \u001b[36m0.8951\u001b[0m     +  0.0100  7.5416\n",
            "      2  \u001b[36m0.6271\u001b[0m       \u001b[32m0.8858\u001b[0m    \u001b[35m0.4853\u001b[0m        \u001b[31m0.8262\u001b[0m        \u001b[94m0.7476\u001b[0m        \u001b[36m0.6826\u001b[0m     +  0.0100  7.9104\n",
            "      3  \u001b[36m0.6833\u001b[0m       \u001b[32m0.9173\u001b[0m    \u001b[35m0.5444\u001b[0m        \u001b[31m0.7587\u001b[0m        \u001b[94m0.4999\u001b[0m        \u001b[36m0.5756\u001b[0m     +  0.0100  8.2598\n",
            "      4  0.6743       \u001b[32m0.9313\u001b[0m    0.5285        \u001b[31m0.7280\u001b[0m        \u001b[94m0.3582\u001b[0m        \u001b[36m0.5300\u001b[0m     +  0.0100  7.6566\n",
            "      5  \u001b[36m0.7403\u001b[0m       0.9207    \u001b[35m0.6190\u001b[0m        \u001b[31m0.7184\u001b[0m        \u001b[94m0.2851\u001b[0m        \u001b[36m0.5161\u001b[0m     +  0.0100  7.9364\n",
            "      6  0.7242       \u001b[32m0.9313\u001b[0m    0.5924        \u001b[31m0.7064\u001b[0m        \u001b[94m0.2467\u001b[0m        \u001b[36m0.4990\u001b[0m     +  0.0100  8.6438\n",
            "      7  0.6828       \u001b[32m0.9387\u001b[0m    0.5366        0.7073        \u001b[94m0.2231\u001b[0m        0.5003        0.0100  7.3790\n",
            "      8  0.7178       0.9312    0.5839        \u001b[31m0.7003\u001b[0m        \u001b[94m0.1689\u001b[0m        \u001b[36m0.4904\u001b[0m     +  0.0010  8.2290\n",
            "      9  0.7147       0.9338    0.5789        \u001b[31m0.6976\u001b[0m        \u001b[94m0.1581\u001b[0m        \u001b[36m0.4867\u001b[0m     +  0.0010  8.4067\n",
            "     10  0.7080       0.9360    0.5693        \u001b[31m0.6973\u001b[0m        \u001b[94m0.1515\u001b[0m        \u001b[36m0.4862\u001b[0m     +  0.0010  7.1354\n",
            "     11  0.7169       0.9357    0.5810        \u001b[31m0.6957\u001b[0m        \u001b[94m0.1469\u001b[0m        \u001b[36m0.4840\u001b[0m     +  0.0010  8.7958\n",
            "     12  0.7219       0.9360    0.5875        \u001b[31m0.6946\u001b[0m        \u001b[94m0.1433\u001b[0m        \u001b[36m0.4825\u001b[0m     +  0.0010  7.9200\n",
            "     13  0.7284       0.9349    0.5966        0.6948        \u001b[94m0.1403\u001b[0m        0.4827        0.0010  7.9030\n",
            "     14  0.7228       0.9351    0.5891        \u001b[31m0.6943\u001b[0m        \u001b[94m0.1375\u001b[0m        \u001b[36m0.4821\u001b[0m     +  0.0010  8.7466\n",
            "     15  0.7275       0.9355    0.5952        \u001b[31m0.6942\u001b[0m        \u001b[94m0.1322\u001b[0m        \u001b[36m0.4819\u001b[0m     +  0.0001  7.7433\n",
            "     16  0.7257       0.9357    0.5927        \u001b[31m0.6941\u001b[0m        \u001b[94m0.1320\u001b[0m        \u001b[36m0.4818\u001b[0m     +  0.0001  7.9198\n",
            "     17  0.7270       0.9354    0.5946        \u001b[31m0.6941\u001b[0m        \u001b[94m0.1315\u001b[0m        \u001b[36m0.4818\u001b[0m     +  0.0001  8.1151\n",
            "     18  0.7261       0.9357    0.5932        \u001b[31m0.6941\u001b[0m        \u001b[94m0.1313\u001b[0m        \u001b[36m0.4817\u001b[0m     +  0.0001  7.9514\n",
            "     19  0.7276       0.9357    0.5952        \u001b[31m0.6940\u001b[0m        \u001b[94m0.1310\u001b[0m        \u001b[36m0.4816\u001b[0m     +  0.0001  7.7857\n",
            "     20  0.7270       0.9354    0.5945        0.6940        \u001b[94m0.1308\u001b[0m        0.4817        0.0001  8.4543\n",
            "     21  0.7274       0.9354    0.5950        \u001b[31m0.6940\u001b[0m        \u001b[94m0.1304\u001b[0m        \u001b[36m0.4816\u001b[0m     +  0.0001  6.7200\n",
            "     22  0.7272       0.9354    0.5949        0.6940        \u001b[94m0.1298\u001b[0m        0.4816        0.0000  8.5401\n",
            "     23  0.7271       0.9354    0.5947        0.6940        0.1301        0.4816        0.0000  7.6201\n",
            "     24  0.7272       0.9355    0.5948        0.6940        0.1298        0.4816        0.0000  8.0303\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 3/3; 4/6] END lr=0.01, module__dropout=0.3, module__linear_size=400, module__size_emb=30;, score=-1.199 total time= 3.6min\n",
            "[CV 1/3; 5/6] START lr=0.01, module__dropout=0.3, module__linear_size=400, module__size_emb=60\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss    cp      lr      dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ----  ------  -------\n",
            "      1  \u001b[36m0.4665\u001b[0m       \u001b[32m0.8476\u001b[0m    \u001b[35m0.3218\u001b[0m        \u001b[31m0.9634\u001b[0m        \u001b[94m0.9995\u001b[0m        \u001b[36m0.9281\u001b[0m     +  0.0100  15.3575\n",
            "      2  \u001b[36m0.6244\u001b[0m       \u001b[32m0.9342\u001b[0m    \u001b[35m0.4689\u001b[0m        \u001b[31m0.7929\u001b[0m        \u001b[94m0.7175\u001b[0m        \u001b[36m0.6286\u001b[0m     +  0.0100  9.6747\n",
            "      3  \u001b[36m0.7096\u001b[0m       \u001b[32m0.9457\u001b[0m    \u001b[35m0.5678\u001b[0m        \u001b[31m0.7289\u001b[0m        \u001b[94m0.3704\u001b[0m        \u001b[36m0.5312\u001b[0m     +  0.0100  8.0849\n",
            "      4  \u001b[36m0.7707\u001b[0m       0.9350    \u001b[35m0.6555\u001b[0m        \u001b[31m0.7086\u001b[0m        \u001b[94m0.2234\u001b[0m        \u001b[36m0.5021\u001b[0m     +  0.0100  9.0347\n",
            "      5  0.7562       0.9449    0.6303        \u001b[31m0.6930\u001b[0m        \u001b[94m0.1635\u001b[0m        \u001b[36m0.4802\u001b[0m     +  0.0100  9.1072\n",
            "      6  \u001b[36m0.7707\u001b[0m       0.9440    0.6512        \u001b[31m0.6890\u001b[0m        \u001b[94m0.1344\u001b[0m        \u001b[36m0.4747\u001b[0m     +  0.0100  8.9133\n",
            "      7  0.7180       \u001b[32m0.9505\u001b[0m    0.5769        \u001b[31m0.6853\u001b[0m        \u001b[94m0.1172\u001b[0m        \u001b[36m0.4696\u001b[0m     +  0.0100  8.0804\n",
            "      8  0.7319       \u001b[32m0.9512\u001b[0m    0.5948        \u001b[31m0.6773\u001b[0m        \u001b[94m0.0879\u001b[0m        \u001b[36m0.4587\u001b[0m     +  0.0010  9.0324\n",
            "      9  0.7325       \u001b[32m0.9513\u001b[0m    0.5956        \u001b[31m0.6728\u001b[0m        \u001b[94m0.0749\u001b[0m        \u001b[36m0.4526\u001b[0m     +  0.0010  9.4597\n",
            "     10  0.7490       0.9503    0.6181        \u001b[31m0.6704\u001b[0m        \u001b[94m0.0666\u001b[0m        \u001b[36m0.4495\u001b[0m     +  0.0010  8.1941\n",
            "     11  0.7526       0.9498    0.6232        \u001b[31m0.6688\u001b[0m        \u001b[94m0.0613\u001b[0m        \u001b[36m0.4474\u001b[0m     +  0.0010  9.1171\n",
            "     12  0.7527       0.9504    0.6231        \u001b[31m0.6677\u001b[0m        \u001b[94m0.0573\u001b[0m        \u001b[36m0.4458\u001b[0m     +  0.0010  9.1129\n",
            "     13  0.7627       0.9501    0.6371        \u001b[31m0.6672\u001b[0m        \u001b[94m0.0543\u001b[0m        \u001b[36m0.4452\u001b[0m     +  0.0010  9.5202\n",
            "     14  0.7604       0.9502    0.6338        \u001b[31m0.6664\u001b[0m        \u001b[94m0.0520\u001b[0m        \u001b[36m0.4440\u001b[0m     +  0.0010  8.0352\n",
            "     15  0.7577       0.9501    0.6301        \u001b[31m0.6663\u001b[0m        \u001b[94m0.0490\u001b[0m        \u001b[36m0.4439\u001b[0m     +  0.0001  9.5666\n",
            "     16  0.7577       0.9502    0.6301        \u001b[31m0.6663\u001b[0m        \u001b[94m0.0488\u001b[0m        \u001b[36m0.4439\u001b[0m     +  0.0001  9.1674\n",
            "     17  0.7610       0.9501    0.6347        0.6663        \u001b[94m0.0484\u001b[0m        0.4439        0.0001  9.0956\n",
            "     18  0.7626       0.9502    0.6369        0.6663        \u001b[94m0.0484\u001b[0m        0.4439        0.0001  8.3465\n",
            "     19  0.7603       0.9506    0.6335        \u001b[31m0.6662\u001b[0m        \u001b[94m0.0481\u001b[0m        \u001b[36m0.4438\u001b[0m     +  0.0001  9.2853\n",
            "     20  0.7618       0.9508    0.6355        \u001b[31m0.6662\u001b[0m        \u001b[94m0.0479\u001b[0m        \u001b[36m0.4438\u001b[0m     +  0.0001  9.0981\n",
            "     21  0.7607       0.9506    0.6340        \u001b[31m0.6661\u001b[0m        \u001b[94m0.0478\u001b[0m        \u001b[36m0.4437\u001b[0m     +  0.0001  8.5283\n",
            "     22  0.7608       0.9507    0.6341        \u001b[31m0.6661\u001b[0m        \u001b[94m0.0474\u001b[0m        \u001b[36m0.4437\u001b[0m     +  0.0000  9.7928\n",
            "     23  0.7610       0.9506    0.6345        0.6661        \u001b[94m0.0473\u001b[0m        0.4437        0.0000  9.2139\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 1/3; 5/6] END lr=0.01, module__dropout=0.3, module__linear_size=400, module__size_emb=60;, score=-1.213 total time= 4.0min\n",
            "[CV 2/3; 5/6] START lr=0.01, module__dropout=0.3, module__linear_size=400, module__size_emb=60\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss    cp      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ----  ------  ------\n",
            "      1  \u001b[36m0.4855\u001b[0m       \u001b[32m0.8604\u001b[0m    \u001b[35m0.3382\u001b[0m        \u001b[31m0.9348\u001b[0m        \u001b[94m1.0087\u001b[0m        \u001b[36m0.8739\u001b[0m     +  0.0100  8.3783\n",
            "      2  \u001b[36m0.6989\u001b[0m       \u001b[32m0.9051\u001b[0m    \u001b[35m0.5692\u001b[0m        \u001b[31m0.7552\u001b[0m        \u001b[94m0.7093\u001b[0m        \u001b[36m0.5703\u001b[0m     +  0.0100  9.0217\n",
            "      3  \u001b[36m0.7593\u001b[0m       \u001b[32m0.9239\u001b[0m    \u001b[35m0.6444\u001b[0m        \u001b[31m0.6925\u001b[0m        \u001b[94m0.3542\u001b[0m        \u001b[36m0.4795\u001b[0m     +  0.0100  9.3671\n",
            "      4  0.7571       \u001b[32m0.9405\u001b[0m    0.6335        \u001b[31m0.6615\u001b[0m        \u001b[94m0.2111\u001b[0m        \u001b[36m0.4376\u001b[0m     +  0.0100  8.6392\n",
            "      5  0.7126       \u001b[32m0.9527\u001b[0m    0.5691        \u001b[31m0.6510\u001b[0m        \u001b[94m0.1541\u001b[0m        \u001b[36m0.4238\u001b[0m     +  0.0100  8.7238\n",
            "      6  0.7583       0.9400    0.6354        \u001b[31m0.6465\u001b[0m        \u001b[94m0.1263\u001b[0m        \u001b[36m0.4180\u001b[0m     +  0.0100  9.1278\n",
            "      7  \u001b[36m0.7811\u001b[0m       0.9387    \u001b[35m0.6688\u001b[0m        \u001b[31m0.6451\u001b[0m        \u001b[94m0.1090\u001b[0m        \u001b[36m0.4162\u001b[0m     +  0.0100  9.5195\n",
            "      8  0.7320       0.9467    0.5967        \u001b[31m0.6361\u001b[0m        \u001b[94m0.0826\u001b[0m        \u001b[36m0.4046\u001b[0m     +  0.0010  7.8229\n",
            "      9  0.7511       0.9464    0.6227        \u001b[31m0.6316\u001b[0m        \u001b[94m0.0694\u001b[0m        \u001b[36m0.3989\u001b[0m     +  0.0010  9.0722\n",
            "     10  0.7504       0.9469    0.6214        \u001b[31m0.6291\u001b[0m        \u001b[94m0.0614\u001b[0m        \u001b[36m0.3958\u001b[0m     +  0.0010  9.0794\n",
            "     11  0.7592       0.9470    0.6336        \u001b[31m0.6277\u001b[0m        \u001b[94m0.0560\u001b[0m        \u001b[36m0.3941\u001b[0m     +  0.0010  9.4192\n",
            "     12  0.7554       0.9466    0.6284        \u001b[31m0.6270\u001b[0m        \u001b[94m0.0521\u001b[0m        \u001b[36m0.3932\u001b[0m     +  0.0010  7.9719\n",
            "     13  0.7546       0.9465    0.6274        \u001b[31m0.6263\u001b[0m        \u001b[94m0.0493\u001b[0m        \u001b[36m0.3923\u001b[0m     +  0.0010  9.5583\n",
            "     14  0.7493       0.9463    0.6201        \u001b[31m0.6262\u001b[0m        \u001b[94m0.0470\u001b[0m        \u001b[36m0.3921\u001b[0m     +  0.0010  9.0373\n",
            "     15  0.7580       0.9460    0.6324        \u001b[31m0.6261\u001b[0m        \u001b[94m0.0442\u001b[0m        \u001b[36m0.3920\u001b[0m     +  0.0001  8.9263\n",
            "     16  0.7598       0.9459    0.6348        \u001b[31m0.6260\u001b[0m        \u001b[94m0.0440\u001b[0m        \u001b[36m0.3919\u001b[0m     +  0.0001  8.7265\n",
            "     17  0.7575       0.9458    0.6318        0.6260        \u001b[94m0.0439\u001b[0m        0.3919        0.0001  9.0892\n",
            "     18  0.7593       0.9458    0.6342        \u001b[31m0.6259\u001b[0m        \u001b[94m0.0436\u001b[0m        \u001b[36m0.3918\u001b[0m     +  0.0001  9.4335\n",
            "     19  0.7596       0.9458    0.6347        \u001b[31m0.6259\u001b[0m        \u001b[94m0.0435\u001b[0m        \u001b[36m0.3918\u001b[0m     +  0.0001  7.9384\n",
            "     20  0.7598       0.9458    0.6349        \u001b[31m0.6259\u001b[0m        \u001b[94m0.0434\u001b[0m        \u001b[36m0.3917\u001b[0m     +  0.0001  9.4236\n",
            "     21  0.7619       0.9461    0.6378        \u001b[31m0.6258\u001b[0m        \u001b[94m0.0430\u001b[0m        \u001b[36m0.3917\u001b[0m     +  0.0001  9.1799\n",
            "     22  0.7604       0.9460    0.6357        0.6259        \u001b[94m0.0427\u001b[0m        0.3917        0.0000  9.1910\n",
            "     23  0.7603       0.9460    0.6356        0.6259        \u001b[94m0.0427\u001b[0m        0.3917        0.0000  8.3460\n",
            "     24  0.7594       0.9459    0.6343        0.6259        0.0427        0.3917        0.0000  9.5247\n",
            "     25  0.7595       0.9459    0.6345        0.6259        0.0428        0.3917        0.0000  8.7112\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 2/3; 5/6] END lr=0.01, module__dropout=0.3, module__linear_size=400, module__size_emb=60;, score=-1.087 total time= 4.2min\n",
            "[CV 3/3; 5/6] START lr=0.01, module__dropout=0.3, module__linear_size=400, module__size_emb=60\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss    cp      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ----  ------  ------\n",
            "      1  \u001b[36m0.5065\u001b[0m       \u001b[32m0.8445\u001b[0m    \u001b[35m0.3617\u001b[0m        \u001b[31m0.9368\u001b[0m        \u001b[94m0.9861\u001b[0m        \u001b[36m0.8776\u001b[0m     +  0.0100  9.3136\n",
            "      2  \u001b[36m0.6173\u001b[0m       \u001b[32m0.9323\u001b[0m    \u001b[35m0.4614\u001b[0m        \u001b[31m0.7701\u001b[0m        \u001b[94m0.6990\u001b[0m        \u001b[36m0.5930\u001b[0m     +  0.0100  9.2971\n",
            "      3  \u001b[36m0.6923\u001b[0m       \u001b[32m0.9389\u001b[0m    \u001b[35m0.5483\u001b[0m        \u001b[31m0.7039\u001b[0m        \u001b[94m0.3640\u001b[0m        \u001b[36m0.4955\u001b[0m     +  0.0100  9.4603\n",
            "      4  \u001b[36m0.7018\u001b[0m       \u001b[32m0.9464\u001b[0m    \u001b[35m0.5577\u001b[0m        \u001b[31m0.6782\u001b[0m        \u001b[94m0.2220\u001b[0m        \u001b[36m0.4599\u001b[0m     +  0.0100  8.3501\n",
            "      5  \u001b[36m0.7239\u001b[0m       0.9450    \u001b[35m0.5866\u001b[0m        \u001b[31m0.6661\u001b[0m        \u001b[94m0.1628\u001b[0m        \u001b[36m0.4437\u001b[0m     +  0.0100  9.0060\n",
            "      6  \u001b[36m0.7362\u001b[0m       0.9434    \u001b[35m0.6036\u001b[0m        \u001b[31m0.6599\u001b[0m        \u001b[94m0.1322\u001b[0m        \u001b[36m0.4355\u001b[0m     +  0.0100  9.4037\n",
            "      7  0.7291       0.9435    0.5941        \u001b[31m0.6581\u001b[0m        \u001b[94m0.1145\u001b[0m        \u001b[36m0.4331\u001b[0m     +  0.0100  8.5505\n",
            "      8  0.7194       0.9463    0.5803        \u001b[31m0.6514\u001b[0m        \u001b[94m0.0852\u001b[0m        \u001b[36m0.4243\u001b[0m     +  0.0010  8.6046\n",
            "      9  0.7231       0.9459    0.5852        \u001b[31m0.6474\u001b[0m        \u001b[94m0.0732\u001b[0m        \u001b[36m0.4191\u001b[0m     +  0.0010  8.9919\n",
            "     10  0.7290       0.9460    0.5930        \u001b[31m0.6453\u001b[0m        \u001b[94m0.0653\u001b[0m        \u001b[36m0.4164\u001b[0m     +  0.0010  10.1627\n",
            "     11  \u001b[36m0.7386\u001b[0m       \u001b[32m0.9469\u001b[0m    \u001b[35m0.6054\u001b[0m        \u001b[31m0.6436\u001b[0m        \u001b[94m0.0599\u001b[0m        \u001b[36m0.4143\u001b[0m     +  0.0010  8.5119\n",
            "     12  \u001b[36m0.7487\u001b[0m       0.9464    \u001b[35m0.6193\u001b[0m        \u001b[31m0.6431\u001b[0m        \u001b[94m0.0560\u001b[0m        \u001b[36m0.4136\u001b[0m     +  0.0010  15.9241\n",
            "     13  0.7442       0.9469    0.6130        \u001b[31m0.6425\u001b[0m        \u001b[94m0.0535\u001b[0m        \u001b[36m0.4129\u001b[0m     +  0.0010  7.9820\n",
            "     14  0.7329       0.9455    0.5983        0.6428        \u001b[94m0.0512\u001b[0m        0.4132        0.0010  9.3970\n",
            "     15  0.7425       0.9462    0.6109        \u001b[31m0.6423\u001b[0m        \u001b[94m0.0484\u001b[0m        \u001b[36m0.4126\u001b[0m     +  0.0001  9.1898\n",
            "     16  0.7432       0.9461    0.6119        \u001b[31m0.6423\u001b[0m        \u001b[94m0.0482\u001b[0m        \u001b[36m0.4125\u001b[0m     +  0.0001  8.9091\n",
            "     17  0.7448       0.9462    0.6141        \u001b[31m0.6422\u001b[0m        \u001b[94m0.0479\u001b[0m        \u001b[36m0.4124\u001b[0m     +  0.0001  8.2780\n",
            "     18  0.7447       0.9461    0.6141        0.6422        \u001b[94m0.0478\u001b[0m        0.4124        0.0001  9.5882\n",
            "     19  0.7442       0.9460    0.6134        \u001b[31m0.6421\u001b[0m        \u001b[94m0.0476\u001b[0m        \u001b[36m0.4123\u001b[0m     +  0.0001  9.0013\n",
            "     20  0.7469       0.9460    0.6170        0.6422        \u001b[94m0.0474\u001b[0m        0.4124        0.0001  8.3389\n",
            "     21  0.7479       0.9461    0.6183        0.6422        \u001b[94m0.0472\u001b[0m        0.4124        0.0001  8.9282\n",
            "     22  0.7466       0.9461    0.6165        0.6422        \u001b[94m0.0468\u001b[0m        0.4124        0.0000  9.6341\n",
            "     23  0.7463       0.9461    0.6162        0.6422        \u001b[94m0.0467\u001b[0m        0.4124        0.0000  8.3607\n",
            "     24  0.7461       0.9461    0.6160        0.6422        \u001b[94m0.0467\u001b[0m        0.4124        0.0000  9.2429\n",
            "     25  0.7461       0.9461    0.6159        0.6422        0.0467        0.4124        0.0000  9.5162\n",
            "     26  0.7461       0.9462    0.6159        0.6422        0.0468        0.4124        0.0000  7.9264\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 3/3; 5/6] END lr=0.01, module__dropout=0.3, module__linear_size=400, module__size_emb=60;, score=-1.151 total time= 4.5min\n",
            "[CV 1/3; 6/6] START lr=0.01, module__dropout=0.3, module__linear_size=400, module__size_emb=120\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss    cp      lr      dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ----  ------  -------\n",
            "      1  \u001b[36m0.5682\u001b[0m       \u001b[32m0.8243\u001b[0m    \u001b[35m0.4335\u001b[0m        \u001b[31m0.9577\u001b[0m        \u001b[94m0.9955\u001b[0m        \u001b[36m0.9172\u001b[0m     +  0.0100  10.6827\n",
            "      2  \u001b[36m0.6712\u001b[0m       \u001b[32m0.9486\u001b[0m    \u001b[35m0.5193\u001b[0m        \u001b[31m0.7511\u001b[0m        \u001b[94m0.6659\u001b[0m        \u001b[36m0.5642\u001b[0m     +  0.0100  10.9547\n",
            "      3  \u001b[36m0.7677\u001b[0m       0.9434    \u001b[35m0.6471\u001b[0m        \u001b[31m0.7032\u001b[0m        \u001b[94m0.2764\u001b[0m        \u001b[36m0.4944\u001b[0m     +  0.0100  11.1565\n",
            "      4  0.7532       \u001b[32m0.9541\u001b[0m    0.6222        \u001b[31m0.6771\u001b[0m        \u001b[94m0.1481\u001b[0m        \u001b[36m0.4584\u001b[0m     +  0.0100  10.6750\n",
            "      5  0.7306       \u001b[32m0.9576\u001b[0m    0.5906        \u001b[31m0.6706\u001b[0m        \u001b[94m0.1057\u001b[0m        \u001b[36m0.4497\u001b[0m     +  0.0100  10.7943\n",
            "      6  0.7305       \u001b[32m0.9603\u001b[0m    0.5894        \u001b[31m0.6661\u001b[0m        \u001b[94m0.0892\u001b[0m        \u001b[36m0.4437\u001b[0m     +  0.0100  9.5780\n",
            "      7  0.7573       0.9548    0.6275        \u001b[31m0.6653\u001b[0m        \u001b[94m0.0804\u001b[0m        \u001b[36m0.4427\u001b[0m     +  0.0100  10.8305\n",
            "      8  0.7464       0.9598    0.6106        \u001b[31m0.6559\u001b[0m        \u001b[94m0.0648\u001b[0m        \u001b[36m0.4302\u001b[0m     +  0.0010  10.7167\n",
            "      9  0.7520       0.9597    0.6182        \u001b[31m0.6507\u001b[0m        \u001b[94m0.0494\u001b[0m        \u001b[36m0.4234\u001b[0m     +  0.0010  11.1767\n",
            "     10  0.7605       0.9586    0.6303        \u001b[31m0.6475\u001b[0m        \u001b[94m0.0398\u001b[0m        \u001b[36m0.4192\u001b[0m     +  0.0010  10.7560\n",
            "     11  0.7629       0.9590    0.6334        \u001b[31m0.6454\u001b[0m        \u001b[94m0.0335\u001b[0m        \u001b[36m0.4165\u001b[0m     +  0.0010  11.1760\n",
            "     12  \u001b[36m0.7695\u001b[0m       0.9585    0.6427        \u001b[31m0.6439\u001b[0m        \u001b[94m0.0291\u001b[0m        \u001b[36m0.4147\u001b[0m     +  0.0010  10.8305\n",
            "     13  \u001b[36m0.7746\u001b[0m       0.9586    \u001b[35m0.6498\u001b[0m        \u001b[31m0.6431\u001b[0m        \u001b[94m0.0261\u001b[0m        \u001b[36m0.4136\u001b[0m     +  0.0010  10.6713\n",
            "     14  \u001b[36m0.7853\u001b[0m       0.9585    \u001b[35m0.6651\u001b[0m        \u001b[31m0.6426\u001b[0m        \u001b[94m0.0240\u001b[0m        \u001b[36m0.4129\u001b[0m     +  0.0010  9.9948\n",
            "     15  \u001b[36m0.7896\u001b[0m       0.9583    \u001b[35m0.6715\u001b[0m        0.6426        \u001b[94m0.0220\u001b[0m        0.4129        0.0001  11.1496\n",
            "     16  0.7857       0.9582    0.6658        \u001b[31m0.6424\u001b[0m        \u001b[94m0.0219\u001b[0m        \u001b[36m0.4127\u001b[0m     +  0.0001  10.8052\n",
            "     17  0.7880       0.9581    0.6691        \u001b[31m0.6424\u001b[0m        \u001b[94m0.0216\u001b[0m        \u001b[36m0.4127\u001b[0m     +  0.0001  11.1588\n",
            "     18  0.7884       0.9582    0.6697        \u001b[31m0.6424\u001b[0m        \u001b[94m0.0215\u001b[0m        \u001b[36m0.4126\u001b[0m     +  0.0001  10.7219\n",
            "     19  0.7893       0.9580    0.6711        \u001b[31m0.6423\u001b[0m        \u001b[94m0.0213\u001b[0m        \u001b[36m0.4126\u001b[0m     +  0.0001  11.2448\n",
            "     20  \u001b[36m0.7913\u001b[0m       0.9581    \u001b[35m0.6740\u001b[0m        \u001b[31m0.6423\u001b[0m        \u001b[94m0.0212\u001b[0m        \u001b[36m0.4125\u001b[0m     +  0.0001  10.3173\n",
            "     21  0.7911       0.9579    0.6737        \u001b[31m0.6422\u001b[0m        \u001b[94m0.0210\u001b[0m        \u001b[36m0.4124\u001b[0m     +  0.0001  10.3691\n",
            "     22  0.7905       0.9580    0.6728        \u001b[31m0.6422\u001b[0m        \u001b[94m0.0208\u001b[0m        \u001b[36m0.4124\u001b[0m     +  0.0000  10.5437\n",
            "     23  0.7898       0.9580    0.6718        \u001b[31m0.6422\u001b[0m        \u001b[94m0.0207\u001b[0m        \u001b[36m0.4124\u001b[0m     +  0.0000  11.2411\n",
            "     24  0.7902       0.9580    0.6724        \u001b[31m0.6422\u001b[0m        \u001b[94m0.0207\u001b[0m        \u001b[36m0.4124\u001b[0m     +  0.0000  10.7119\n",
            "     25  0.7900       0.9579    0.6721        \u001b[31m0.6421\u001b[0m        0.0207        \u001b[36m0.4124\u001b[0m     +  0.0000  11.2485\n",
            "     26  0.7902       0.9580    0.6725        \u001b[31m0.6421\u001b[0m        \u001b[94m0.0207\u001b[0m        \u001b[36m0.4123\u001b[0m     +  0.0000  10.8169\n",
            "     27  0.7906       0.9579    0.6731        \u001b[31m0.6421\u001b[0m        \u001b[94m0.0207\u001b[0m        \u001b[36m0.4123\u001b[0m     +  0.0000  11.5032\n",
            "     28  0.7911       0.9579    0.6737        \u001b[31m0.6421\u001b[0m        \u001b[94m0.0206\u001b[0m        \u001b[36m0.4123\u001b[0m     +  0.0000  9.8417\n",
            "     29  0.7911       0.9579    0.6738        0.6421        0.0207        0.4123        0.0000  10.6122\n",
            "     30  0.7911       0.9579    0.6738        \u001b[31m0.6421\u001b[0m        0.0207        \u001b[36m0.4123\u001b[0m     +  0.0000  11.1190\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 1/3; 6/6] END lr=0.01, module__dropout=0.3, module__linear_size=400, module__size_emb=120;, score=-1.179 total time= 6.1min\n",
            "[CV 2/3; 6/6] START lr=0.01, module__dropout=0.3, module__linear_size=400, module__size_emb=120\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss    cp      lr      dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ----  ------  -------\n",
            "      1  \u001b[36m0.2327\u001b[0m       \u001b[32m0.9282\u001b[0m    \u001b[35m0.1330\u001b[0m        \u001b[31m0.9439\u001b[0m        \u001b[94m1.0160\u001b[0m        \u001b[36m0.8910\u001b[0m     +  0.0100  10.9659\n",
            "      2  \u001b[36m0.7086\u001b[0m       0.9269    \u001b[35m0.5736\u001b[0m        \u001b[31m0.7233\u001b[0m        \u001b[94m0.6599\u001b[0m        \u001b[36m0.5231\u001b[0m     +  0.0100  10.9819\n",
            "      3  \u001b[36m0.7335\u001b[0m       \u001b[32m0.9444\u001b[0m    \u001b[35m0.5996\u001b[0m        \u001b[31m0.6612\u001b[0m        \u001b[94m0.2704\u001b[0m        \u001b[36m0.4371\u001b[0m     +  0.0100  10.6640\n",
            "      4  0.6983       \u001b[32m0.9572\u001b[0m    0.5496        \u001b[31m0.6393\u001b[0m        \u001b[94m0.1435\u001b[0m        \u001b[36m0.4087\u001b[0m     +  0.0100  10.8521\n",
            "      5  0.7059       0.9538    0.5603        \u001b[31m0.6338\u001b[0m        \u001b[94m0.1011\u001b[0m        \u001b[36m0.4018\u001b[0m     +  0.0100  9.4611\n",
            "      6  \u001b[36m0.7662\u001b[0m       0.9459    \u001b[35m0.6439\u001b[0m        \u001b[31m0.6290\u001b[0m        \u001b[94m0.0849\u001b[0m        \u001b[36m0.3956\u001b[0m     +  0.0100  10.4799\n",
            "      7  0.7642       0.9479    0.6402        \u001b[31m0.6283\u001b[0m        \u001b[94m0.0778\u001b[0m        \u001b[36m0.3947\u001b[0m     +  0.0100  11.0431\n",
            "      8  0.7285       0.9545    0.5890        \u001b[31m0.6184\u001b[0m        \u001b[94m0.0642\u001b[0m        \u001b[36m0.3824\u001b[0m     +  0.0010  17.7266\n",
            "      9  0.7450       0.9533    0.6114        \u001b[31m0.6127\u001b[0m        \u001b[94m0.0486\u001b[0m        \u001b[36m0.3754\u001b[0m     +  0.0010  10.6368\n",
            "     10  0.7442       0.9537    0.6101        \u001b[31m0.6093\u001b[0m        \u001b[94m0.0389\u001b[0m        \u001b[36m0.3712\u001b[0m     +  0.0010  11.0316\n",
            "     11  0.7605       0.9541    0.6322        \u001b[31m0.6072\u001b[0m        \u001b[94m0.0326\u001b[0m        \u001b[36m0.3686\u001b[0m     +  0.0010  10.6518\n",
            "     12  0.7617       0.9542    0.6338        \u001b[31m0.6059\u001b[0m        \u001b[94m0.0283\u001b[0m        \u001b[36m0.3671\u001b[0m     +  0.0010  11.1651\n",
            "     13  \u001b[36m0.7724\u001b[0m       0.9537    \u001b[35m0.6490\u001b[0m        \u001b[31m0.6051\u001b[0m        \u001b[94m0.0254\u001b[0m        \u001b[36m0.3662\u001b[0m     +  0.0010  10.7599\n",
            "     14  0.7699       0.9534    0.6457        \u001b[31m0.6046\u001b[0m        \u001b[94m0.0232\u001b[0m        \u001b[36m0.3655\u001b[0m     +  0.0010  11.1590\n",
            "     15  \u001b[36m0.7813\u001b[0m       0.9541    \u001b[35m0.6616\u001b[0m        \u001b[31m0.6045\u001b[0m        \u001b[94m0.0214\u001b[0m        \u001b[36m0.3654\u001b[0m     +  0.0001  10.1483\n",
            "     16  0.7808       0.9540    0.6608        \u001b[31m0.6044\u001b[0m        \u001b[94m0.0212\u001b[0m        \u001b[36m0.3653\u001b[0m     +  0.0001  10.6830\n",
            "     17  \u001b[36m0.7832\u001b[0m       0.9541    \u001b[35m0.6642\u001b[0m        \u001b[31m0.6044\u001b[0m        \u001b[94m0.0210\u001b[0m        \u001b[36m0.3653\u001b[0m     +  0.0001  10.7169\n",
            "     18  \u001b[36m0.7861\u001b[0m       0.9544    \u001b[35m0.6682\u001b[0m        \u001b[31m0.6043\u001b[0m        \u001b[94m0.0209\u001b[0m        \u001b[36m0.3652\u001b[0m     +  0.0001  11.1547\n",
            "     19  \u001b[36m0.7862\u001b[0m       0.9543    \u001b[35m0.6684\u001b[0m        \u001b[31m0.6042\u001b[0m        \u001b[94m0.0207\u001b[0m        \u001b[36m0.3651\u001b[0m     +  0.0001  10.8615\n",
            "     20  0.7854       0.9542    0.6674        \u001b[31m0.6042\u001b[0m        \u001b[94m0.0205\u001b[0m        \u001b[36m0.3650\u001b[0m     +  0.0001  11.0157\n",
            "     21  0.7851       0.9540    0.6670        \u001b[31m0.6041\u001b[0m        \u001b[94m0.0204\u001b[0m        \u001b[36m0.3650\u001b[0m     +  0.0001  10.6112\n",
            "     22  0.7858       0.9541    0.6680        0.6041        \u001b[94m0.0202\u001b[0m        0.3650        0.0000  10.2911\n",
            "     23  \u001b[36m0.7866\u001b[0m       0.9542    \u001b[35m0.6691\u001b[0m        \u001b[31m0.6041\u001b[0m        \u001b[94m0.0201\u001b[0m        \u001b[36m0.3650\u001b[0m     +  0.0000  10.6652\n",
            "     24  0.7853       0.9541    0.6672        \u001b[31m0.6041\u001b[0m        \u001b[94m0.0201\u001b[0m        \u001b[36m0.3650\u001b[0m     +  0.0000  10.7077\n",
            "     25  0.7851       0.9540    0.6670        \u001b[31m0.6041\u001b[0m        0.0201        \u001b[36m0.3650\u001b[0m     +  0.0000  11.1390\n",
            "     26  0.7851       0.9540    0.6670        0.6041        0.0201        0.3650        0.0000  10.7055\n",
            "     27  0.7853       0.9541    0.6672        \u001b[31m0.6041\u001b[0m        \u001b[94m0.0200\u001b[0m        \u001b[36m0.3650\u001b[0m     +  0.0000  11.2462\n",
            "     28  0.7852       0.9541    0.6671        \u001b[31m0.6041\u001b[0m        \u001b[94m0.0200\u001b[0m        \u001b[36m0.3650\u001b[0m     +  0.0000  10.1972\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 2/3; 6/6] END lr=0.01, module__dropout=0.3, module__linear_size=400, module__size_emb=120;, score=-1.042 total time= 5.8min\n",
            "[CV 3/3; 6/6] START lr=0.01, module__dropout=0.3, module__linear_size=400, module__size_emb=120\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss    cp      lr      dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ----  ------  -------\n",
            "      1  \u001b[36m0.5288\u001b[0m       \u001b[32m0.8568\u001b[0m    \u001b[35m0.3824\u001b[0m        \u001b[31m0.9201\u001b[0m        \u001b[94m0.9975\u001b[0m        \u001b[36m0.8466\u001b[0m     +  0.0100  10.1031\n",
            "      2  \u001b[36m0.6332\u001b[0m       \u001b[32m0.9508\u001b[0m    \u001b[35m0.4746\u001b[0m        \u001b[31m0.7302\u001b[0m        \u001b[94m0.6409\u001b[0m        \u001b[36m0.5332\u001b[0m     +  0.0100  10.1809\n",
            "      3  \u001b[36m0.6818\u001b[0m       \u001b[32m0.9574\u001b[0m    \u001b[35m0.5294\u001b[0m        \u001b[31m0.6731\u001b[0m        \u001b[94m0.2670\u001b[0m        \u001b[36m0.4530\u001b[0m     +  0.0100  10.9571\n",
            "      4  \u001b[36m0.6828\u001b[0m       \u001b[32m0.9596\u001b[0m    \u001b[35m0.5300\u001b[0m        \u001b[31m0.6557\u001b[0m        \u001b[94m0.1468\u001b[0m        \u001b[36m0.4300\u001b[0m     +  0.0100  10.5584\n",
            "      5  \u001b[36m0.7155\u001b[0m       0.9554    \u001b[35m0.5719\u001b[0m        \u001b[31m0.6451\u001b[0m        \u001b[94m0.1057\u001b[0m        \u001b[36m0.4162\u001b[0m     +  0.0100  10.9509\n",
            "      6  \u001b[36m0.7428\u001b[0m       0.9515    \u001b[35m0.6091\u001b[0m        \u001b[31m0.6400\u001b[0m        \u001b[94m0.0863\u001b[0m        \u001b[36m0.4096\u001b[0m     +  0.0100  10.5110\n",
            "      7  0.7030       0.9553    0.5561        \u001b[31m0.6391\u001b[0m        \u001b[94m0.0784\u001b[0m        \u001b[36m0.4084\u001b[0m     +  0.0100  10.6792\n",
            "      8  0.7295       0.9562    0.5897        \u001b[31m0.6297\u001b[0m        \u001b[94m0.0637\u001b[0m        \u001b[36m0.3965\u001b[0m     +  0.0010  11.8809\n",
            "      9  0.7322       0.9572    0.5929        \u001b[31m0.6247\u001b[0m        \u001b[94m0.0482\u001b[0m        \u001b[36m0.3902\u001b[0m     +  0.0010  9.8897\n",
            "     10  0.7407       0.9571    0.6041        \u001b[31m0.6218\u001b[0m        \u001b[94m0.0390\u001b[0m        \u001b[36m0.3866\u001b[0m     +  0.0010  11.1495\n",
            "     11  0.7409       0.9566    0.6046        \u001b[31m0.6199\u001b[0m        \u001b[94m0.0329\u001b[0m        \u001b[36m0.3843\u001b[0m     +  0.0010  10.6445\n",
            "     12  \u001b[36m0.7511\u001b[0m       0.9568    \u001b[35m0.6181\u001b[0m        \u001b[31m0.6187\u001b[0m        \u001b[94m0.0288\u001b[0m        \u001b[36m0.3828\u001b[0m     +  0.0010  11.1003\n",
            "     13  \u001b[36m0.7652\u001b[0m       0.9569    \u001b[35m0.6375\u001b[0m        \u001b[31m0.6176\u001b[0m        \u001b[94m0.0258\u001b[0m        \u001b[36m0.3815\u001b[0m     +  0.0010  10.8181\n",
            "     14  0.7643       0.9571    0.6361        \u001b[31m0.6173\u001b[0m        \u001b[94m0.0238\u001b[0m        \u001b[36m0.3810\u001b[0m     +  0.0010  11.2844\n",
            "     15  0.7617       0.9572    0.6325        \u001b[31m0.6172\u001b[0m        \u001b[94m0.0219\u001b[0m        \u001b[36m0.3810\u001b[0m     +  0.0001  10.7656\n",
            "     16  0.7606       0.9572    0.6310        \u001b[31m0.6172\u001b[0m        \u001b[94m0.0217\u001b[0m        \u001b[36m0.3809\u001b[0m     +  0.0001  10.2559\n",
            "     17  0.7600       0.9570    0.6303        \u001b[31m0.6171\u001b[0m        \u001b[94m0.0216\u001b[0m        \u001b[36m0.3808\u001b[0m     +  0.0001  10.5553\n",
            "     18  0.7629       0.9570    0.6342        \u001b[31m0.6171\u001b[0m        \u001b[94m0.0214\u001b[0m        \u001b[36m0.3808\u001b[0m     +  0.0001  11.4039\n",
            "     19  0.7617       0.9569    0.6327        \u001b[31m0.6170\u001b[0m        \u001b[94m0.0212\u001b[0m        \u001b[36m0.3807\u001b[0m     +  0.0001  11.1789\n",
            "     20  0.7629       0.9568    0.6344        \u001b[31m0.6169\u001b[0m        \u001b[94m0.0209\u001b[0m        \u001b[36m0.3806\u001b[0m     +  0.0001  10.7834\n",
            "     21  \u001b[36m0.7667\u001b[0m       0.9567    \u001b[35m0.6396\u001b[0m        \u001b[31m0.6169\u001b[0m        \u001b[94m0.0209\u001b[0m        \u001b[36m0.3805\u001b[0m     +  0.0001  11.1831\n",
            "     22  0.7650       0.9569    0.6373        0.6169        \u001b[94m0.0206\u001b[0m        0.3805        0.0000  10.7794\n",
            "     23  0.7641       0.9568    0.6360        0.6169        0.0206        0.3806        0.0000  10.3788\n",
            "     24  0.7638       0.9568    0.6356        0.6169        0.0207        0.3806        0.0000  10.3409\n",
            "     25  0.7642       0.9567    0.6362        0.6169        \u001b[94m0.0206\u001b[0m        0.3806        0.0000  11.2272\n",
            "     26  0.7642       0.9568    0.6362        0.6169        0.0206        0.3805        0.0000  10.8358\n",
            "     27  0.7639       0.9567    0.6358        0.6169        \u001b[94m0.0206\u001b[0m        0.3805        0.0000  11.3063\n",
            "     28  0.7641       0.9567    0.6361        0.6169        0.0206        0.3805        0.0000  9.9484\n",
            "     29  0.7643       0.9567    0.6363        0.6169        \u001b[94m0.0205\u001b[0m        0.3805        0.0000  10.9119\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 3/3; 6/6] END lr=0.01, module__dropout=0.3, module__linear_size=400, module__size_emb=120;, score=-1.105 total time= 5.8min\n",
            "-1.1080869833628337 {'lr': 0.01, 'module__dropout': 0.3, 'module__linear_size': 200, 'module__size_emb': 120}\n"
          ]
        }
      ],
      "source": [
        "params = {\n",
        "    'lr': [0.01],\n",
        "    'module__size_emb': [30, 60, 120],\n",
        "    'module__dropout': [0.3],\n",
        "    'module__linear_size': [200, 400]\n",
        "}\n",
        "gs = GridSearchCV(ncfnet,\n",
        "                  params,\n",
        "                  verbose=50,\n",
        "                  refit=False,\n",
        "                  pre_dispatch=2,\n",
        "                  n_jobs=1,\n",
        "                  cv=3,\n",
        "                  scoring='neg_mean_squared_error')\n",
        "\n",
        "X_ds = SliceDataset(train, idx=0)\n",
        "y_ds = SliceDataset(train, idx=1)\n",
        "gs.fit(X_ds, y_ds)\n",
        "\n",
        "print(gs.best_score_, gs.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeHOCM-gNgJG"
      },
      "source": [
        "### Deep and Wide"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FirOYkr9NgJG"
      },
      "source": [
        "#### Manually specify hyperparamers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgVTuwedriTI"
      },
      "outputs": [],
      "source": [
        "deepnwidenet = NeuralNet(\n",
        "    deepnwide,\n",
        "    module__users_emb=train.users_emb,\n",
        "    module__movies_emb=train.movies_emb,\n",
        "    module__users_ohe=train.users_ohe,\n",
        "    module__movies_ohe=train.movies_ohe,\n",
        "    #### Manually specify hyperparamers e=train.movies_ohe,\n",
        "    module__interact=train.interact,\n",
        "    module__size_emb=30,\n",
        "    module__y_range=train.y_range,\n",
        "    module__dropout=0.2,\n",
        "    max_epochs=30,\n",
        "    lr=0.001,\n",
        "    optimizer=torch.optim.Adam,\n",
        "    criterion=torch.nn.MSELoss,\n",
        "    device=device,\n",
        "    iterator_train__batch_size=1024,\n",
        "    iterator_train__num_workers=0,\n",
        "    iterator_train__shuffle=True,\n",
        "    iterator_valid__batch_size=4096,\n",
        "    train_split=predefined_split(valid_dataset),\n",
        "    callbacks=[\n",
        "               earlystopping,\n",
        "               epoch_rmse,\n",
        "               epoch_precision,\n",
        "               epoch_recall,\n",
        "               epoch_f1,\n",
        "               #checkpoint,\n",
        "               lr_scheduler,\n",
        "               #TensorBoard(writer),\n",
        "               #progressbar\n",
        "               ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6qm5_snshUi",
        "outputId": "804f0741-7d1e-4c58-ca8d-e5c83f8f951a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.5067\u001b[0m       \u001b[32m0.8408\u001b[0m    \u001b[35m0.3626\u001b[0m        \u001b[31m0.9640\u001b[0m        \u001b[94m1.0904\u001b[0m        \u001b[36m0.9294\u001b[0m  0.0010  7.7631\n",
            "      2  \u001b[36m0.5385\u001b[0m       0.8312    \u001b[35m0.3982\u001b[0m        \u001b[31m0.9474\u001b[0m        \u001b[94m0.8823\u001b[0m        \u001b[36m0.8976\u001b[0m  0.0010  2.9171\n",
            "      3  0.4556       \u001b[32m0.8532\u001b[0m    0.3107        \u001b[31m0.9453\u001b[0m        \u001b[94m0.8592\u001b[0m        \u001b[36m0.8936\u001b[0m  0.0010  2.8487\n",
            "      4  \u001b[36m0.5504\u001b[0m       0.8346    \u001b[35m0.4106\u001b[0m        \u001b[31m0.9405\u001b[0m        \u001b[94m0.8501\u001b[0m        \u001b[36m0.8845\u001b[0m  0.0010  3.6234\n",
            "      5  0.4931       0.8494    0.3474        \u001b[31m0.9397\u001b[0m        \u001b[94m0.8375\u001b[0m        \u001b[36m0.8830\u001b[0m  0.0010  3.3253\n",
            "      6  0.4781       \u001b[32m0.8581\u001b[0m    0.3313        \u001b[31m0.9387\u001b[0m        \u001b[94m0.8264\u001b[0m        \u001b[36m0.8811\u001b[0m  0.0010  3.0694\n",
            "      7  0.5384       0.8439    0.3953        \u001b[31m0.9366\u001b[0m        \u001b[94m0.8172\u001b[0m        \u001b[36m0.8772\u001b[0m  0.0010  2.8755\n",
            "      8  0.5366       0.8480    0.3925        \u001b[31m0.9366\u001b[0m        \u001b[94m0.7887\u001b[0m        \u001b[36m0.8772\u001b[0m  0.0001  3.4009\n",
            "      9  0.5457       0.8444    0.4031        0.9370        \u001b[94m0.7850\u001b[0m        0.8779  0.0001  3.4140\n",
            "     10  0.5394       0.8467    0.3957        \u001b[31m0.9365\u001b[0m        \u001b[94m0.7826\u001b[0m        \u001b[36m0.8770\u001b[0m  0.0001  2.8268\n",
            "     11  \u001b[36m0.5586\u001b[0m       0.8420    \u001b[35m0.4179\u001b[0m        0.9371        \u001b[94m0.7811\u001b[0m        0.8782  0.0001  2.8510\n",
            "     12  0.5544       0.8442    0.4127        0.9373        \u001b[94m0.7792\u001b[0m        0.8786  0.0001  3.6220\n",
            "     13  0.5547       0.8423    0.4135        0.9377        \u001b[94m0.7775\u001b[0m        0.8793  0.0001  3.0911\n",
            "     14  0.5208       0.8485    0.3756        0.9382        \u001b[94m0.7759\u001b[0m        0.8801  0.0001  2.8236\n",
            "     15  0.5514       0.8439    0.4095        0.9381        \u001b[94m0.7722\u001b[0m        0.8801  0.0000  3.0198\n",
            "     16  0.5540       0.8430    0.4125        0.9382        \u001b[94m0.7719\u001b[0m        0.8803  0.0000  3.2988\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<class 'skorch.net.NeuralNet'>[initialized](\n",
              "  module_=deepnwide(\n",
              "    (emb_UserID): Embedding(943, 30)\n",
              "    (emb_Gender): Embedding(2, 30)\n",
              "    (emb_Age): Embedding(7, 30)\n",
              "    (emb_Occupation): Embedding(21, 30)\n",
              "    (emb_MovieID): Embedding(1682, 30)\n",
              "    (h1): Linear(in_features=150, out_features=500, bias=True)\n",
              "    (h2): Linear(in_features=500, out_features=500, bias=True)\n",
              "    (h3): Linear(in_features=500, out_features=500, bias=True)\n",
              "    (dropout1): Dropout(p=0.2, inplace=False)\n",
              "    (dropout2): Dropout(p=0.2, inplace=False)\n",
              "    (dropout3): Dropout(p=0.2, inplace=False)\n",
              "    (last_layer): Linear(in_features=845, out_features=1, bias=True)\n",
              "  ),\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "deepnwidenet.fit(train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9pokwLlNgJH"
      },
      "source": [
        "#### GridsearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4Ecaxc4euD1",
        "outputId": "9a03a04e-fef9-41bc-b142-db9a64f7a27e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
            "[CV 1/3; 1/36] START lr=0.01, module__dropout=0.3, module__linear_size=400, module__size_emb=30\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.3928\u001b[0m       \u001b[32m0.8538\u001b[0m    \u001b[35m0.2551\u001b[0m        \u001b[31m0.9929\u001b[0m        \u001b[94m0.9993\u001b[0m        \u001b[36m0.9858\u001b[0m  0.0100  3.6870\n",
            "      2  \u001b[36m0.4698\u001b[0m       0.8389    \u001b[35m0.3262\u001b[0m        \u001b[31m0.9738\u001b[0m        \u001b[94m0.8905\u001b[0m        \u001b[36m0.9482\u001b[0m  0.0100  3.6836\n",
            "      3  \u001b[36m0.5845\u001b[0m       0.8091    \u001b[35m0.4575\u001b[0m        0.9857        \u001b[94m0.8617\u001b[0m        0.9717  0.0100  4.2670\n",
            "      4  0.5128       0.8349    0.3700        \u001b[31m0.9646\u001b[0m        \u001b[94m0.8515\u001b[0m        \u001b[36m0.9305\u001b[0m  0.0100  3.6281\n",
            "      5  \u001b[36m0.6784\u001b[0m       0.7743    \u001b[35m0.6037\u001b[0m        1.0065        \u001b[94m0.8368\u001b[0m        1.0131  0.0100  3.6015\n",
            "      6  0.4505       0.8528    0.3061        \u001b[31m0.9574\u001b[0m        \u001b[94m0.8337\u001b[0m        \u001b[36m0.9165\u001b[0m  0.0100  4.4760\n",
            "      7  0.5798       0.8228    0.4476        \u001b[31m0.9528\u001b[0m        \u001b[94m0.8095\u001b[0m        \u001b[36m0.9078\u001b[0m  0.0100  3.6169\n",
            "      8  0.5315       0.8376    0.3893        \u001b[31m0.9390\u001b[0m        \u001b[94m0.7574\u001b[0m        \u001b[36m0.8818\u001b[0m  0.0010  3.6049\n",
            "      9  0.5345       0.8397    0.3920        \u001b[31m0.9354\u001b[0m        \u001b[94m0.7399\u001b[0m        \u001b[36m0.8751\u001b[0m  0.0010  4.2820\n",
            "     10  0.5455       0.8405    0.4038        \u001b[31m0.9334\u001b[0m        \u001b[94m0.7323\u001b[0m        \u001b[36m0.8712\u001b[0m  0.0010  4.1939\n",
            "     11  0.5340       0.8428    0.3908        \u001b[31m0.9316\u001b[0m        \u001b[94m0.7257\u001b[0m        \u001b[36m0.8679\u001b[0m  0.0010  3.5769\n",
            "     12  0.5318       0.8453    0.3879        \u001b[31m0.9285\u001b[0m        \u001b[94m0.7199\u001b[0m        \u001b[36m0.8621\u001b[0m  0.0010  3.9192\n",
            "     13  0.5194       0.8494    0.3740        \u001b[31m0.9272\u001b[0m        \u001b[94m0.7145\u001b[0m        \u001b[36m0.8597\u001b[0m  0.0010  4.0736\n",
            "     14  0.5700       0.8359    0.4324        \u001b[31m0.9258\u001b[0m        \u001b[94m0.7111\u001b[0m        \u001b[36m0.8570\u001b[0m  0.0010  3.5868\n",
            "     15  0.5423       0.8437    0.3996        \u001b[31m0.9238\u001b[0m        \u001b[94m0.6996\u001b[0m        \u001b[36m0.8534\u001b[0m  0.0001  3.6154\n",
            "     16  0.5479       0.8432    0.4058        \u001b[31m0.9236\u001b[0m        \u001b[94m0.6976\u001b[0m        \u001b[36m0.8530\u001b[0m  0.0001  4.4788\n",
            "     17  0.5470       0.8439    0.4047        \u001b[31m0.9230\u001b[0m        0.6993        \u001b[36m0.8520\u001b[0m  0.0001  3.6612\n",
            "     18  0.5435       0.8454    0.4004        \u001b[31m0.9228\u001b[0m        0.6982        \u001b[36m0.8517\u001b[0m  0.0001  3.6077\n",
            "     19  0.5462       0.8438    0.4038        \u001b[31m0.9228\u001b[0m        \u001b[94m0.6963\u001b[0m        \u001b[36m0.8516\u001b[0m  0.0001  4.4851\n",
            "     20  0.5487       0.8430    0.4067        \u001b[31m0.9225\u001b[0m        \u001b[94m0.6945\u001b[0m        \u001b[36m0.8511\u001b[0m  0.0001  3.6474\n",
            "     21  0.5495       0.8428    0.4076        \u001b[31m0.9224\u001b[0m        0.6958        \u001b[36m0.8509\u001b[0m  0.0001  3.6413\n",
            "     22  0.5493       0.8428    0.4074        \u001b[31m0.9224\u001b[0m        \u001b[94m0.6944\u001b[0m        \u001b[36m0.8508\u001b[0m  0.0000  3.8129\n",
            "     23  0.5492       0.8429    0.4073        \u001b[31m0.9223\u001b[0m        0.6949        \u001b[36m0.8507\u001b[0m  0.0000  3.8936\n",
            "     24  0.5492       0.8430    0.4073        0.9223        0.6957        0.8507  0.0000  3.6259\n",
            "     25  0.5492       0.8432    0.4073        \u001b[31m0.9223\u001b[0m        0.6958        \u001b[36m0.8507\u001b[0m  0.0000  3.7001\n",
            "     26  0.5494       0.8431    0.4074        \u001b[31m0.9223\u001b[0m        \u001b[94m0.6940\u001b[0m        \u001b[36m0.8506\u001b[0m  0.0000  4.4671\n",
            "     27  0.5502       0.8429    0.4084        \u001b[31m0.9223\u001b[0m        0.6951        \u001b[36m0.8506\u001b[0m  0.0000  3.6083\n",
            "     28  0.5506       0.8429    0.4089        \u001b[31m0.9222\u001b[0m        \u001b[94m0.6939\u001b[0m        \u001b[36m0.8505\u001b[0m  0.0000  3.7187\n",
            "     29  0.5505       0.8430    0.4087        \u001b[31m0.9222\u001b[0m        0.6949        \u001b[36m0.8505\u001b[0m  0.0000  4.4676\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 1/3; 1/36] END lr=0.01, module__dropout=0.3, module__linear_size=400, module__size_emb=30;, score=-1.178 total time= 2.0min\n",
            "[CV 2/3; 1/36] START lr=0.01, module__dropout=0.3, module__linear_size=400, module__size_emb=30\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.5859\u001b[0m       \u001b[32m0.8080\u001b[0m    \u001b[35m0.4595\u001b[0m        \u001b[31m0.9625\u001b[0m        \u001b[94m1.0347\u001b[0m        \u001b[36m0.9264\u001b[0m  0.0100  3.5741\n",
            "      2  0.4878       \u001b[32m0.8382\u001b[0m    0.3440        \u001b[31m0.9473\u001b[0m        \u001b[94m0.8920\u001b[0m        \u001b[36m0.8974\u001b[0m  0.0100  4.4949\n",
            "      3  0.4999       \u001b[32m0.8420\u001b[0m    0.3555        \u001b[31m0.9400\u001b[0m        \u001b[94m0.8658\u001b[0m        \u001b[36m0.8836\u001b[0m  0.0100  3.5583\n",
            "      4  0.4762       \u001b[32m0.8541\u001b[0m    0.3302        0.9412        \u001b[94m0.8483\u001b[0m        0.8858  0.0100  3.6083\n",
            "      5  0.4978       0.8517    0.3517        \u001b[31m0.9310\u001b[0m        \u001b[94m0.8399\u001b[0m        \u001b[36m0.8668\u001b[0m  0.0100  4.4750\n",
            "      6  \u001b[36m0.6545\u001b[0m       0.8084    \u001b[35m0.5499\u001b[0m        0.9424        \u001b[94m0.8202\u001b[0m        0.8882  0.0100  4.0296\n",
            "      7  0.5406       \u001b[32m0.8581\u001b[0m    0.3946        \u001b[31m0.9134\u001b[0m        \u001b[94m0.8076\u001b[0m        \u001b[36m0.8342\u001b[0m  0.0100  3.6342\n",
            "      8  0.5379       \u001b[32m0.8617\u001b[0m    0.3910        \u001b[31m0.9061\u001b[0m        \u001b[94m0.7463\u001b[0m        \u001b[36m0.8209\u001b[0m  0.0010  4.5509\n",
            "      9  0.5456       0.8616    0.3992        \u001b[31m0.9024\u001b[0m        \u001b[94m0.7288\u001b[0m        \u001b[36m0.8144\u001b[0m  0.0010  3.6561\n",
            "     10  0.5240       \u001b[32m0.8691\u001b[0m    0.3751        \u001b[31m0.9002\u001b[0m        \u001b[94m0.7177\u001b[0m        \u001b[36m0.8103\u001b[0m  0.0010  3.6517\n",
            "     11  0.5583       0.8610    0.4131        \u001b[31m0.8976\u001b[0m        \u001b[94m0.7105\u001b[0m        \u001b[36m0.8057\u001b[0m  0.0010  4.1946\n",
            "     12  0.5445       0.8662    0.3970        \u001b[31m0.8953\u001b[0m        \u001b[94m0.7037\u001b[0m        \u001b[36m0.8015\u001b[0m  0.0010  3.8996\n",
            "     13  0.5471       0.8668    0.3996        \u001b[31m0.8936\u001b[0m        \u001b[94m0.6998\u001b[0m        \u001b[36m0.7985\u001b[0m  0.0010  3.6807\n",
            "     14  0.5670       0.8627    0.4223        \u001b[31m0.8907\u001b[0m        \u001b[94m0.6938\u001b[0m        \u001b[36m0.7933\u001b[0m  0.0010  3.6076\n",
            "     15  0.5582       0.8643    0.4122        \u001b[31m0.8904\u001b[0m        \u001b[94m0.6812\u001b[0m        \u001b[36m0.7929\u001b[0m  0.0001  4.3394\n",
            "     16  0.5670       0.8628    0.4222        \u001b[31m0.8902\u001b[0m        0.6812        \u001b[36m0.7924\u001b[0m  0.0001  3.5822\n",
            "     17  0.5648       0.8638    0.4196        \u001b[31m0.8899\u001b[0m        0.6818        \u001b[36m0.7920\u001b[0m  0.0001  3.6681\n",
            "     18  0.5605       0.8645    0.4147        \u001b[31m0.8896\u001b[0m        \u001b[94m0.6782\u001b[0m        \u001b[36m0.7914\u001b[0m  0.0001  4.5316\n",
            "     19  0.5598       0.8657    0.4137        \u001b[31m0.8891\u001b[0m        0.6782        \u001b[36m0.7905\u001b[0m  0.0001  3.5624\n",
            "     20  0.5594       0.8657    0.4132        \u001b[31m0.8889\u001b[0m        0.6784        \u001b[36m0.7902\u001b[0m  0.0001  3.5686\n",
            "     21  0.5629       0.8640    0.4175        \u001b[31m0.8887\u001b[0m        \u001b[94m0.6767\u001b[0m        \u001b[36m0.7898\u001b[0m  0.0001  4.4497\n",
            "     22  0.5638       0.8640    0.4184        \u001b[31m0.8887\u001b[0m        \u001b[94m0.6755\u001b[0m        \u001b[36m0.7897\u001b[0m  0.0000  3.6315\n",
            "     23  0.5628       0.8640    0.4174        \u001b[31m0.8886\u001b[0m        \u001b[94m0.6742\u001b[0m        \u001b[36m0.7896\u001b[0m  0.0000  3.6170\n",
            "     24  0.5632       0.8639    0.4177        \u001b[31m0.8886\u001b[0m        0.6770        \u001b[36m0.7896\u001b[0m  0.0000  4.1173\n",
            "     25  0.5628       0.8640    0.4174        \u001b[31m0.8885\u001b[0m        0.6749        \u001b[36m0.7895\u001b[0m  0.0000  3.9666\n",
            "     26  0.5623       0.8647    0.4166        \u001b[31m0.8885\u001b[0m        0.6751        \u001b[36m0.7895\u001b[0m  0.0000  3.5671\n",
            "     27  0.5627       0.8644    0.4171        \u001b[31m0.8885\u001b[0m        0.6765        \u001b[36m0.7895\u001b[0m  0.0000  3.6613\n",
            "     28  0.5624       0.8646    0.4167        \u001b[31m0.8885\u001b[0m        0.6756        \u001b[36m0.7894\u001b[0m  0.0000  4.4212\n",
            "     29  0.5624       0.8646    0.4167        \u001b[31m0.8885\u001b[0m        0.6759        \u001b[36m0.7894\u001b[0m  0.0000  3.5768\n",
            "     30  0.5624       0.8646    0.4167        \u001b[31m0.8885\u001b[0m        0.6744        \u001b[36m0.7894\u001b[0m  0.0000  3.6013\n",
            "[CV 2/3; 1/36] END lr=0.01, module__dropout=0.3, module__linear_size=400, module__size_emb=30;, score=-1.052 total time= 2.0min\n",
            "[CV 3/3; 1/36] START lr=0.01, module__dropout=0.3, module__linear_size=400, module__size_emb=30\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4297\u001b[0m       \u001b[32m0.8461\u001b[0m    \u001b[35m0.2880\u001b[0m        \u001b[31m0.9671\u001b[0m        \u001b[94m1.0158\u001b[0m        \u001b[36m0.9352\u001b[0m  0.0100  3.9402\n",
            "      2  0.3615       \u001b[32m0.8662\u001b[0m    0.2284        \u001b[31m0.9669\u001b[0m        \u001b[94m0.8758\u001b[0m        \u001b[36m0.9350\u001b[0m  0.0100  3.6545\n",
            "      3  0.3917       0.8585    0.2537        \u001b[31m0.9541\u001b[0m        \u001b[94m0.8524\u001b[0m        \u001b[36m0.9102\u001b[0m  0.0100  4.0583\n",
            "      4  \u001b[36m0.6616\u001b[0m       0.7862    \u001b[35m0.5711\u001b[0m        0.9769        \u001b[94m0.8354\u001b[0m        0.9543  0.0100  4.2409\n",
            "      5  0.5000       0.8422    0.3555        \u001b[31m0.9373\u001b[0m        \u001b[94m0.8249\u001b[0m        \u001b[36m0.8784\u001b[0m  0.0100  3.6239\n",
            "      6  0.3388       \u001b[32m0.8834\u001b[0m    0.2096        0.9461        \u001b[94m0.8022\u001b[0m        0.8950  0.0100  3.6233\n",
            "      7  0.4880       0.8630    0.3402        \u001b[31m0.9269\u001b[0m        \u001b[94m0.7890\u001b[0m        \u001b[36m0.8592\u001b[0m  0.0100  4.4810\n",
            "      8  0.5214       0.8582    0.3745        \u001b[31m0.9216\u001b[0m        \u001b[94m0.7333\u001b[0m        \u001b[36m0.8494\u001b[0m  0.0010  3.6291\n",
            "      9  0.4933       0.8668    0.3448        \u001b[31m0.9199\u001b[0m        \u001b[94m0.7194\u001b[0m        \u001b[36m0.8461\u001b[0m  0.0010  3.5726\n",
            "     10  0.5511       0.8538    0.4068        \u001b[31m0.9152\u001b[0m        \u001b[94m0.7115\u001b[0m        \u001b[36m0.8377\u001b[0m  0.0010  4.2190\n",
            "     11  0.5308       0.8597    0.3840        \u001b[31m0.9126\u001b[0m        \u001b[94m0.7040\u001b[0m        \u001b[36m0.8328\u001b[0m  0.0010  3.6413\n",
            "     12  0.5047       0.8673    0.3559        \u001b[31m0.9099\u001b[0m        \u001b[94m0.6977\u001b[0m        \u001b[36m0.8279\u001b[0m  0.0010  3.5375\n",
            "     13  0.5487       0.8570    0.4035        \u001b[31m0.9084\u001b[0m        \u001b[94m0.6907\u001b[0m        \u001b[36m0.8252\u001b[0m  0.0010  3.8677\n",
            "     14  0.5354       0.8606    0.3885        \u001b[31m0.9049\u001b[0m        \u001b[94m0.6872\u001b[0m        \u001b[36m0.8188\u001b[0m  0.0010  4.0834\n",
            "     15  0.5320       0.8630    0.3845        0.9053        \u001b[94m0.6777\u001b[0m        0.8195  0.0001  3.5821\n",
            "     16  0.5380       0.8613    0.3912        \u001b[31m0.9048\u001b[0m        \u001b[94m0.6749\u001b[0m        \u001b[36m0.8186\u001b[0m  0.0001  3.5632\n",
            "     17  0.5333       0.8629    0.3859        \u001b[31m0.9047\u001b[0m        \u001b[94m0.6735\u001b[0m        \u001b[36m0.8185\u001b[0m  0.0001  4.4053\n",
            "     18  0.5332       0.8640    0.3856        \u001b[31m0.9045\u001b[0m        0.6738        \u001b[36m0.8181\u001b[0m  0.0001  3.6316\n",
            "     19  0.5427       0.8598    0.3965        \u001b[31m0.9042\u001b[0m        \u001b[94m0.6723\u001b[0m        \u001b[36m0.8176\u001b[0m  0.0001  3.6167\n",
            "     20  0.5404       0.8607    0.3938        \u001b[31m0.9038\u001b[0m        \u001b[94m0.6717\u001b[0m        \u001b[36m0.8169\u001b[0m  0.0001  4.4659\n",
            "     21  0.5371       0.8623    0.3901        \u001b[31m0.9038\u001b[0m        0.6725        \u001b[36m0.8168\u001b[0m  0.0001  3.6570\n",
            "     22  0.5366       0.8623    0.3895        0.9038        \u001b[94m0.6686\u001b[0m        0.8168  0.0000  3.5874\n",
            "     23  0.5368       0.8622    0.3897        \u001b[31m0.9037\u001b[0m        0.6703        \u001b[36m0.8167\u001b[0m  0.0000  4.1221\n",
            "     24  0.5368       0.8622    0.3897        \u001b[31m0.9036\u001b[0m        0.6708        \u001b[36m0.8166\u001b[0m  0.0000  3.8805\n",
            "     25  0.5373       0.8623    0.3902        \u001b[31m0.9036\u001b[0m        0.6696        \u001b[36m0.8165\u001b[0m  0.0000  3.6312\n",
            "     26  0.5374       0.8623    0.3903        \u001b[31m0.9035\u001b[0m        0.6708        \u001b[36m0.8164\u001b[0m  0.0000  3.7842\n",
            "     27  0.5378       0.8625    0.3907        \u001b[31m0.9035\u001b[0m        0.6697        \u001b[36m0.8163\u001b[0m  0.0000  4.3191\n",
            "     28  0.5378       0.8625    0.3907        \u001b[31m0.9035\u001b[0m        0.6693        \u001b[36m0.8163\u001b[0m  0.0000  3.6288\n",
            "     29  0.5378       0.8623    0.3908        \u001b[31m0.9035\u001b[0m        0.6697        \u001b[36m0.8163\u001b[0m  0.0000  3.6966\n",
            "     30  0.5378       0.8623    0.3908        \u001b[31m0.9035\u001b[0m        0.6689        \u001b[36m0.8163\u001b[0m  0.0000  4.5056\n",
            "[CV 3/3; 1/36] END lr=0.01, module__dropout=0.3, module__linear_size=400, module__size_emb=30;, score=-1.131 total time= 2.0min\n",
            "[CV 1/3; 2/36] START lr=0.01, module__dropout=0.3, module__linear_size=400, module__size_emb=60\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.5648\u001b[0m       \u001b[32m0.8021\u001b[0m    \u001b[35m0.4359\u001b[0m        \u001b[31m0.9965\u001b[0m        \u001b[94m1.0049\u001b[0m        \u001b[36m0.9931\u001b[0m  0.0100  3.5239\n",
            "      2  0.4469       \u001b[32m0.8421\u001b[0m    0.3041        \u001b[31m0.9763\u001b[0m        \u001b[94m0.8872\u001b[0m        \u001b[36m0.9531\u001b[0m  0.0100  3.6820\n",
            "      3  0.3459       \u001b[32m0.8617\u001b[0m    0.2164        \u001b[31m0.9757\u001b[0m        \u001b[94m0.8618\u001b[0m        \u001b[36m0.9520\u001b[0m  0.0100  4.0864\n",
            "      4  0.4477       0.8482    0.3041        \u001b[31m0.9693\u001b[0m        \u001b[94m0.8538\u001b[0m        \u001b[36m0.9396\u001b[0m  0.0100  3.6157\n",
            "      5  \u001b[36m0.6484\u001b[0m       0.7936    \u001b[35m0.5481\u001b[0m        0.9863        \u001b[94m0.8430\u001b[0m        0.9728  0.0100  3.5317\n",
            "      6  0.5512       0.8304    0.4125        \u001b[31m0.9576\u001b[0m        \u001b[94m0.8279\u001b[0m        \u001b[36m0.9170\u001b[0m  0.0100  4.4529\n",
            "      7  0.5250       0.8384    0.3822        \u001b[31m0.9538\u001b[0m        \u001b[94m0.8151\u001b[0m        \u001b[36m0.9096\u001b[0m  0.0100  3.6021\n",
            "      8  0.5155       0.8474    0.3705        \u001b[31m0.9422\u001b[0m        \u001b[94m0.7635\u001b[0m        \u001b[36m0.8877\u001b[0m  0.0010  3.5072\n",
            "      9  0.5187       0.8519    0.3729        \u001b[31m0.9375\u001b[0m        \u001b[94m0.7462\u001b[0m        \u001b[36m0.8788\u001b[0m  0.0010  4.3236\n",
            "     10  0.5414       0.8470    0.3979        \u001b[31m0.9365\u001b[0m        \u001b[94m0.7354\u001b[0m        \u001b[36m0.8771\u001b[0m  0.0010  3.6374\n",
            "     11  0.5248       0.8526    0.3791        \u001b[31m0.9328\u001b[0m        \u001b[94m0.7296\u001b[0m        \u001b[36m0.8701\u001b[0m  0.0010  3.6116\n",
            "     12  0.5478       0.8496    0.4042        \u001b[31m0.9307\u001b[0m        \u001b[94m0.7201\u001b[0m        \u001b[36m0.8661\u001b[0m  0.0010  3.8602\n",
            "     13  0.5560       0.8476    0.4137        \u001b[31m0.9283\u001b[0m        \u001b[94m0.7174\u001b[0m        \u001b[36m0.8618\u001b[0m  0.0010  4.2181\n",
            "     14  0.5626       0.8432    0.4221        \u001b[31m0.9266\u001b[0m        \u001b[94m0.7118\u001b[0m        \u001b[36m0.8586\u001b[0m  0.0010  3.6150\n",
            "     15  0.5406       0.8485    0.3967        \u001b[31m0.9264\u001b[0m        \u001b[94m0.7027\u001b[0m        \u001b[36m0.8582\u001b[0m  0.0001  3.5841\n",
            "     16  0.5397       0.8493    0.3955        \u001b[31m0.9261\u001b[0m        \u001b[94m0.7018\u001b[0m        \u001b[36m0.8577\u001b[0m  0.0001  4.4909\n",
            "     17  0.5413       0.8488    0.3974        \u001b[31m0.9257\u001b[0m        \u001b[94m0.6983\u001b[0m        \u001b[36m0.8570\u001b[0m  0.0001  3.6251\n",
            "     18  0.5370       0.8498    0.3926        0.9257        0.6990        0.8570  0.0001  3.5561\n",
            "     19  0.5376       0.8498    0.3931        \u001b[31m0.9254\u001b[0m        0.6994        \u001b[36m0.8563\u001b[0m  0.0001  4.5584\n",
            "     20  0.5362       0.8489    0.3919        \u001b[31m0.9251\u001b[0m        0.6992        \u001b[36m0.8558\u001b[0m  0.0001  3.6492\n",
            "     21  0.5382       0.8486    0.3940        \u001b[31m0.9248\u001b[0m        \u001b[94m0.6976\u001b[0m        \u001b[36m0.8553\u001b[0m  0.0001  3.5851\n",
            "     22  0.5385       0.8489    0.3944        0.9249        \u001b[94m0.6963\u001b[0m        0.8554  0.0000  3.8496\n",
            "     23  0.5364       0.8491    0.3920        0.9249        0.6965        0.8554  0.0000  3.9562\n",
            "     24  0.5365       0.8490    0.3921        0.9249        \u001b[94m0.6962\u001b[0m        0.8554  0.0000  3.5493\n",
            "     25  0.5362       0.8492    0.3918        0.9249        0.6969        0.8554  0.0000  3.5711\n",
            "     26  0.5364       0.8493    0.3919        0.9249        0.6963        0.8554  0.0000  4.4707\n",
            "     27  0.5366       0.8492    0.3922        0.9249        \u001b[94m0.6957\u001b[0m        0.8555  0.0000  3.6457\n",
            "     28  0.5366       0.8491    0.3923        0.9249        0.6957        0.8554  0.0000  3.5312\n",
            "     29  0.5368       0.8490    0.3925        0.9249        0.6968        0.8554  0.0000  4.5362\n",
            "     30  0.5369       0.8490    0.3926        0.9249        \u001b[94m0.6947\u001b[0m        0.8554  0.0000  3.5939\n",
            "[CV 1/3; 2/36] END lr=0.01, module__dropout=0.3, module__linear_size=400, module__size_emb=60;, score=-1.181 total time= 2.0min\n",
            "[CV 2/3; 2/36] START lr=0.01, module__dropout=0.3, module__linear_size=400, module__size_emb=60\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.5391\u001b[0m       \u001b[32m0.8191\u001b[0m    \u001b[35m0.4018\u001b[0m        \u001b[31m0.9599\u001b[0m        \u001b[94m1.0262\u001b[0m        \u001b[36m0.9214\u001b[0m  0.0100  3.5543\n",
            "      2  0.5273       \u001b[32m0.8328\u001b[0m    0.3858        \u001b[31m0.9438\u001b[0m        \u001b[94m0.8895\u001b[0m        \u001b[36m0.8908\u001b[0m  0.0100  4.4520\n",
            "      3  \u001b[36m0.5402\u001b[0m       \u001b[32m0.8355\u001b[0m    0.3991        \u001b[31m0.9406\u001b[0m        \u001b[94m0.8657\u001b[0m        \u001b[36m0.8847\u001b[0m  0.0100  3.5226\n",
            "      4  \u001b[36m0.5920\u001b[0m       0.8287    \u001b[35m0.4605\u001b[0m        \u001b[31m0.9381\u001b[0m        \u001b[94m0.8490\u001b[0m        \u001b[36m0.8800\u001b[0m  0.0100  3.5665\n",
            "      5  0.4815       \u001b[32m0.8572\u001b[0m    0.3347        \u001b[31m0.9351\u001b[0m        \u001b[94m0.8379\u001b[0m        \u001b[36m0.8744\u001b[0m  0.0100  4.5182\n",
            "      6  \u001b[36m0.6031\u001b[0m       0.8316    \u001b[35m0.4731\u001b[0m        \u001b[31m0.9338\u001b[0m        \u001b[94m0.8303\u001b[0m        \u001b[36m0.8720\u001b[0m  0.0100  3.6191\n",
            "      7  0.5909       0.8426    0.4550        \u001b[31m0.9318\u001b[0m        \u001b[94m0.8178\u001b[0m        \u001b[36m0.8683\u001b[0m  0.0100  3.6249\n",
            "      8  0.5036       \u001b[32m0.8679\u001b[0m    0.3547        \u001b[31m0.9156\u001b[0m        \u001b[94m0.7646\u001b[0m        \u001b[36m0.8384\u001b[0m  0.0010  4.0894\n",
            "      9  0.5263       0.8641    0.3784        \u001b[31m0.9116\u001b[0m        \u001b[94m0.7462\u001b[0m        \u001b[36m0.8311\u001b[0m  0.0010  3.7302\n",
            "     10  0.5358       0.8598    0.3892        \u001b[31m0.9083\u001b[0m        \u001b[94m0.7389\u001b[0m        \u001b[36m0.8250\u001b[0m  0.0010  3.5747\n",
            "     11  0.5297       0.8629    0.3821        \u001b[31m0.9067\u001b[0m        \u001b[94m0.7309\u001b[0m        \u001b[36m0.8221\u001b[0m  0.0010  3.6266\n",
            "     12  0.5063       \u001b[32m0.8707\u001b[0m    0.3569        \u001b[31m0.9053\u001b[0m        \u001b[94m0.7266\u001b[0m        \u001b[36m0.8196\u001b[0m  0.0010  4.5630\n",
            "     13  0.4947       \u001b[32m0.8739\u001b[0m    0.3450        \u001b[31m0.9039\u001b[0m        \u001b[94m0.7181\u001b[0m        \u001b[36m0.8170\u001b[0m  0.0010  3.5944\n",
            "     14  0.5292       0.8634    0.3815        \u001b[31m0.9022\u001b[0m        \u001b[94m0.7143\u001b[0m        \u001b[36m0.8140\u001b[0m  0.0010  3.5718\n",
            "     15  0.5221       0.8655    0.3738        \u001b[31m0.9005\u001b[0m        \u001b[94m0.7043\u001b[0m        \u001b[36m0.8109\u001b[0m  0.0001  4.4472\n",
            "     16  0.5181       0.8675    0.3693        \u001b[31m0.9002\u001b[0m        \u001b[94m0.7023\u001b[0m        \u001b[36m0.8103\u001b[0m  0.0001  3.5624\n",
            "     17  0.5201       0.8673    0.3714        \u001b[31m0.8998\u001b[0m        0.7032        \u001b[36m0.8097\u001b[0m  0.0001  3.5583\n",
            "     18  0.5115       0.8692    0.3624        \u001b[31m0.8998\u001b[0m        \u001b[94m0.7011\u001b[0m        \u001b[36m0.8097\u001b[0m  0.0001  4.1897\n",
            "     19  0.5198       0.8682    0.3709        \u001b[31m0.8994\u001b[0m        0.7013        \u001b[36m0.8089\u001b[0m  0.0001  3.7687\n",
            "     20  0.5220       0.8675    0.3733        \u001b[31m0.8990\u001b[0m        \u001b[94m0.6992\u001b[0m        \u001b[36m0.8082\u001b[0m  0.0001  3.6015\n",
            "     21  0.5090       0.8722    0.3594        0.8993        0.6995        0.8088  0.0001  3.8737\n",
            "     22  0.5103       0.8714    0.3608        0.8992        \u001b[94m0.6975\u001b[0m        0.8086  0.0000  4.2996\n",
            "     23  0.5132       0.8713    0.3637        0.8991        0.6978        0.8084  0.0000  3.6174\n",
            "     24  0.5154       0.8709    0.3660        \u001b[31m0.8990\u001b[0m        0.6982        \u001b[36m0.8082\u001b[0m  0.0000  3.5912\n",
            "     25  0.5149       0.8708    0.3655        \u001b[31m0.8990\u001b[0m        0.6989        \u001b[36m0.8081\u001b[0m  0.0000  4.4274\n",
            "     26  0.5160       0.8702    0.3667        \u001b[31m0.8989\u001b[0m        0.6997        \u001b[36m0.8080\u001b[0m  0.0000  3.5610\n",
            "     27  0.5149       0.8709    0.3654        0.8989        0.6999        0.8080  0.0000  3.6002\n",
            "     28  0.5161       0.8701    0.3669        \u001b[31m0.8989\u001b[0m        \u001b[94m0.6972\u001b[0m        \u001b[36m0.8079\u001b[0m  0.0000  4.5061\n",
            "     29  0.5161       0.8702    0.3668        \u001b[31m0.8988\u001b[0m        0.6992        \u001b[36m0.8079\u001b[0m  0.0000  3.5422\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 2/3; 2/36] END lr=0.01, module__dropout=0.3, module__linear_size=400, module__size_emb=60;, score=-1.025 total time= 2.0min\n",
            "[CV 3/3; 2/36] START lr=0.01, module__dropout=0.3, module__linear_size=400, module__size_emb=60\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.3397\u001b[0m       \u001b[32m0.8657\u001b[0m    \u001b[35m0.2113\u001b[0m        \u001b[31m0.9755\u001b[0m        \u001b[94m1.0006\u001b[0m        \u001b[36m0.9515\u001b[0m  0.0100  4.4181\n",
            "      2  \u001b[36m0.3570\u001b[0m       0.8630    \u001b[35m0.2251\u001b[0m        \u001b[31m0.9625\u001b[0m        \u001b[94m0.8758\u001b[0m        \u001b[36m0.9264\u001b[0m  0.0100  3.6352\n",
            "      3  0.3413       \u001b[32m0.8763\u001b[0m    0.2119        \u001b[31m0.9593\u001b[0m        \u001b[94m0.8494\u001b[0m        \u001b[36m0.9202\u001b[0m  0.0100  3.5635\n",
            "      4  \u001b[36m0.5972\u001b[0m       0.8186    \u001b[35m0.4700\u001b[0m        \u001b[31m0.9513\u001b[0m        \u001b[94m0.8395\u001b[0m        \u001b[36m0.9050\u001b[0m  0.0100  4.3429\n",
            "      5  0.5102       0.8378    0.3668        \u001b[31m0.9448\u001b[0m        \u001b[94m0.8236\u001b[0m        \u001b[36m0.8927\u001b[0m  0.0100  3.7298\n",
            "      6  0.3797       \u001b[32m0.8810\u001b[0m    0.2420        0.9550        \u001b[94m0.8111\u001b[0m        0.9119  0.0100  3.5354\n",
            "      7  0.4540       0.8657    0.3077        \u001b[31m0.9358\u001b[0m        \u001b[94m0.7917\u001b[0m        \u001b[36m0.8757\u001b[0m  0.0100  3.8020\n",
            "      8  0.4750       0.8661    0.3272        \u001b[31m0.9244\u001b[0m        \u001b[94m0.7413\u001b[0m        \u001b[36m0.8546\u001b[0m  0.0010  4.2030\n",
            "      9  0.4888       0.8683    0.3401        \u001b[31m0.9218\u001b[0m        \u001b[94m0.7233\u001b[0m        \u001b[36m0.8498\u001b[0m  0.0010  3.5790\n",
            "     10  0.4600       0.8746    0.3121        \u001b[31m0.9198\u001b[0m        \u001b[94m0.7142\u001b[0m        \u001b[36m0.8460\u001b[0m  0.0010  3.5782\n",
            "     11  0.5231       0.8618    0.3756        \u001b[31m0.9140\u001b[0m        \u001b[94m0.7069\u001b[0m        \u001b[36m0.8354\u001b[0m  0.0010  4.4275\n",
            "     12  0.4893       0.8738    0.3397        \u001b[31m0.9111\u001b[0m        \u001b[94m0.7011\u001b[0m        \u001b[36m0.8302\u001b[0m  0.0010  3.6021\n",
            "     13  0.4917       0.8761    0.3417        \u001b[31m0.9104\u001b[0m        \u001b[94m0.6909\u001b[0m        \u001b[36m0.8289\u001b[0m  0.0010  3.5942\n",
            "     14  0.4969       0.8751    0.3469        \u001b[31m0.9085\u001b[0m        \u001b[94m0.6862\u001b[0m        \u001b[36m0.8254\u001b[0m  0.0010  4.4269\n",
            "     15  0.5083       0.8708    0.3589        \u001b[31m0.9078\u001b[0m        \u001b[94m0.6753\u001b[0m        \u001b[36m0.8240\u001b[0m  0.0001  3.5253\n",
            "     16  0.5034       0.8719    0.3539        \u001b[31m0.9076\u001b[0m        \u001b[94m0.6751\u001b[0m        \u001b[36m0.8237\u001b[0m  0.0001  3.6524\n",
            "     17  0.5024       0.8726    0.3527        \u001b[31m0.9072\u001b[0m        \u001b[94m0.6737\u001b[0m        \u001b[36m0.8229\u001b[0m  0.0001  4.2228\n",
            "     18  0.5063       0.8709    0.3568        \u001b[31m0.9061\u001b[0m        \u001b[94m0.6735\u001b[0m        \u001b[36m0.8210\u001b[0m  0.0001  3.9358\n",
            "     19  0.5085       0.8699    0.3593        \u001b[31m0.9060\u001b[0m        \u001b[94m0.6733\u001b[0m        \u001b[36m0.8208\u001b[0m  0.0001  3.6119\n",
            "     20  0.5137       0.8690    0.3646        \u001b[31m0.9053\u001b[0m        \u001b[94m0.6718\u001b[0m        \u001b[36m0.8195\u001b[0m  0.0001  3.3819\n",
            "     21  0.5044       0.8721    0.3548        0.9055        \u001b[94m0.6711\u001b[0m        0.8199  0.0001  4.4839\n",
            "     22  0.5058       0.8716    0.3562        0.9054        \u001b[94m0.6687\u001b[0m        0.8197  0.0000  3.6455\n",
            "     23  0.5059       0.8717    0.3564        0.9053        0.6705        0.8196  0.0000  3.6799\n",
            "     24  0.5059       0.8717    0.3564        0.9053        0.6702        0.8195  0.0000  4.5239\n",
            "     25  0.5060       0.8717    0.3565        \u001b[31m0.9052\u001b[0m        0.6695        \u001b[36m0.8194\u001b[0m  0.0000  3.7096\n",
            "     26  0.5062       0.8718    0.3567        0.9052        \u001b[94m0.6672\u001b[0m        0.8194  0.0000  3.6654\n",
            "     27  0.5072       0.8717    0.3577        0.9052        0.6694        0.8194  0.0000  4.4398\n",
            "     28  0.5076       0.8715    0.3581        \u001b[31m0.9051\u001b[0m        0.6707        \u001b[36m0.8193\u001b[0m  0.0000  3.6554\n",
            "     29  0.5076       0.8716    0.3580        0.9051        0.6695        0.8193  0.0000  3.5259\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 3/3; 2/36] END lr=0.01, module__dropout=0.3, module__linear_size=400, module__size_emb=60;, score=-1.145 total time= 2.0min\n",
            "[CV 1/3; 3/36] START lr=0.01, module__dropout=0.3, module__linear_size=400, module__size_emb=120\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4325\u001b[0m       \u001b[32m0.8499\u001b[0m    \u001b[35m0.2901\u001b[0m        \u001b[31m0.9876\u001b[0m        \u001b[94m0.9893\u001b[0m        \u001b[36m0.9754\u001b[0m  0.0100  3.6263\n",
            "      2  0.3678       \u001b[32m0.8615\u001b[0m    0.2338        \u001b[31m0.9841\u001b[0m        \u001b[94m0.8823\u001b[0m        \u001b[36m0.9685\u001b[0m  0.0100  3.5705\n",
            "      3  \u001b[36m0.5529\u001b[0m       0.8103    \u001b[35m0.4196\u001b[0m        \u001b[31m0.9708\u001b[0m        \u001b[94m0.8633\u001b[0m        \u001b[36m0.9425\u001b[0m  0.0100  4.2200\n",
            "      4  0.3190       \u001b[32m0.8727\u001b[0m    0.1952        0.9751        \u001b[94m0.8519\u001b[0m        0.9509  0.0100  3.7538\n",
            "      5  0.4617       0.8497    0.3169        0.9736        \u001b[94m0.8403\u001b[0m        0.9480  0.0100  3.4973\n",
            "      6  0.5507       0.8320    0.4115        \u001b[31m0.9540\u001b[0m        \u001b[94m0.8258\u001b[0m        \u001b[36m0.9102\u001b[0m  0.0100  3.7781\n",
            "      7  0.4546       0.8591    0.3090        \u001b[31m0.9460\u001b[0m        \u001b[94m0.8134\u001b[0m        \u001b[36m0.8950\u001b[0m  0.0100  4.4761\n",
            "      8  0.5227       0.8524    0.3769        \u001b[31m0.9347\u001b[0m        \u001b[94m0.7553\u001b[0m        \u001b[36m0.8737\u001b[0m  0.0010  3.5566\n",
            "      9  0.5075       0.8587    0.3602        \u001b[31m0.9321\u001b[0m        \u001b[94m0.7348\u001b[0m        \u001b[36m0.8688\u001b[0m  0.0010  3.5548\n",
            "     10  0.5118       0.8574    0.3648        \u001b[31m0.9284\u001b[0m        \u001b[94m0.7245\u001b[0m        \u001b[36m0.8619\u001b[0m  0.0010  4.4557\n",
            "     11  0.4971       0.8578    0.3500        0.9285        \u001b[94m0.7174\u001b[0m        0.8621  0.0010  3.5847\n",
            "     12  0.5090       0.8601    0.3615        \u001b[31m0.9229\u001b[0m        \u001b[94m0.7092\u001b[0m        \u001b[36m0.8518\u001b[0m  0.0010  3.3653\n",
            "     13  0.4947       0.8630    0.3467        \u001b[31m0.9209\u001b[0m        \u001b[94m0.7030\u001b[0m        \u001b[36m0.8480\u001b[0m  0.0010  4.3368\n",
            "     14  0.4904       0.8627    0.3425        0.9213        \u001b[94m0.6976\u001b[0m        0.8487  0.0010  3.7726\n",
            "     15  0.5109       0.8603    0.3633        \u001b[31m0.9188\u001b[0m        \u001b[94m0.6861\u001b[0m        \u001b[36m0.8442\u001b[0m  0.0001  3.6771\n",
            "     16  0.5051       0.8612    0.3573        \u001b[31m0.9185\u001b[0m        \u001b[94m0.6848\u001b[0m        \u001b[36m0.8437\u001b[0m  0.0001  3.9651\n",
            "     17  0.5089       0.8616    0.3611        \u001b[31m0.9177\u001b[0m        0.6849        \u001b[36m0.8421\u001b[0m  0.0001  4.0714\n",
            "     18  0.5133       0.8600    0.3658        \u001b[31m0.9175\u001b[0m        \u001b[94m0.6841\u001b[0m        \u001b[36m0.8418\u001b[0m  0.0001  3.6378\n",
            "     19  0.5168       0.8594    0.3695        \u001b[31m0.9171\u001b[0m        \u001b[94m0.6803\u001b[0m        \u001b[36m0.8410\u001b[0m  0.0001  3.6421\n",
            "     20  0.5054       0.8626    0.3574        \u001b[31m0.9168\u001b[0m        0.6817        \u001b[36m0.8405\u001b[0m  0.0001  4.5345\n",
            "     21  0.5165       0.8583    0.3694        \u001b[31m0.9163\u001b[0m        0.6808        \u001b[36m0.8396\u001b[0m  0.0001  3.6093\n",
            "     22  0.5155       0.8590    0.3682        \u001b[31m0.9163\u001b[0m        0.6811        \u001b[36m0.8396\u001b[0m  0.0000  3.6584\n",
            "     23  0.5126       0.8598    0.3652        \u001b[31m0.9163\u001b[0m        \u001b[94m0.6780\u001b[0m        \u001b[36m0.8396\u001b[0m  0.0000  4.4879\n",
            "     24  0.5123       0.8597    0.3648        0.9163        0.6790        0.8396  0.0000  3.6156\n",
            "     25  0.5126       0.8598    0.3652        \u001b[31m0.9163\u001b[0m        \u001b[94m0.6773\u001b[0m        \u001b[36m0.8396\u001b[0m  0.0000  3.6430\n",
            "     26  0.5121       0.8603    0.3645        \u001b[31m0.9162\u001b[0m        0.6799        \u001b[36m0.8395\u001b[0m  0.0000  4.3460\n",
            "     27  0.5107       0.8606    0.3631        \u001b[31m0.9162\u001b[0m        0.6802        \u001b[36m0.8394\u001b[0m  0.0000  3.7779\n",
            "     28  0.5119       0.8604    0.3643        \u001b[31m0.9162\u001b[0m        0.6791        \u001b[36m0.8394\u001b[0m  0.0000  3.7048\n",
            "     29  0.5118       0.8606    0.3642        \u001b[31m0.9162\u001b[0m        \u001b[94m0.6770\u001b[0m        \u001b[36m0.8394\u001b[0m  0.0000  4.0412\n",
            "     30  0.5117       0.8606    0.3641        \u001b[31m0.9162\u001b[0m        0.6781        \u001b[36m0.8393\u001b[0m  0.0000  4.0514\n",
            "[CV 1/3; 3/36] END lr=0.01, module__dropout=0.3, module__linear_size=400, module__size_emb=120;, score=-1.169 total time= 2.0min\n",
            "[CV 2/3; 3/36] START lr=0.01, module__dropout=0.3, module__linear_size=400, module__size_emb=120\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.5499\u001b[0m       \u001b[32m0.8190\u001b[0m    \u001b[35m0.4139\u001b[0m        \u001b[31m0.9576\u001b[0m        \u001b[94m1.0141\u001b[0m        \u001b[36m0.9170\u001b[0m  0.0100  3.6557\n",
            "      2  0.0672       \u001b[32m0.9070\u001b[0m    0.0349        1.0071        \u001b[94m0.8920\u001b[0m        1.0143  0.0100  4.1997\n",
            "      3  0.5239       0.8443    0.3798        \u001b[31m0.9397\u001b[0m        \u001b[94m0.8762\u001b[0m        \u001b[36m0.8830\u001b[0m  0.0100  3.7481\n",
            "      4  0.4758       0.8648    0.3282        \u001b[31m0.9343\u001b[0m        \u001b[94m0.8507\u001b[0m        \u001b[36m0.8729\u001b[0m  0.0100  3.6235\n",
            "      5  0.4616       0.8642    0.3150        \u001b[31m0.9313\u001b[0m        \u001b[94m0.8407\u001b[0m        \u001b[36m0.8674\u001b[0m  0.0100  4.0011\n",
            "      6  0.5239       0.8525    0.3782        \u001b[31m0.9296\u001b[0m        \u001b[94m0.8285\u001b[0m        \u001b[36m0.8642\u001b[0m  0.0100  4.0708\n",
            "      7  0.4867       0.8651    0.3386        \u001b[31m0.9198\u001b[0m        \u001b[94m0.8176\u001b[0m        \u001b[36m0.8460\u001b[0m  0.0100  3.6242\n",
            "      8  0.4902       0.8754    0.3404        \u001b[31m0.9127\u001b[0m        \u001b[94m0.7629\u001b[0m        \u001b[36m0.8331\u001b[0m  0.0010  3.6605\n",
            "      9  \u001b[36m0.5549\u001b[0m       0.8588    0.4098        \u001b[31m0.9072\u001b[0m        \u001b[94m0.7439\u001b[0m        \u001b[36m0.8230\u001b[0m  0.0010  4.4170\n",
            "     10  0.5195       0.8712    0.3701        \u001b[31m0.9036\u001b[0m        \u001b[94m0.7325\u001b[0m        \u001b[36m0.8165\u001b[0m  0.0010  3.5472\n",
            "     11  0.5329       0.8670    0.3847        \u001b[31m0.9007\u001b[0m        \u001b[94m0.7261\u001b[0m        \u001b[36m0.8112\u001b[0m  0.0010  3.6106\n",
            "     12  0.5180       0.8736    0.3681        \u001b[31m0.8990\u001b[0m        \u001b[94m0.7195\u001b[0m        \u001b[36m0.8082\u001b[0m  0.0010  4.4770\n",
            "     13  0.5441       0.8678    0.3962        \u001b[31m0.8971\u001b[0m        \u001b[94m0.7122\u001b[0m        \u001b[36m0.8048\u001b[0m  0.0010  3.6944\n",
            "     14  0.5001       0.8798    0.3493        \u001b[31m0.8962\u001b[0m        \u001b[94m0.7062\u001b[0m        \u001b[36m0.8031\u001b[0m  0.0010  3.7262\n",
            "     15  0.5313       0.8748    0.3815        \u001b[31m0.8936\u001b[0m        \u001b[94m0.6946\u001b[0m        \u001b[36m0.7985\u001b[0m  0.0001  4.4307\n",
            "     16  0.5355       0.8746    0.3859        \u001b[31m0.8930\u001b[0m        \u001b[94m0.6934\u001b[0m        \u001b[36m0.7974\u001b[0m  0.0001  3.6578\n",
            "     17  0.5292       0.8771    0.3789        \u001b[31m0.8930\u001b[0m        \u001b[94m0.6931\u001b[0m        \u001b[36m0.7974\u001b[0m  0.0001  3.6804\n",
            "     18  0.5280       0.8773    0.3776        \u001b[31m0.8926\u001b[0m        \u001b[94m0.6911\u001b[0m        \u001b[36m0.7967\u001b[0m  0.0001  3.9378\n",
            "     19  0.5350       0.8746    0.3854        \u001b[31m0.8921\u001b[0m        0.6919        \u001b[36m0.7959\u001b[0m  0.0001  4.0932\n",
            "     20  0.5355       0.8749    0.3859        \u001b[31m0.8918\u001b[0m        \u001b[94m0.6894\u001b[0m        \u001b[36m0.7953\u001b[0m  0.0001  3.4295\n",
            "     21  0.5326       0.8758    0.3826        \u001b[31m0.8915\u001b[0m        \u001b[94m0.6878\u001b[0m        \u001b[36m0.7947\u001b[0m  0.0001  3.8061\n",
            "     22  0.5336       0.8759    0.3836        \u001b[31m0.8914\u001b[0m        0.6883        \u001b[36m0.7946\u001b[0m  0.0000  4.5908\n",
            "     23  0.5323       0.8763    0.3823        \u001b[31m0.8914\u001b[0m        0.6885        \u001b[36m0.7946\u001b[0m  0.0000  3.5987\n",
            "     24  0.5339       0.8760    0.3840        \u001b[31m0.8914\u001b[0m        \u001b[94m0.6870\u001b[0m        \u001b[36m0.7945\u001b[0m  0.0000  3.6568\n",
            "     25  0.5331       0.8760    0.3832        \u001b[31m0.8914\u001b[0m        \u001b[94m0.6864\u001b[0m        \u001b[36m0.7945\u001b[0m  0.0000  4.5124\n",
            "     26  0.5342       0.8761    0.3842        \u001b[31m0.8913\u001b[0m        0.6871        \u001b[36m0.7944\u001b[0m  0.0000  3.6669\n",
            "     27  0.5346       0.8760    0.3847        \u001b[31m0.8913\u001b[0m        0.6876        \u001b[36m0.7944\u001b[0m  0.0000  3.7114\n",
            "     28  0.5341       0.8761    0.3842        \u001b[31m0.8912\u001b[0m        \u001b[94m0.6861\u001b[0m        \u001b[36m0.7943\u001b[0m  0.0000  4.5537\n",
            "     29  0.5341       0.8761    0.3842        \u001b[31m0.8912\u001b[0m        \u001b[94m0.6859\u001b[0m        \u001b[36m0.7943\u001b[0m  0.0000  3.6964\n",
            "     30  0.5340       0.8762    0.3840        0.8912        0.6878        0.7943  0.0000  3.6916\n",
            "[CV 2/3; 3/36] END lr=0.01, module__dropout=0.3, module__linear_size=400, module__size_emb=120;, score=-1.038 total time= 2.0min\n",
            "[CV 3/3; 3/36] START lr=0.01, module__dropout=0.3, module__linear_size=400, module__size_emb=120\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.2787\u001b[0m       \u001b[32m0.8752\u001b[0m    \u001b[35m0.1657\u001b[0m        \u001b[31m0.9826\u001b[0m        \u001b[94m0.9977\u001b[0m        \u001b[36m0.9656\u001b[0m  0.0100  4.4651\n",
            "      2  \u001b[36m0.3714\u001b[0m       0.8626    \u001b[35m0.2366\u001b[0m        \u001b[31m0.9650\u001b[0m        \u001b[94m0.8764\u001b[0m        \u001b[36m0.9312\u001b[0m  0.0100  3.5783\n",
            "      3  \u001b[36m0.5166\u001b[0m       0.8384    \u001b[35m0.3733\u001b[0m        \u001b[31m0.9493\u001b[0m        \u001b[94m0.8513\u001b[0m        \u001b[36m0.9011\u001b[0m  0.0100  3.5910\n",
            "      4  0.3614       0.8722    0.2279        0.9557        \u001b[94m0.8394\u001b[0m        0.9133  0.0100  4.4597\n",
            "      5  0.3775       \u001b[32m0.8781\u001b[0m    0.2405        \u001b[31m0.9477\u001b[0m        \u001b[94m0.8222\u001b[0m        \u001b[36m0.8981\u001b[0m  0.0100  3.6071\n",
            "      6  0.3893       \u001b[32m0.8794\u001b[0m    0.2500        \u001b[31m0.9473\u001b[0m        \u001b[94m0.8107\u001b[0m        \u001b[36m0.8974\u001b[0m  0.0100  3.5398\n",
            "      7  \u001b[36m0.5672\u001b[0m       0.8386    \u001b[35m0.4286\u001b[0m        \u001b[31m0.9289\u001b[0m        \u001b[94m0.7969\u001b[0m        \u001b[36m0.8628\u001b[0m  0.0100  4.0154\n",
            "      8  0.4809       0.8668    0.3328        \u001b[31m0.9209\u001b[0m        \u001b[94m0.7372\u001b[0m        \u001b[36m0.8481\u001b[0m  0.0010  3.9799\n",
            "      9  0.4786       0.8706    0.3300        \u001b[31m0.9178\u001b[0m        \u001b[94m0.7216\u001b[0m        \u001b[36m0.8424\u001b[0m  0.0010  3.6527\n",
            "     10  0.5220       0.8638    0.3740        \u001b[31m0.9122\u001b[0m        \u001b[94m0.7113\u001b[0m        \u001b[36m0.8320\u001b[0m  0.0010  3.7246\n",
            "     11  0.5212       0.8645    0.3731        \u001b[31m0.9108\u001b[0m        \u001b[94m0.7042\u001b[0m        \u001b[36m0.8295\u001b[0m  0.0010  4.4551\n",
            "     12  0.5521       0.8567    0.4073        \u001b[31m0.9082\u001b[0m        \u001b[94m0.6972\u001b[0m        \u001b[36m0.8248\u001b[0m  0.0010  3.6233\n",
            "     13  0.5064       0.8722    0.3568        \u001b[31m0.9066\u001b[0m        \u001b[94m0.6950\u001b[0m        \u001b[36m0.8219\u001b[0m  0.0010  3.4273\n",
            "     14  0.4976       0.8772    0.3473        \u001b[31m0.9050\u001b[0m        \u001b[94m0.6871\u001b[0m        \u001b[36m0.8191\u001b[0m  0.0010  4.5084\n",
            "     15  0.5126       0.8724    0.3629        \u001b[31m0.9037\u001b[0m        \u001b[94m0.6771\u001b[0m        \u001b[36m0.8167\u001b[0m  0.0001  3.6167\n",
            "     16  0.5158       0.8712    0.3663        \u001b[31m0.9030\u001b[0m        \u001b[94m0.6739\u001b[0m        \u001b[36m0.8155\u001b[0m  0.0001  3.6245\n",
            "     17  0.5115       0.8735    0.3616        \u001b[31m0.9028\u001b[0m        0.6741        \u001b[36m0.8151\u001b[0m  0.0001  4.4044\n",
            "     18  0.5167       0.8715    0.3672        \u001b[31m0.9025\u001b[0m        0.6742        \u001b[36m0.8145\u001b[0m  0.0001  3.6561\n",
            "     19  0.5125       0.8740    0.3626        \u001b[31m0.9023\u001b[0m        0.6746        \u001b[36m0.8141\u001b[0m  0.0001  3.6279\n",
            "     20  0.5112       0.8741    0.3612        \u001b[31m0.9022\u001b[0m        \u001b[94m0.6735\u001b[0m        \u001b[36m0.8140\u001b[0m  0.0001  4.1107\n",
            "     21  0.5163       0.8728    0.3666        \u001b[31m0.9021\u001b[0m        \u001b[94m0.6708\u001b[0m        \u001b[36m0.8137\u001b[0m  0.0001  4.0793\n",
            "     22  0.5170       0.8728    0.3673        \u001b[31m0.9020\u001b[0m        \u001b[94m0.6706\u001b[0m        \u001b[36m0.8136\u001b[0m  0.0000  3.5997\n",
            "     23  0.5154       0.8730    0.3656        0.9020        0.6713        0.8137  0.0000  3.6489\n",
            "     24  0.5174       0.8727    0.3677        \u001b[31m0.9020\u001b[0m        \u001b[94m0.6703\u001b[0m        \u001b[36m0.8135\u001b[0m  0.0000  4.5422\n",
            "     25  0.5167       0.8727    0.3670        \u001b[31m0.9019\u001b[0m        0.6713        \u001b[36m0.8135\u001b[0m  0.0000  3.5922\n",
            "     26  0.5158       0.8730    0.3661        \u001b[31m0.9019\u001b[0m        0.6713        \u001b[36m0.8134\u001b[0m  0.0000  3.6025\n",
            "     27  0.5163       0.8726    0.3666        \u001b[31m0.9019\u001b[0m        \u001b[94m0.6692\u001b[0m        \u001b[36m0.8134\u001b[0m  0.0000  4.4689\n",
            "     28  0.5160       0.8725    0.3663        \u001b[31m0.9019\u001b[0m        0.6700        \u001b[36m0.8134\u001b[0m  0.0000  3.6702\n",
            "     29  0.5160       0.8725    0.3663        \u001b[31m0.9019\u001b[0m        0.6698        \u001b[36m0.8134\u001b[0m  0.0000  3.6786\n",
            "     30  0.5160       0.8725    0.3663        \u001b[31m0.9019\u001b[0m        0.6694        \u001b[36m0.8134\u001b[0m  0.0000  4.4988\n",
            "[CV 3/3; 3/36] END lr=0.01, module__dropout=0.3, module__linear_size=400, module__size_emb=120;, score=-1.126 total time= 2.0min\n",
            "[CV 1/3; 4/36] START lr=0.01, module__dropout=0.3, module__linear_size=500, module__size_emb=30\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.3061\u001b[0m       \u001b[32m0.8764\u001b[0m    \u001b[35m0.1854\u001b[0m        \u001b[31m0.9966\u001b[0m        \u001b[94m1.0350\u001b[0m        \u001b[36m0.9933\u001b[0m  0.0100  3.4029\n",
            "      2  \u001b[36m0.5603\u001b[0m       0.8145    \u001b[35m0.4270\u001b[0m        \u001b[31m0.9753\u001b[0m        \u001b[94m0.8850\u001b[0m        \u001b[36m0.9513\u001b[0m  0.0100  3.8682\n",
            "      3  0.4401       0.8476    0.2972        0.9875        \u001b[94m0.8618\u001b[0m        0.9752  0.0100  4.2207\n",
            "      4  0.5226       0.8355    0.3802        0.9799        \u001b[94m0.8458\u001b[0m        0.9602  0.0100  3.5388\n",
            "      5  0.5313       0.8325    0.3902        \u001b[31m0.9644\u001b[0m        \u001b[94m0.8340\u001b[0m        \u001b[36m0.9300\u001b[0m  0.0100  3.5434\n",
            "      6  0.5542       0.8338    0.4150        \u001b[31m0.9569\u001b[0m        \u001b[94m0.8192\u001b[0m        \u001b[36m0.9157\u001b[0m  0.0100  4.4698\n",
            "      7  \u001b[36m0.5646\u001b[0m       0.8280    \u001b[35m0.4283\u001b[0m        \u001b[31m0.9479\u001b[0m        \u001b[94m0.8033\u001b[0m        \u001b[36m0.8984\u001b[0m  0.0100  3.7896\n",
            "      8  0.5386       0.8468    0.3949        \u001b[31m0.9371\u001b[0m        \u001b[94m0.7464\u001b[0m        \u001b[36m0.8782\u001b[0m  0.0010  3.6495\n",
            "      9  0.5155       0.8532    0.3693        \u001b[31m0.9330\u001b[0m        \u001b[94m0.7316\u001b[0m        \u001b[36m0.8706\u001b[0m  0.0010  4.1019\n",
            "     10  0.5067       0.8552    0.3600        \u001b[31m0.9313\u001b[0m        \u001b[94m0.7230\u001b[0m        \u001b[36m0.8674\u001b[0m  0.0010  4.1652\n",
            "     11  0.5286       0.8529    0.3830        \u001b[31m0.9312\u001b[0m        \u001b[94m0.7162\u001b[0m        \u001b[36m0.8671\u001b[0m  0.0010  3.6547\n",
            "     12  0.5299       0.8546    0.3840        \u001b[31m0.9251\u001b[0m        \u001b[94m0.7077\u001b[0m        \u001b[36m0.8558\u001b[0m  0.0010  3.7180\n",
            "     13  0.5558       0.8498    0.4129        \u001b[31m0.9235\u001b[0m        \u001b[94m0.7039\u001b[0m        \u001b[36m0.8528\u001b[0m  0.0010  4.4669\n",
            "     14  \u001b[36m0.5719\u001b[0m       0.8458    \u001b[35m0.4321\u001b[0m        0.9250        \u001b[94m0.6986\u001b[0m        0.8557  0.0010  3.6129\n",
            "     15  0.5533       0.8520    0.4097        \u001b[31m0.9217\u001b[0m        \u001b[94m0.6886\u001b[0m        \u001b[36m0.8496\u001b[0m  0.0001  3.5868\n",
            "     16  0.5593       0.8494    0.4169        \u001b[31m0.9216\u001b[0m        \u001b[94m0.6868\u001b[0m        \u001b[36m0.8493\u001b[0m  0.0001  4.6189\n",
            "     17  0.5560       0.8519    0.4126        \u001b[31m0.9213\u001b[0m        \u001b[94m0.6850\u001b[0m        \u001b[36m0.8487\u001b[0m  0.0001  3.7446\n",
            "     18  0.5541       0.8525    0.4105        \u001b[31m0.9211\u001b[0m        0.6862        \u001b[36m0.8484\u001b[0m  0.0001  3.6491\n",
            "     19  0.5613       0.8495    0.4191        \u001b[31m0.9209\u001b[0m        0.6856        \u001b[36m0.8481\u001b[0m  0.0001  4.5296\n",
            "     20  0.5580       0.8517    0.4150        \u001b[31m0.9203\u001b[0m        \u001b[94m0.6836\u001b[0m        \u001b[36m0.8470\u001b[0m  0.0001  3.4254\n",
            "     21  0.5593       0.8518    0.4163        0.9204        \u001b[94m0.6824\u001b[0m        0.8472  0.0001  3.6722\n",
            "     22  0.5593       0.8517    0.4164        0.9203        \u001b[94m0.6818\u001b[0m        0.8470  0.0000  4.2077\n",
            "     23  0.5599       0.8514    0.4171        \u001b[31m0.9202\u001b[0m        0.6819        \u001b[36m0.8469\u001b[0m  0.0000  4.1935\n",
            "     24  0.5599       0.8516    0.4171        \u001b[31m0.9202\u001b[0m        \u001b[94m0.6811\u001b[0m        \u001b[36m0.8467\u001b[0m  0.0000  3.7376\n",
            "     25  0.5599       0.8514    0.4171        0.9202        0.6823        0.8468  0.0000  3.9717\n",
            "     26  0.5600       0.8513    0.4172        0.9202        0.6812        0.8467  0.0000  4.4341\n",
            "     27  0.5603       0.8515    0.4175        \u001b[31m0.9202\u001b[0m        \u001b[94m0.6796\u001b[0m        \u001b[36m0.8467\u001b[0m  0.0000  3.7935\n",
            "     28  0.5601       0.8516    0.4173        \u001b[31m0.9201\u001b[0m        0.6817        \u001b[36m0.8467\u001b[0m  0.0000  3.8134\n",
            "     29  0.5602       0.8517    0.4174        \u001b[31m0.9201\u001b[0m        \u001b[94m0.6792\u001b[0m        \u001b[36m0.8466\u001b[0m  0.0000  4.5496\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 1/3; 4/36] END lr=0.01, module__dropout=0.3, module__linear_size=500, module__size_emb=30;, score=-1.183 total time= 2.0min\n",
            "[CV 2/3; 4/36] START lr=0.01, module__dropout=0.3, module__linear_size=500, module__size_emb=30\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.3751\u001b[0m       \u001b[32m0.8553\u001b[0m    \u001b[35m0.2402\u001b[0m        \u001b[31m0.9652\u001b[0m        \u001b[94m1.0857\u001b[0m        \u001b[36m0.9315\u001b[0m  0.0100  4.4949\n",
            "      2  \u001b[36m0.4797\u001b[0m       0.8427    \u001b[35m0.3353\u001b[0m        \u001b[31m0.9552\u001b[0m        \u001b[94m0.8911\u001b[0m        \u001b[36m0.9124\u001b[0m  0.0100  3.9379\n",
            "      3  0.2847       \u001b[32m0.8791\u001b[0m    0.1698        0.9568        \u001b[94m0.8602\u001b[0m        0.9155  0.0100  3.7936\n",
            "      4  0.4206       0.8637    0.2780        \u001b[31m0.9437\u001b[0m        \u001b[94m0.8444\u001b[0m        \u001b[36m0.8906\u001b[0m  0.0100  4.2577\n",
            "      5  0.3995       0.8740    0.2589        \u001b[31m0.9381\u001b[0m        \u001b[94m0.8338\u001b[0m        \u001b[36m0.8800\u001b[0m  0.0100  4.1068\n",
            "      6  \u001b[36m0.5543\u001b[0m       0.8396    \u001b[35m0.4137\u001b[0m        \u001b[31m0.9263\u001b[0m        \u001b[94m0.8214\u001b[0m        \u001b[36m0.8579\u001b[0m  0.0100  3.7115\n",
            "      7  0.4531       0.8700    0.3063        \u001b[31m0.9228\u001b[0m        \u001b[94m0.8011\u001b[0m        \u001b[36m0.8516\u001b[0m  0.0100  3.7781\n",
            "      8  0.4552       0.8716    0.3081        \u001b[31m0.9165\u001b[0m        \u001b[94m0.7493\u001b[0m        \u001b[36m0.8401\u001b[0m  0.0010  4.3803\n",
            "      9  0.5098       0.8594    0.3624        \u001b[31m0.9095\u001b[0m        \u001b[94m0.7352\u001b[0m        \u001b[36m0.8272\u001b[0m  0.0010  3.5027\n",
            "     10  0.4734       0.8705    0.3251        \u001b[31m0.9093\u001b[0m        \u001b[94m0.7266\u001b[0m        \u001b[36m0.8268\u001b[0m  0.0010  3.7855\n",
            "     11  0.4778       0.8705    0.3293        \u001b[31m0.9068\u001b[0m        \u001b[94m0.7193\u001b[0m        \u001b[36m0.8223\u001b[0m  0.0010  4.5597\n",
            "     12  0.4685       0.8745    0.3200        \u001b[31m0.9053\u001b[0m        \u001b[94m0.7141\u001b[0m        \u001b[36m0.8195\u001b[0m  0.0010  3.7022\n",
            "     13  0.4630       0.8765    0.3146        0.9053        \u001b[94m0.7082\u001b[0m        0.8195  0.0010  3.7072\n",
            "     14  0.5215       0.8660    0.3731        \u001b[31m0.8989\u001b[0m        \u001b[94m0.7046\u001b[0m        \u001b[36m0.8081\u001b[0m  0.0010  4.4892\n",
            "     15  0.5093       0.8689    0.3603        0.9000        \u001b[94m0.6939\u001b[0m        0.8100  0.0001  3.7372\n",
            "     16  0.5016       0.8702    0.3524        0.9002        \u001b[94m0.6898\u001b[0m        0.8104  0.0001  3.6695\n",
            "     17  0.5100       0.8686    0.3610        0.8995        \u001b[94m0.6888\u001b[0m        0.8090  0.0001  4.3856\n",
            "     18  0.5180       0.8666    0.3694        \u001b[31m0.8986\u001b[0m        \u001b[94m0.6888\u001b[0m        \u001b[36m0.8075\u001b[0m  0.0001  3.7933\n",
            "     19  0.5185       0.8668    0.3699        \u001b[31m0.8985\u001b[0m        \u001b[94m0.6880\u001b[0m        \u001b[36m0.8073\u001b[0m  0.0001  3.7549\n",
            "     20  0.5141       0.8672    0.3654        0.8987        \u001b[94m0.6867\u001b[0m        0.8076  0.0001  4.2232\n",
            "     21  0.5156       0.8671    0.3669        \u001b[31m0.8984\u001b[0m        \u001b[94m0.6855\u001b[0m        \u001b[36m0.8071\u001b[0m  0.0001  4.1525\n",
            "     22  0.5167       0.8666    0.3681        \u001b[31m0.8983\u001b[0m        0.6860        \u001b[36m0.8069\u001b[0m  0.0000  3.6531\n",
            "     23  0.5172       0.8667    0.3686        \u001b[31m0.8982\u001b[0m        \u001b[94m0.6838\u001b[0m        \u001b[36m0.8068\u001b[0m  0.0000  3.9258\n",
            "     24  0.5163       0.8668    0.3677        0.8983        0.6871        0.8069  0.0000  4.3229\n",
            "     25  0.5159       0.8670    0.3672        0.8983        0.6848        0.8069  0.0000  3.6478\n",
            "     26  0.5159       0.8669    0.3672        0.8982        0.6854        0.8068  0.0000  3.7348\n",
            "     27  0.5163       0.8668    0.3677        \u001b[31m0.8982\u001b[0m        0.6858        \u001b[36m0.8068\u001b[0m  0.0000  4.6710\n",
            "     28  0.5153       0.8670    0.3666        0.8982        0.6841        0.8068  0.0000  3.8224\n",
            "     29  0.5153       0.8670    0.3666        0.8982        0.6851        0.8068  0.0000  3.8070\n",
            "     30  0.5151       0.8670    0.3664        0.8982        0.6858        0.8069  0.0000  4.6883\n",
            "[CV 2/3; 4/36] END lr=0.01, module__dropout=0.3, module__linear_size=500, module__size_emb=30;, score=-1.055 total time= 2.1min\n",
            "[CV 3/3; 4/36] START lr=0.01, module__dropout=0.3, module__linear_size=500, module__size_emb=30\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.2428\u001b[0m       \u001b[32m0.8655\u001b[0m    \u001b[35m0.1412\u001b[0m        \u001b[31m0.9841\u001b[0m        \u001b[94m1.0476\u001b[0m        \u001b[36m0.9684\u001b[0m  0.0100  3.7857\n",
            "      2  \u001b[36m0.3959\u001b[0m       0.8543    \u001b[35m0.2577\u001b[0m        \u001b[31m0.9558\u001b[0m        \u001b[94m0.8674\u001b[0m        \u001b[36m0.9136\u001b[0m  0.0100  4.1822\n",
            "      3  0.3353       \u001b[32m0.8763\u001b[0m    0.2073        0.9585        \u001b[94m0.8416\u001b[0m        0.9188  0.0100  4.0715\n",
            "      4  0.2782       \u001b[32m0.8811\u001b[0m    0.1652        0.9618        \u001b[94m0.8314\u001b[0m        0.9251  0.0100  3.7574\n",
            "      5  \u001b[36m0.5191\u001b[0m       0.8397    \u001b[35m0.3756\u001b[0m        \u001b[31m0.9367\u001b[0m        \u001b[94m0.8149\u001b[0m        \u001b[36m0.8775\u001b[0m  0.0100  3.9037\n",
            "      6  0.4155       0.8713    0.2728        0.9389        \u001b[94m0.8024\u001b[0m        0.8816  0.0100  4.4407\n",
            "      7  0.5047       0.8523    0.3585        \u001b[31m0.9258\u001b[0m        \u001b[94m0.7899\u001b[0m        \u001b[36m0.8572\u001b[0m  0.0100  3.7831\n",
            "      8  0.4287       0.8741    0.2840        \u001b[31m0.9247\u001b[0m        \u001b[94m0.7360\u001b[0m        \u001b[36m0.8550\u001b[0m  0.0010  3.7916\n",
            "      9  0.4431       0.8755    0.2966        \u001b[31m0.9208\u001b[0m        \u001b[94m0.7207\u001b[0m        \u001b[36m0.8479\u001b[0m  0.0010  4.5884\n",
            "     10  0.4587       0.8721    0.3112        \u001b[31m0.9195\u001b[0m        \u001b[94m0.7131\u001b[0m        \u001b[36m0.8456\u001b[0m  0.0010  3.7743\n",
            "     11  0.4306       0.8768    0.2854        \u001b[31m0.9193\u001b[0m        \u001b[94m0.7054\u001b[0m        \u001b[36m0.8450\u001b[0m  0.0010  3.7256\n",
            "     12  0.4796       0.8720    0.3307        \u001b[31m0.9125\u001b[0m        \u001b[94m0.6991\u001b[0m        \u001b[36m0.8326\u001b[0m  0.0010  4.5271\n",
            "     13  0.4376       0.8760    0.2917        0.9136        \u001b[94m0.6922\u001b[0m        0.8347  0.0010  3.4387\n",
            "     14  0.4536       0.8768    0.3059        \u001b[31m0.9092\u001b[0m        \u001b[94m0.6862\u001b[0m        \u001b[36m0.8267\u001b[0m  0.0010  3.7985\n",
            "     15  0.4610       0.8722    0.3133        \u001b[31m0.9080\u001b[0m        \u001b[94m0.6752\u001b[0m        \u001b[36m0.8245\u001b[0m  0.0001  4.5849\n",
            "     16  0.4660       0.8725    0.3179        \u001b[31m0.9078\u001b[0m        \u001b[94m0.6725\u001b[0m        \u001b[36m0.8242\u001b[0m  0.0001  3.7094\n",
            "     17  0.4642       0.8731    0.3161        \u001b[31m0.9071\u001b[0m        0.6725        \u001b[36m0.8228\u001b[0m  0.0001  3.7176\n",
            "     18  0.4610       0.8732    0.3132        0.9079        0.6729        0.8242  0.0001  4.2946\n",
            "     19  0.4658       0.8724    0.3177        \u001b[31m0.9066\u001b[0m        0.6725        \u001b[36m0.8219\u001b[0m  0.0001  4.0173\n",
            "     20  0.4620       0.8733    0.3141        0.9072        \u001b[94m0.6703\u001b[0m        0.8229  0.0001  3.7889\n",
            "     21  0.4708       0.8713    0.3226        \u001b[31m0.9059\u001b[0m        0.6705        \u001b[36m0.8207\u001b[0m  0.0001  4.0545\n",
            "     22  0.4697       0.8718    0.3215        0.9060        \u001b[94m0.6695\u001b[0m        0.8209  0.0000  4.1850\n",
            "     23  0.4682       0.8717    0.3201        0.9061        0.6703        0.8210  0.0000  3.8712\n",
            "     24  0.4680       0.8721    0.3198        0.9061        \u001b[94m0.6680\u001b[0m        0.8210  0.0000  3.8153\n",
            "     25  0.4673       0.8721    0.3192        0.9062        0.6689        0.8212  0.0000  4.4161\n",
            "     26  0.4671       0.8722    0.3190        0.9062        0.6688        0.8211  0.0000  3.6815\n",
            "     27  0.4675       0.8719    0.3193        0.9061        0.6684        0.8210  0.0000  3.6558\n",
            "     28  0.4685       0.8716    0.3203        0.9060        0.6685        0.8208  0.0000  4.5295\n",
            "     29  0.4684       0.8720    0.3202        0.9060        0.6694        0.8208  0.0000  3.7289\n",
            "     30  0.4679       0.8719    0.3198        0.9060        0.6689        0.8209  0.0000  3.6818\n",
            "[CV 3/3; 4/36] END lr=0.01, module__dropout=0.3, module__linear_size=500, module__size_emb=30;, score=-1.136 total time= 2.0min\n",
            "[CV 1/3; 5/36] START lr=0.01, module__dropout=0.3, module__linear_size=500, module__size_emb=60\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.3743\u001b[0m       \u001b[32m0.8624\u001b[0m    \u001b[35m0.2390\u001b[0m        \u001b[31m0.9927\u001b[0m        \u001b[94m1.0048\u001b[0m        \u001b[36m0.9854\u001b[0m  0.0100  4.1636\n",
            "      2  0.3579       0.8620    0.2259        \u001b[31m0.9818\u001b[0m        \u001b[94m0.8837\u001b[0m        \u001b[36m0.9640\u001b[0m  0.0100  3.6744\n",
            "      3  0.3524       \u001b[32m0.8768\u001b[0m    0.2205        \u001b[31m0.9718\u001b[0m        \u001b[94m0.8640\u001b[0m        \u001b[36m0.9444\u001b[0m  0.0100  3.7014\n",
            "      4  \u001b[36m0.5655\u001b[0m       0.8189    \u001b[35m0.4319\u001b[0m        0.9759        \u001b[94m0.8515\u001b[0m        0.9523  0.0100  4.4939\n",
            "      5  0.5382       0.8269    0.3989        \u001b[31m0.9601\u001b[0m        \u001b[94m0.8393\u001b[0m        \u001b[36m0.9218\u001b[0m  0.0100  3.4792\n",
            "      6  0.4952       0.8454    0.3501        \u001b[31m0.9589\u001b[0m        \u001b[94m0.8248\u001b[0m        \u001b[36m0.9194\u001b[0m  0.0100  3.5599\n",
            "      7  0.5080       0.8424    0.3637        \u001b[31m0.9496\u001b[0m        \u001b[94m0.8137\u001b[0m        \u001b[36m0.9017\u001b[0m  0.0100  4.6165\n",
            "      8  0.5158       0.8462    0.3710        \u001b[31m0.9371\u001b[0m        \u001b[94m0.7549\u001b[0m        \u001b[36m0.8782\u001b[0m  0.0010  3.7397\n",
            "      9  0.5003       0.8531    0.3540        \u001b[31m0.9331\u001b[0m        \u001b[94m0.7381\u001b[0m        \u001b[36m0.8707\u001b[0m  0.0010  3.7392\n",
            "     10  0.5022       0.8558    0.3553        \u001b[31m0.9289\u001b[0m        \u001b[94m0.7296\u001b[0m        \u001b[36m0.8629\u001b[0m  0.0010  4.5626\n",
            "     11  0.5256       0.8526    0.3799        \u001b[31m0.9272\u001b[0m        \u001b[94m0.7221\u001b[0m        \u001b[36m0.8597\u001b[0m  0.0010  3.7378\n",
            "     12  0.5187       0.8553    0.3722        \u001b[31m0.9250\u001b[0m        \u001b[94m0.7177\u001b[0m        \u001b[36m0.8557\u001b[0m  0.0010  3.6853\n",
            "     13  0.5174       0.8556    0.3708        \u001b[31m0.9230\u001b[0m        \u001b[94m0.7088\u001b[0m        \u001b[36m0.8519\u001b[0m  0.0010  4.2257\n",
            "     14  0.5094       0.8591    0.3620        \u001b[31m0.9198\u001b[0m        \u001b[94m0.7036\u001b[0m        \u001b[36m0.8461\u001b[0m  0.0010  3.9675\n",
            "     15  0.5195       0.8576    0.3726        \u001b[31m0.9188\u001b[0m        \u001b[94m0.6917\u001b[0m        \u001b[36m0.8442\u001b[0m  0.0001  3.7008\n",
            "     16  0.5313       0.8553    0.3853        \u001b[31m0.9183\u001b[0m        \u001b[94m0.6890\u001b[0m        \u001b[36m0.8433\u001b[0m  0.0001  3.8159\n",
            "     17  0.5360       0.8548    0.3904        \u001b[31m0.9180\u001b[0m        0.6903        \u001b[36m0.8427\u001b[0m  0.0001  4.3573\n",
            "     18  0.5333       0.8560    0.3873        \u001b[31m0.9177\u001b[0m        \u001b[94m0.6880\u001b[0m        \u001b[36m0.8422\u001b[0m  0.0001  3.6834\n",
            "     19  0.5263       0.8584    0.3795        \u001b[31m0.9176\u001b[0m        0.6894        \u001b[36m0.8421\u001b[0m  0.0001  3.6751\n",
            "     20  0.5369       0.8563    0.3910        \u001b[31m0.9171\u001b[0m        \u001b[94m0.6869\u001b[0m        \u001b[36m0.8411\u001b[0m  0.0001  4.6234\n",
            "     21  0.5378       0.8559    0.3920        \u001b[31m0.9169\u001b[0m        0.6875        \u001b[36m0.8406\u001b[0m  0.0001  3.6952\n",
            "     22  0.5368       0.8564    0.3909        \u001b[31m0.9168\u001b[0m        \u001b[94m0.6866\u001b[0m        \u001b[36m0.8406\u001b[0m  0.0000  3.7149\n",
            "     23  0.5338       0.8561    0.3878        \u001b[31m0.9168\u001b[0m        \u001b[94m0.6845\u001b[0m        \u001b[36m0.8406\u001b[0m  0.0000  4.6066\n",
            "     24  0.5322       0.8567    0.3860        \u001b[31m0.9168\u001b[0m        0.6858        \u001b[36m0.8406\u001b[0m  0.0000  3.7268\n",
            "     25  0.5323       0.8566    0.3861        \u001b[31m0.9168\u001b[0m        \u001b[94m0.6839\u001b[0m        \u001b[36m0.8405\u001b[0m  0.0000  3.4353\n",
            "     26  0.5335       0.8565    0.3874        \u001b[31m0.9167\u001b[0m        \u001b[94m0.6838\u001b[0m        \u001b[36m0.8404\u001b[0m  0.0000  4.5707\n",
            "     27  0.5335       0.8562    0.3875        \u001b[31m0.9167\u001b[0m        \u001b[94m0.6834\u001b[0m        \u001b[36m0.8404\u001b[0m  0.0000  3.7347\n",
            "     28  0.5323       0.8568    0.3861        \u001b[31m0.9167\u001b[0m        0.6862        \u001b[36m0.8403\u001b[0m  0.0000  3.7244\n",
            "     29  0.5323       0.8568    0.3861        \u001b[31m0.9167\u001b[0m        0.6850        \u001b[36m0.8403\u001b[0m  0.0000  4.3878\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 1/3; 5/36] END lr=0.01, module__dropout=0.3, module__linear_size=500, module__size_emb=60;, score=-1.179 total time= 2.0min\n",
            "[CV 2/3; 5/36] START lr=0.01, module__dropout=0.3, module__linear_size=500, module__size_emb=60\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4098\u001b[0m       \u001b[32m0.8526\u001b[0m    \u001b[35m0.2697\u001b[0m        \u001b[31m0.9827\u001b[0m        \u001b[94m1.0312\u001b[0m        \u001b[36m0.9657\u001b[0m  0.0100  3.7540\n",
            "      2  \u001b[36m0.5346\u001b[0m       0.8373    \u001b[35m0.3927\u001b[0m        \u001b[31m0.9658\u001b[0m        \u001b[94m0.8889\u001b[0m        \u001b[36m0.9327\u001b[0m  0.0100  4.8724\n",
            "      3  0.5170       0.8399    0.3734        \u001b[31m0.9369\u001b[0m        \u001b[94m0.8603\u001b[0m        \u001b[36m0.8777\u001b[0m  0.0100  3.7660\n",
            "      4  0.3057       \u001b[32m0.8925\u001b[0m    0.1844        0.9445        \u001b[94m0.8488\u001b[0m        0.8921  0.0100  3.7567\n",
            "      5  \u001b[36m0.5687\u001b[0m       0.8403    \u001b[35m0.4297\u001b[0m        \u001b[31m0.9237\u001b[0m        \u001b[94m0.8286\u001b[0m        \u001b[36m0.8532\u001b[0m  0.0100  4.5819\n",
            "      6  0.3924       0.8898    0.2517        0.9281        \u001b[94m0.8087\u001b[0m        0.8613  0.0100  3.7890\n",
            "      7  0.4195       0.8845    0.2749        0.9242        \u001b[94m0.7923\u001b[0m        0.8542  0.0100  3.8104\n",
            "      8  0.5251       0.8660    0.3768        \u001b[31m0.9042\u001b[0m        \u001b[94m0.7387\u001b[0m        \u001b[36m0.8176\u001b[0m  0.0010  4.4616\n",
            "      9  0.5211       0.8710    0.3717        \u001b[31m0.8996\u001b[0m        \u001b[94m0.7179\u001b[0m        \u001b[36m0.8092\u001b[0m  0.0010  3.8084\n",
            "     10  0.5325       0.8687    0.3839        \u001b[31m0.8958\u001b[0m        \u001b[94m0.7087\u001b[0m        \u001b[36m0.8025\u001b[0m  0.0010  3.8012\n",
            "     11  0.5354       0.8701    0.3867        \u001b[31m0.8932\u001b[0m        \u001b[94m0.7000\u001b[0m        \u001b[36m0.7978\u001b[0m  0.0010  3.9077\n",
            "     12  0.5337       0.8719    0.3845        \u001b[31m0.8915\u001b[0m        \u001b[94m0.6941\u001b[0m        \u001b[36m0.7947\u001b[0m  0.0010  4.1578\n",
            "     13  0.5508       0.8677    0.4034        \u001b[31m0.8889\u001b[0m        \u001b[94m0.6887\u001b[0m        \u001b[36m0.7901\u001b[0m  0.0010  3.7194\n",
            "     14  0.5369       0.8730    0.3876        \u001b[31m0.8873\u001b[0m        \u001b[94m0.6807\u001b[0m        \u001b[36m0.7874\u001b[0m  0.0010  3.9776\n",
            "     15  0.5427       0.8713    0.3941        \u001b[31m0.8866\u001b[0m        \u001b[94m0.6720\u001b[0m        \u001b[36m0.7860\u001b[0m  0.0001  4.4896\n",
            "     16  0.5433       0.8722    0.3945        \u001b[31m0.8861\u001b[0m        \u001b[94m0.6696\u001b[0m        \u001b[36m0.7852\u001b[0m  0.0001  3.6943\n",
            "     17  0.5411       0.8732    0.3920        \u001b[31m0.8857\u001b[0m        \u001b[94m0.6680\u001b[0m        \u001b[36m0.7845\u001b[0m  0.0001  3.7042\n",
            "     18  0.5348       0.8753    0.3850        0.8858        \u001b[94m0.6677\u001b[0m        0.7846  0.0001  4.5613\n",
            "     19  0.5369       0.8747    0.3874        \u001b[31m0.8852\u001b[0m        \u001b[94m0.6653\u001b[0m        \u001b[36m0.7836\u001b[0m  0.0001  3.7681\n",
            "     20  0.5457       0.8715    0.3972        \u001b[31m0.8848\u001b[0m        0.6665        \u001b[36m0.7829\u001b[0m  0.0001  3.6521\n",
            "     21  0.5448       0.8724    0.3961        \u001b[31m0.8847\u001b[0m        0.6658        \u001b[36m0.7827\u001b[0m  0.0001  4.6158\n",
            "     22  0.5437       0.8729    0.3948        0.8847        \u001b[94m0.6644\u001b[0m        0.7827  0.0000  3.7252\n",
            "     23  0.5423       0.8735    0.3932        0.8847        \u001b[94m0.6637\u001b[0m        0.7828  0.0000  3.6965\n",
            "     24  0.5417       0.8740    0.3925        0.8847        0.6644        0.7827  0.0000  4.6337\n",
            "     25  0.5420       0.8733    0.3929        \u001b[31m0.8847\u001b[0m        0.6648        \u001b[36m0.7827\u001b[0m  0.0000  3.7089\n",
            "     26  0.5415       0.8735    0.3924        \u001b[31m0.8847\u001b[0m        \u001b[94m0.6637\u001b[0m        \u001b[36m0.7826\u001b[0m  0.0000  3.6413\n",
            "     27  0.5413       0.8737    0.3921        \u001b[31m0.8846\u001b[0m        0.6639        \u001b[36m0.7826\u001b[0m  0.0000  4.2716\n",
            "     28  0.5416       0.8732    0.3926        \u001b[31m0.8846\u001b[0m        0.6642        \u001b[36m0.7825\u001b[0m  0.0000  3.9580\n",
            "     29  0.5416       0.8732    0.3926        \u001b[31m0.8846\u001b[0m        0.6650        \u001b[36m0.7825\u001b[0m  0.0000  3.6714\n",
            "     30  0.5416       0.8733    0.3925        0.8846        \u001b[94m0.6634\u001b[0m        0.7825  0.0000  4.0479\n",
            "[CV 2/3; 5/36] END lr=0.01, module__dropout=0.3, module__linear_size=500, module__size_emb=60;, score=-1.033 total time= 2.1min\n",
            "[CV 3/3; 5/36] START lr=0.01, module__dropout=0.3, module__linear_size=500, module__size_emb=60\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.3419\u001b[0m       \u001b[32m0.8651\u001b[0m    \u001b[35m0.2131\u001b[0m        \u001b[31m0.9727\u001b[0m        \u001b[94m1.0204\u001b[0m        \u001b[36m0.9462\u001b[0m  0.0100  3.4330\n",
            "      2  \u001b[36m0.6050\u001b[0m       0.8054    \u001b[35m0.4844\u001b[0m        \u001b[31m0.9594\u001b[0m        \u001b[94m0.8664\u001b[0m        \u001b[36m0.9204\u001b[0m  0.0100  3.6369\n",
            "      3  0.4157       0.8603    0.2740        \u001b[31m0.9494\u001b[0m        \u001b[94m0.8422\u001b[0m        \u001b[36m0.9014\u001b[0m  0.0100  4.3950\n",
            "      4  0.4702       0.8524    0.3246        \u001b[31m0.9431\u001b[0m        \u001b[94m0.8238\u001b[0m        \u001b[36m0.8894\u001b[0m  0.0100  3.7318\n",
            "      5  0.5685       0.8321    0.4318        \u001b[31m0.9399\u001b[0m        \u001b[94m0.8054\u001b[0m        \u001b[36m0.8834\u001b[0m  0.0100  3.6457\n",
            "      6  0.4798       0.8561    0.3333        \u001b[31m0.9303\u001b[0m        \u001b[94m0.7878\u001b[0m        \u001b[36m0.8654\u001b[0m  0.0100  4.0745\n",
            "      7  0.5388       0.8461    0.3953        \u001b[31m0.9226\u001b[0m        \u001b[94m0.7764\u001b[0m        \u001b[36m0.8512\u001b[0m  0.0100  4.1193\n",
            "      8  0.5291       0.8596    0.3822        \u001b[31m0.9136\u001b[0m        \u001b[94m0.7183\u001b[0m        \u001b[36m0.8346\u001b[0m  0.0010  3.5280\n",
            "      9  0.4947       \u001b[32m0.8705\u001b[0m    0.3456        \u001b[31m0.9098\u001b[0m        \u001b[94m0.7028\u001b[0m        \u001b[36m0.8278\u001b[0m  0.0010  3.6114\n",
            "     10  0.5011       \u001b[32m0.8739\u001b[0m    0.3512        \u001b[31m0.9072\u001b[0m        \u001b[94m0.6913\u001b[0m        \u001b[36m0.8229\u001b[0m  0.0010  4.5920\n",
            "     11  0.5096       0.8737    0.3597        \u001b[31m0.9037\u001b[0m        \u001b[94m0.6860\u001b[0m        \u001b[36m0.8167\u001b[0m  0.0010  3.6161\n",
            "     12  0.5123       0.8723    0.3627        \u001b[31m0.9025\u001b[0m        \u001b[94m0.6807\u001b[0m        \u001b[36m0.8146\u001b[0m  0.0010  3.5223\n",
            "     13  0.5554       0.8579    0.4107        \u001b[31m0.8995\u001b[0m        \u001b[94m0.6758\u001b[0m        \u001b[36m0.8092\u001b[0m  0.0010  4.4566\n",
            "     14  0.5364       0.8680    0.3881        \u001b[31m0.8977\u001b[0m        \u001b[94m0.6705\u001b[0m        \u001b[36m0.8059\u001b[0m  0.0010  3.5128\n",
            "     15  0.5383       0.8676    0.3902        0.8982        \u001b[94m0.6618\u001b[0m        0.8068  0.0001  3.5789\n",
            "     16  0.5323       0.8672    0.3840        0.8979        \u001b[94m0.6613\u001b[0m        0.8063  0.0001  3.7104\n",
            "     17  0.5441       0.8662    0.3966        \u001b[31m0.8973\u001b[0m        \u001b[94m0.6604\u001b[0m        \u001b[36m0.8051\u001b[0m  0.0001  4.1575\n",
            "     18  0.5438       0.8658    0.3963        \u001b[31m0.8968\u001b[0m        \u001b[94m0.6582\u001b[0m        \u001b[36m0.8043\u001b[0m  0.0001  3.5834\n",
            "     19  0.5354       0.8682    0.3870        0.8971        \u001b[94m0.6575\u001b[0m        0.8048  0.0001  3.6703\n",
            "     20  0.5379       0.8685    0.3896        \u001b[31m0.8967\u001b[0m        0.6575        \u001b[36m0.8041\u001b[0m  0.0001  4.4566\n",
            "     21  0.5420       0.8661    0.3944        \u001b[31m0.8966\u001b[0m        \u001b[94m0.6558\u001b[0m        \u001b[36m0.8039\u001b[0m  0.0001  3.8797\n",
            "     22  0.5414       0.8667    0.3936        \u001b[31m0.8966\u001b[0m        0.6566        \u001b[36m0.8039\u001b[0m  0.0000  3.7734\n",
            "     23  0.5415       0.8666    0.3938        0.8966        \u001b[94m0.6548\u001b[0m        0.8039  0.0000  4.5800\n",
            "     24  0.5424       0.8659    0.3949        \u001b[31m0.8965\u001b[0m        \u001b[94m0.6540\u001b[0m        \u001b[36m0.8038\u001b[0m  0.0000  3.6075\n",
            "     25  0.5422       0.8658    0.3946        \u001b[31m0.8965\u001b[0m        \u001b[94m0.6539\u001b[0m        \u001b[36m0.8037\u001b[0m  0.0000  3.5929\n",
            "     26  0.5428       0.8659    0.3953        \u001b[31m0.8965\u001b[0m        0.6547        \u001b[36m0.8037\u001b[0m  0.0000  4.4921\n",
            "     27  0.5427       0.8662    0.3952        \u001b[31m0.8965\u001b[0m        0.6561        \u001b[36m0.8037\u001b[0m  0.0000  3.6886\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 3/3; 5/36] END lr=0.01, module__dropout=0.3, module__linear_size=500, module__size_emb=60;, score=-1.119 total time= 1.9min\n",
            "[CV 1/3; 6/36] START lr=0.01, module__dropout=0.3, module__linear_size=500, module__size_emb=120\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4791\u001b[0m       \u001b[32m0.8379\u001b[0m    \u001b[35m0.3355\u001b[0m        \u001b[31m0.9873\u001b[0m        \u001b[94m0.9978\u001b[0m        \u001b[36m0.9747\u001b[0m  0.0100  4.5811\n",
            "      2  \u001b[36m0.5537\u001b[0m       0.8099    \u001b[35m0.4207\u001b[0m        \u001b[31m0.9830\u001b[0m        \u001b[94m0.8837\u001b[0m        \u001b[36m0.9662\u001b[0m  0.0100  3.6881\n",
            "      3  0.4736       0.8351    0.3305        \u001b[31m0.9733\u001b[0m        \u001b[94m0.8650\u001b[0m        \u001b[36m0.9473\u001b[0m  0.0100  3.6354\n",
            "      4  0.3864       \u001b[32m0.8649\u001b[0m    0.2488        \u001b[31m0.9721\u001b[0m        \u001b[94m0.8506\u001b[0m        \u001b[36m0.9450\u001b[0m  0.0100  4.4337\n",
            "      5  0.3769       \u001b[32m0.8658\u001b[0m    0.2409        \u001b[31m0.9662\u001b[0m        \u001b[94m0.8424\u001b[0m        \u001b[36m0.9336\u001b[0m  0.0100  3.5760\n",
            "      6  0.3987       0.8602    0.2594        \u001b[31m0.9600\u001b[0m        \u001b[94m0.8268\u001b[0m        \u001b[36m0.9216\u001b[0m  0.0100  3.3366\n",
            "      7  \u001b[36m0.5700\u001b[0m       0.8228    \u001b[35m0.4360\u001b[0m        \u001b[31m0.9510\u001b[0m        \u001b[94m0.8071\u001b[0m        \u001b[36m0.9045\u001b[0m  0.0100  4.0422\n",
            "      8  0.5145       0.8453    0.3698        \u001b[31m0.9363\u001b[0m        \u001b[94m0.7520\u001b[0m        \u001b[36m0.8767\u001b[0m  0.0010  3.9381\n",
            "      9  0.5253       0.8477    0.3806        \u001b[31m0.9317\u001b[0m        \u001b[94m0.7336\u001b[0m        \u001b[36m0.8681\u001b[0m  0.0010  3.5842\n",
            "     10  0.5387       0.8456    0.3953        \u001b[31m0.9294\u001b[0m        \u001b[94m0.7228\u001b[0m        \u001b[36m0.8637\u001b[0m  0.0010  3.9340\n",
            "     11  0.5241       0.8528    0.3782        \u001b[31m0.9277\u001b[0m        \u001b[94m0.7151\u001b[0m        \u001b[36m0.8607\u001b[0m  0.0010  4.3522\n",
            "     12  0.5458       0.8529    0.4013        \u001b[31m0.9248\u001b[0m        \u001b[94m0.7085\u001b[0m        \u001b[36m0.8553\u001b[0m  0.0010  3.6826\n",
            "     13  0.5566       0.8486    0.4141        \u001b[31m0.9224\u001b[0m        \u001b[94m0.7006\u001b[0m        \u001b[36m0.8508\u001b[0m  0.0010  3.5336\n",
            "     14  0.5212       0.8562    0.3747        \u001b[31m0.9212\u001b[0m        \u001b[94m0.6964\u001b[0m        \u001b[36m0.8486\u001b[0m  0.0010  4.3838\n",
            "     15  0.5345       0.8552    0.3887        \u001b[31m0.9201\u001b[0m        \u001b[94m0.6845\u001b[0m        \u001b[36m0.8465\u001b[0m  0.0001  3.5973\n",
            "     16  0.5467       0.8517    0.4025        \u001b[31m0.9196\u001b[0m        \u001b[94m0.6822\u001b[0m        \u001b[36m0.8456\u001b[0m  0.0001  3.5751\n",
            "     17  0.5424       0.8537    0.3975        \u001b[31m0.9194\u001b[0m        \u001b[94m0.6804\u001b[0m        \u001b[36m0.8453\u001b[0m  0.0001  4.4151\n",
            "     18  0.5481       0.8505    0.4044        \u001b[31m0.9191\u001b[0m        \u001b[94m0.6794\u001b[0m        \u001b[36m0.8448\u001b[0m  0.0001  3.6157\n",
            "     19  0.5463       0.8528    0.4019        \u001b[31m0.9187\u001b[0m        \u001b[94m0.6794\u001b[0m        \u001b[36m0.8441\u001b[0m  0.0001  3.5757\n",
            "     20  0.5490       0.8515    0.4051        \u001b[31m0.9185\u001b[0m        \u001b[94m0.6773\u001b[0m        \u001b[36m0.8437\u001b[0m  0.0001  3.9427\n",
            "     21  0.5411       0.8539    0.3961        \u001b[31m0.9184\u001b[0m        0.6794        \u001b[36m0.8435\u001b[0m  0.0001  4.0901\n",
            "     22  0.5434       0.8534    0.3987        \u001b[31m0.9183\u001b[0m        \u001b[94m0.6743\u001b[0m        \u001b[36m0.8433\u001b[0m  0.0000  3.5997\n",
            "     23  0.5450       0.8533    0.4004        \u001b[31m0.9183\u001b[0m        0.6762        \u001b[36m0.8432\u001b[0m  0.0000  3.4035\n",
            "     24  0.5464       0.8528    0.4020        \u001b[31m0.9182\u001b[0m        0.6762        \u001b[36m0.8432\u001b[0m  0.0000  4.5243\n",
            "     25  0.5476       0.8529    0.4032        \u001b[31m0.9182\u001b[0m        0.6761        \u001b[36m0.8431\u001b[0m  0.0000  3.5738\n",
            "     26  0.5468       0.8529    0.4024        \u001b[31m0.9182\u001b[0m        0.6779        \u001b[36m0.8431\u001b[0m  0.0000  3.6005\n",
            "     27  0.5475       0.8530    0.4031        \u001b[31m0.9182\u001b[0m        0.6749        \u001b[36m0.8430\u001b[0m  0.0000  4.4368\n",
            "     28  0.5461       0.8535    0.4015        \u001b[31m0.9182\u001b[0m        0.6764        \u001b[36m0.8430\u001b[0m  0.0000  3.6101\n",
            "     29  0.5461       0.8533    0.4015        \u001b[31m0.9182\u001b[0m        0.6756        \u001b[36m0.8430\u001b[0m  0.0000  3.5072\n",
            "     30  0.5458       0.8534    0.4012        0.9182        0.6755        0.8430  0.0000  4.1332\n",
            "[CV 1/3; 6/36] END lr=0.01, module__dropout=0.3, module__linear_size=500, module__size_emb=120;, score=-1.193 total time= 2.0min\n",
            "[CV 2/3; 6/36] START lr=0.01, module__dropout=0.3, module__linear_size=500, module__size_emb=120\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.2495\u001b[0m       \u001b[32m0.8755\u001b[0m    \u001b[35m0.1455\u001b[0m        \u001b[31m0.9704\u001b[0m        \u001b[94m1.0183\u001b[0m        \u001b[36m0.9417\u001b[0m  0.0100  3.5662\n",
            "      2  \u001b[36m0.5699\u001b[0m       0.8229    \u001b[35m0.4359\u001b[0m        \u001b[31m0.9576\u001b[0m        \u001b[94m0.8895\u001b[0m        \u001b[36m0.9170\u001b[0m  0.0100  3.5245\n",
            "      3  0.5255       0.8432    0.3817        \u001b[31m0.9428\u001b[0m        \u001b[94m0.8696\u001b[0m        \u001b[36m0.8889\u001b[0m  0.0100  4.4136\n",
            "      4  0.5139       0.8508    0.3681        \u001b[31m0.9349\u001b[0m        \u001b[94m0.8484\u001b[0m        \u001b[36m0.8740\u001b[0m  0.0100  3.5553\n",
            "      5  0.4529       0.8635    0.3070        \u001b[31m0.9290\u001b[0m        \u001b[94m0.8349\u001b[0m        \u001b[36m0.8631\u001b[0m  0.0100  3.5333\n",
            "      6  \u001b[36m0.6238\u001b[0m       0.8267    \u001b[35m0.5009\u001b[0m        \u001b[31m0.9278\u001b[0m        \u001b[94m0.8227\u001b[0m        \u001b[36m0.8609\u001b[0m  0.0100  3.8526\n",
            "      7  0.4828       0.8732    0.3337        \u001b[31m0.9172\u001b[0m        \u001b[94m0.8095\u001b[0m        \u001b[36m0.8412\u001b[0m  0.0100  4.0952\n",
            "      8  0.5009       0.8752    0.3509        \u001b[31m0.9034\u001b[0m        \u001b[94m0.7457\u001b[0m        \u001b[36m0.8161\u001b[0m  0.0010  3.6137\n",
            "      9  0.5118       \u001b[32m0.8758\u001b[0m    0.3616        \u001b[31m0.8985\u001b[0m        \u001b[94m0.7231\u001b[0m        \u001b[36m0.8073\u001b[0m  0.0010  3.5802\n",
            "     10  0.5062       \u001b[32m0.8804\u001b[0m    0.3552        \u001b[31m0.8949\u001b[0m        \u001b[94m0.7136\u001b[0m        \u001b[36m0.8009\u001b[0m  0.0010  4.4131\n",
            "     11  0.5208       0.8772    0.3704        \u001b[31m0.8912\u001b[0m        \u001b[94m0.7035\u001b[0m        \u001b[36m0.7943\u001b[0m  0.0010  3.6034\n",
            "     12  0.4996       \u001b[32m0.8823\u001b[0m    0.3484        \u001b[31m0.8893\u001b[0m        \u001b[94m0.6945\u001b[0m        \u001b[36m0.7909\u001b[0m  0.0010  3.6114\n",
            "     13  0.5029       \u001b[32m0.8871\u001b[0m    0.3509        \u001b[31m0.8878\u001b[0m        \u001b[94m0.6856\u001b[0m        \u001b[36m0.7881\u001b[0m  0.0010  4.5226\n",
            "     14  0.4755       \u001b[32m0.8895\u001b[0m    0.3244        \u001b[31m0.8866\u001b[0m        \u001b[94m0.6805\u001b[0m        \u001b[36m0.7861\u001b[0m  0.0010  3.5070\n",
            "     15  0.5075       0.8830    0.3560        \u001b[31m0.8825\u001b[0m        \u001b[94m0.6691\u001b[0m        \u001b[36m0.7788\u001b[0m  0.0001  3.7266\n",
            "     16  0.5117       0.8834    0.3602        \u001b[31m0.8819\u001b[0m        \u001b[94m0.6650\u001b[0m        \u001b[36m0.7777\u001b[0m  0.0001  4.2494\n",
            "     17  0.5141       0.8831    0.3626        \u001b[31m0.8814\u001b[0m        \u001b[94m0.6650\u001b[0m        \u001b[36m0.7768\u001b[0m  0.0001  3.8585\n",
            "     18  0.5110       0.8834    0.3594        \u001b[31m0.8811\u001b[0m        \u001b[94m0.6636\u001b[0m        \u001b[36m0.7763\u001b[0m  0.0001  3.8863\n",
            "     19  0.5103       0.8838    0.3587        \u001b[31m0.8808\u001b[0m        \u001b[94m0.6619\u001b[0m        \u001b[36m0.7758\u001b[0m  0.0001  4.3362\n",
            "     20  0.5088       0.8848    0.3570        \u001b[31m0.8805\u001b[0m        \u001b[94m0.6613\u001b[0m        \u001b[36m0.7752\u001b[0m  0.0001  4.1728\n",
            "     21  0.5184       0.8826    0.3670        \u001b[31m0.8798\u001b[0m        \u001b[94m0.6605\u001b[0m        \u001b[36m0.7741\u001b[0m  0.0001  3.7304\n",
            "     22  0.5177       0.8832    0.3662        0.8798        \u001b[94m0.6597\u001b[0m        0.7741  0.0000  3.7963\n",
            "     23  0.5165       0.8841    0.3648        0.8799        0.6601        0.7742  0.0000  4.4392\n",
            "     24  0.5147       0.8846    0.3629        0.8799        \u001b[94m0.6585\u001b[0m        0.7742  0.0000  3.8657\n",
            "     25  0.5153       0.8847    0.3635        0.8798        0.6595        0.7741  0.0000  3.8814\n",
            "     26  0.5147       0.8846    0.3629        0.8798        \u001b[94m0.6564\u001b[0m        0.7741  0.0000  4.7798\n",
            "     27  0.5159       0.8849    0.3641        \u001b[31m0.8798\u001b[0m        0.6578        \u001b[36m0.7740\u001b[0m  0.0000  3.8415\n",
            "     28  0.5154       0.8847    0.3636        \u001b[31m0.8798\u001b[0m        0.6573        \u001b[36m0.7740\u001b[0m  0.0000  3.8125\n",
            "     29  0.5153       0.8847    0.3635        0.8798        0.6582        0.7740  0.0000  4.4837\n",
            "     30  0.5154       0.8847    0.3636        \u001b[31m0.8797\u001b[0m        0.6592        \u001b[36m0.7739\u001b[0m  0.0000  3.6458\n",
            "[CV 2/3; 6/36] END lr=0.01, module__dropout=0.3, module__linear_size=500, module__size_emb=120;, score=-1.025 total time= 2.0min\n",
            "[CV 3/3; 6/36] START lr=0.01, module__dropout=0.3, module__linear_size=500, module__size_emb=120\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4299\u001b[0m       \u001b[32m0.8533\u001b[0m    \u001b[35m0.2874\u001b[0m        \u001b[31m0.9616\u001b[0m        \u001b[94m1.0098\u001b[0m        \u001b[36m0.9246\u001b[0m  0.0100  3.6860\n",
            "      2  \u001b[36m0.5917\u001b[0m       0.8117    \u001b[35m0.4655\u001b[0m        \u001b[31m0.9598\u001b[0m        \u001b[94m0.8669\u001b[0m        \u001b[36m0.9213\u001b[0m  0.0100  4.2264\n",
            "      3  0.5358       0.8291    0.3958        \u001b[31m0.9464\u001b[0m        \u001b[94m0.8489\u001b[0m        \u001b[36m0.8956\u001b[0m  0.0100  3.6350\n",
            "      4  0.5679       0.8259    0.4327        0.9653        \u001b[94m0.8355\u001b[0m        0.9317  0.0100  3.5732\n",
            "      5  \u001b[36m0.6040\u001b[0m       0.8119    \u001b[35m0.4808\u001b[0m        0.9503        \u001b[94m0.8284\u001b[0m        0.9031  0.0100  4.4704\n",
            "      6  0.3047       \u001b[32m0.8847\u001b[0m    0.1841        0.9495        \u001b[94m0.8167\u001b[0m        0.9016  0.0100  3.7005\n",
            "      7  0.4797       0.8598    0.3327        0.9530        \u001b[94m0.7976\u001b[0m        0.9081  0.0100  3.7089\n",
            "      8  0.5031       0.8656    0.3546        \u001b[31m0.9244\u001b[0m        \u001b[94m0.7462\u001b[0m        \u001b[36m0.8545\u001b[0m  0.0010  4.5209\n",
            "      9  0.4957       0.8688    0.3468        \u001b[31m0.9216\u001b[0m        \u001b[94m0.7285\u001b[0m        \u001b[36m0.8494\u001b[0m  0.0010  3.6771\n",
            "     10  0.4967       0.8672    0.3480        \u001b[31m0.9162\u001b[0m        \u001b[94m0.7181\u001b[0m        \u001b[36m0.8394\u001b[0m  0.0010  3.7070\n",
            "     11  0.5121       0.8668    0.3634        \u001b[31m0.9129\u001b[0m        \u001b[94m0.7097\u001b[0m        \u001b[36m0.8334\u001b[0m  0.0010  4.4445\n",
            "     12  0.4980       0.8721    0.3485        \u001b[31m0.9109\u001b[0m        \u001b[94m0.7018\u001b[0m        \u001b[36m0.8297\u001b[0m  0.0010  3.7877\n",
            "     13  0.5314       0.8656    0.3833        \u001b[31m0.9071\u001b[0m        \u001b[94m0.6960\u001b[0m        \u001b[36m0.8228\u001b[0m  0.0010  3.6862\n",
            "     14  0.5355       0.8678    0.3872        \u001b[31m0.9064\u001b[0m        \u001b[94m0.6910\u001b[0m        \u001b[36m0.8216\u001b[0m  0.0010  4.0922\n",
            "     15  0.5204       0.8722    0.3708        \u001b[31m0.9063\u001b[0m        \u001b[94m0.6802\u001b[0m        \u001b[36m0.8214\u001b[0m  0.0001  4.1199\n",
            "     16  0.5088       0.8747    0.3587        0.9065        \u001b[94m0.6778\u001b[0m        0.8217  0.0001  3.5739\n",
            "     17  0.5148       0.8739    0.3649        \u001b[31m0.9058\u001b[0m        \u001b[94m0.6772\u001b[0m        \u001b[36m0.8205\u001b[0m  0.0001  3.8866\n",
            "     18  0.5109       0.8744    0.3609        \u001b[31m0.9055\u001b[0m        \u001b[94m0.6755\u001b[0m        \u001b[36m0.8199\u001b[0m  0.0001  4.4965\n",
            "     19  0.5080       0.8759    0.3577        0.9057        \u001b[94m0.6747\u001b[0m        0.8204  0.0001  3.7491\n",
            "     20  0.5136       0.8752    0.3635        \u001b[31m0.9049\u001b[0m        0.6751        \u001b[36m0.8189\u001b[0m  0.0001  3.4746\n",
            "     21  0.5057       0.8764    0.3554        0.9052        \u001b[94m0.6735\u001b[0m        0.8195  0.0001  4.6344\n",
            "     22  0.5092       0.8763    0.3589        0.9050        \u001b[94m0.6705\u001b[0m        0.8189  0.0000  3.7456\n",
            "     23  0.5108       0.8757    0.3606        \u001b[31m0.9047\u001b[0m        0.6730        \u001b[36m0.8185\u001b[0m  0.0000  3.7852\n",
            "     24  0.5116       0.8744    0.3616        \u001b[31m0.9046\u001b[0m        0.6719        \u001b[36m0.8182\u001b[0m  0.0000  4.6512\n",
            "     25  0.5122       0.8746    0.3621        0.9046        0.6712        0.8183  0.0000  3.7545\n",
            "     26  0.5122       0.8753    0.3620        \u001b[31m0.9045\u001b[0m        0.6715        \u001b[36m0.8182\u001b[0m  0.0000  3.7648\n",
            "     27  0.5130       0.8748    0.3629        \u001b[31m0.9045\u001b[0m        0.6728        \u001b[36m0.8181\u001b[0m  0.0000  4.5097\n",
            "     28  0.5134       0.8748    0.3633        \u001b[31m0.9044\u001b[0m        0.6716        \u001b[36m0.8180\u001b[0m  0.0000  3.7619\n",
            "     29  0.5134       0.8748    0.3633        \u001b[31m0.9044\u001b[0m        0.6713        \u001b[36m0.8180\u001b[0m  0.0000  3.7424\n",
            "     30  0.5135       0.8748    0.3634        \u001b[31m0.9044\u001b[0m        0.6718        \u001b[36m0.8180\u001b[0m  0.0000  4.4342\n",
            "[CV 3/3; 6/36] END lr=0.01, module__dropout=0.3, module__linear_size=500, module__size_emb=120;, score=-1.131 total time= 2.0min\n",
            "[CV 1/3; 7/36] START lr=0.01, module__dropout=0.3, module__linear_size=600, module__size_emb=30\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4031\u001b[0m       \u001b[32m0.8389\u001b[0m    \u001b[35m0.2653\u001b[0m        \u001b[31m1.0089\u001b[0m        \u001b[94m1.0345\u001b[0m        \u001b[36m1.0178\u001b[0m  0.0100  3.8001\n",
            "      2  0.3751       \u001b[32m0.8595\u001b[0m    0.2399        \u001b[31m0.9744\u001b[0m        \u001b[94m0.8887\u001b[0m        \u001b[36m0.9495\u001b[0m  0.0100  3.7245\n",
            "      3  0.3998       \u001b[32m0.8604\u001b[0m    0.2604        \u001b[31m0.9729\u001b[0m        \u001b[94m0.8620\u001b[0m        \u001b[36m0.9465\u001b[0m  0.0100  4.6833\n",
            "      4  \u001b[36m0.4499\u001b[0m       0.8466    \u001b[35m0.3064\u001b[0m        \u001b[31m0.9646\u001b[0m        \u001b[94m0.8517\u001b[0m        \u001b[36m0.9305\u001b[0m  0.0100  3.6425\n",
            "      5  0.3407       \u001b[32m0.8806\u001b[0m    0.2112        0.9682        \u001b[94m0.8415\u001b[0m        0.9374  0.0100  3.7560\n",
            "      6  0.2440       \u001b[32m0.8827\u001b[0m    0.1415        0.9755        \u001b[94m0.8308\u001b[0m        0.9515  0.0100  4.4992\n",
            "      7  \u001b[36m0.5097\u001b[0m       0.8393    \u001b[35m0.3660\u001b[0m        \u001b[31m0.9518\u001b[0m        \u001b[94m0.8187\u001b[0m        \u001b[36m0.9059\u001b[0m  0.0100  3.6701\n",
            "      8  0.4619       0.8531    0.3167        \u001b[31m0.9450\u001b[0m        \u001b[94m0.7695\u001b[0m        \u001b[36m0.8930\u001b[0m  0.0010  3.6858\n",
            "      9  0.4511       0.8561    0.3063        \u001b[31m0.9426\u001b[0m        \u001b[94m0.7559\u001b[0m        \u001b[36m0.8884\u001b[0m  0.0010  4.5198\n",
            "     10  0.4467       0.8610    0.3016        \u001b[31m0.9399\u001b[0m        \u001b[94m0.7455\u001b[0m        \u001b[36m0.8835\u001b[0m  0.0010  3.8574\n",
            "     11  0.5087       0.8471    0.3635        \u001b[31m0.9345\u001b[0m        \u001b[94m0.7392\u001b[0m        \u001b[36m0.8732\u001b[0m  0.0010  3.4215\n",
            "     12  0.4897       0.8493    0.3440        \u001b[31m0.9343\u001b[0m        \u001b[94m0.7334\u001b[0m        \u001b[36m0.8729\u001b[0m  0.0010  4.0596\n",
            "     13  0.4835       0.8522    0.3375        \u001b[31m0.9319\u001b[0m        \u001b[94m0.7251\u001b[0m        \u001b[36m0.8684\u001b[0m  0.0010  4.2071\n",
            "     14  \u001b[36m0.5120\u001b[0m       0.8449    \u001b[35m0.3672\u001b[0m        \u001b[31m0.9285\u001b[0m        \u001b[94m0.7178\u001b[0m        \u001b[36m0.8621\u001b[0m  0.0010  3.7356\n",
            "     15  0.4903       0.8507    0.3444        0.9287        \u001b[94m0.7075\u001b[0m        0.8625  0.0001  3.8309\n",
            "     16  0.4967       0.8503    0.3509        \u001b[31m0.9281\u001b[0m        \u001b[94m0.7066\u001b[0m        \u001b[36m0.8615\u001b[0m  0.0001  4.5000\n",
            "     17  0.4979       0.8486    0.3523        \u001b[31m0.9276\u001b[0m        \u001b[94m0.7042\u001b[0m        \u001b[36m0.8605\u001b[0m  0.0001  3.6443\n",
            "     18  0.4965       0.8502    0.3506        \u001b[31m0.9274\u001b[0m        \u001b[94m0.7033\u001b[0m        \u001b[36m0.8600\u001b[0m  0.0001  3.6613\n",
            "     19  0.5024       0.8474    0.3570        \u001b[31m0.9270\u001b[0m        \u001b[94m0.7022\u001b[0m        \u001b[36m0.8593\u001b[0m  0.0001  4.5514\n",
            "     20  0.5112       0.8463    0.3662        \u001b[31m0.9264\u001b[0m        \u001b[94m0.7001\u001b[0m        \u001b[36m0.8582\u001b[0m  0.0001  3.6560\n",
            "     21  0.5052       0.8477    0.3598        \u001b[31m0.9262\u001b[0m        \u001b[94m0.7000\u001b[0m        \u001b[36m0.8578\u001b[0m  0.0001  3.6983\n",
            "     22  0.5078       0.8468    0.3627        \u001b[31m0.9261\u001b[0m        0.7003        \u001b[36m0.8576\u001b[0m  0.0000  4.4639\n",
            "     23  0.5080       0.8463    0.3629        \u001b[31m0.9261\u001b[0m        \u001b[94m0.6995\u001b[0m        \u001b[36m0.8576\u001b[0m  0.0000  3.6982\n",
            "     24  0.5109       0.8465    0.3659        \u001b[31m0.9260\u001b[0m        0.6998        \u001b[36m0.8575\u001b[0m  0.0000  3.6647\n",
            "     25  0.5105       0.8464    0.3654        \u001b[31m0.9260\u001b[0m        0.7000        \u001b[36m0.8575\u001b[0m  0.0000  4.3381\n",
            "     26  0.5105       0.8464    0.3654        \u001b[31m0.9260\u001b[0m        \u001b[94m0.6990\u001b[0m        \u001b[36m0.8574\u001b[0m  0.0000  3.9948\n",
            "     27  0.5105       0.8464    0.3654        \u001b[31m0.9259\u001b[0m        0.6999        \u001b[36m0.8574\u001b[0m  0.0000  3.7189\n",
            "     28  0.5113       0.8462    0.3663        \u001b[31m0.9259\u001b[0m        \u001b[94m0.6988\u001b[0m        \u001b[36m0.8573\u001b[0m  0.0000  3.9896\n",
            "     29  0.5112       0.8461    0.3662        \u001b[31m0.9259\u001b[0m        0.6990        \u001b[36m0.8573\u001b[0m  0.0000  4.2986\n",
            "     30  0.5113       0.8462    0.3663        \u001b[31m0.9259\u001b[0m        0.6998        \u001b[36m0.8573\u001b[0m  0.0000  3.7015\n",
            "[CV 1/3; 7/36] END lr=0.01, module__dropout=0.3, module__linear_size=600, module__size_emb=30;, score=-1.175 total time= 2.0min\n",
            "[CV 2/3; 7/36] START lr=0.01, module__dropout=0.3, module__linear_size=600, module__size_emb=30\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4413\u001b[0m       \u001b[32m0.8440\u001b[0m    \u001b[35m0.2987\u001b[0m        \u001b[31m0.9579\u001b[0m        \u001b[94m1.0653\u001b[0m        \u001b[36m0.9176\u001b[0m  0.0100  4.4916\n",
            "      2  0.2617       \u001b[32m0.8908\u001b[0m    0.1534        0.9617        \u001b[94m0.8826\u001b[0m        0.9248  0.0100  3.6799\n",
            "      3  0.3699       0.8768    0.2344        \u001b[31m0.9540\u001b[0m        \u001b[94m0.8647\u001b[0m        \u001b[36m0.9100\u001b[0m  0.0100  3.6986\n",
            "      4  0.3221       \u001b[32m0.8944\u001b[0m    0.1964        0.9544        \u001b[94m0.8443\u001b[0m        0.9109  0.0100  4.1510\n",
            "      5  0.3617       0.8862    0.2272        \u001b[31m0.9402\u001b[0m        \u001b[94m0.8323\u001b[0m        \u001b[36m0.8840\u001b[0m  0.0100  4.0341\n",
            "      6  0.3853       0.8753    0.2470        \u001b[31m0.9329\u001b[0m        \u001b[94m0.8188\u001b[0m        \u001b[36m0.8704\u001b[0m  0.0100  3.4248\n",
            "      7  0.3336       0.8855    0.2056        0.9398        \u001b[94m0.8111\u001b[0m        0.8833  0.0100  3.7330\n",
            "      8  0.4336       0.8784    0.2878        \u001b[31m0.9157\u001b[0m        \u001b[94m0.7549\u001b[0m        \u001b[36m0.8385\u001b[0m  0.0010  4.4784\n",
            "      9  \u001b[36m0.4695\u001b[0m       0.8766    \u001b[35m0.3206\u001b[0m        \u001b[31m0.9109\u001b[0m        \u001b[94m0.7378\u001b[0m        \u001b[36m0.8298\u001b[0m  0.0010  3.7186\n",
            "     10  0.4424       0.8793    0.2955        0.9113        \u001b[94m0.7283\u001b[0m        0.8304  0.0010  3.7058\n",
            "     11  0.4504       0.8774    0.3030        \u001b[31m0.9077\u001b[0m        \u001b[94m0.7241\u001b[0m        \u001b[36m0.8240\u001b[0m  0.0010  4.5659\n",
            "     12  0.4162       0.8845    0.2722        0.9097        \u001b[94m0.7155\u001b[0m        0.8275  0.0010  3.7924\n",
            "     13  \u001b[36m0.4718\u001b[0m       0.8779    \u001b[35m0.3226\u001b[0m        \u001b[31m0.9026\u001b[0m        \u001b[94m0.7115\u001b[0m        \u001b[36m0.8147\u001b[0m  0.0010  3.8075\n",
            "     14  \u001b[36m0.4864\u001b[0m       0.8724    \u001b[35m0.3372\u001b[0m        \u001b[31m0.8999\u001b[0m        \u001b[94m0.7072\u001b[0m        \u001b[36m0.8097\u001b[0m  0.0010  4.5369\n",
            "     15  0.4732       0.8788    0.3238        0.9007        \u001b[94m0.6952\u001b[0m        0.8113  0.0001  3.7609\n",
            "     16  0.4662       0.8817    0.3169        0.9010        \u001b[94m0.6927\u001b[0m        0.8118  0.0001  3.7496\n",
            "     17  0.4762       0.8790    0.3266        0.9002        0.6941        0.8104  0.0001  4.6626\n",
            "     18  0.4698       0.8804    0.3204        0.9005        \u001b[94m0.6912\u001b[0m        0.8109  0.0001  3.7254\n",
            "     19  0.4741       0.8796    0.3245        0.9000        0.6932        0.8101  0.0001  3.7535\n",
            "     20  0.4805       0.8777    0.3308        \u001b[31m0.8994\u001b[0m        \u001b[94m0.6909\u001b[0m        \u001b[36m0.8089\u001b[0m  0.0001  4.4855\n",
            "     21  0.4737       0.8799    0.3241        0.8998        0.6911        0.8097  0.0001  3.9346\n",
            "     22  0.4748       0.8792    0.3252        0.8997        \u001b[94m0.6895\u001b[0m        0.8094  0.0000  3.7460\n",
            "     23  0.4739       0.8800    0.3243        0.8997        0.6906        0.8094  0.0000  4.3005\n",
            "     24  0.4744       0.8797    0.3248        0.8996        0.6896        0.8093  0.0000  4.1463\n",
            "     25  0.4748       0.8794    0.3252        0.8996        \u001b[94m0.6887\u001b[0m        0.8092  0.0000  3.8491\n",
            "     26  0.4748       0.8794    0.3252        0.8996        \u001b[94m0.6885\u001b[0m        0.8092  0.0000  3.8725\n",
            "     27  0.4751       0.8785    0.3256        0.8995        \u001b[94m0.6884\u001b[0m        0.8091  0.0000  4.2819\n",
            "     28  0.4767       0.8788    0.3270        0.8994        \u001b[94m0.6884\u001b[0m        0.8089  0.0000  3.7403\n",
            "     29  0.4766       0.8787    0.3269        0.8994        0.6892        0.8089  0.0000  3.8128\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 2/3; 7/36] END lr=0.01, module__dropout=0.3, module__linear_size=600, module__size_emb=30;, score=-1.034 total time= 2.1min\n",
            "[CV 3/3; 7/36] START lr=0.01, module__dropout=0.3, module__linear_size=600, module__size_emb=30\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.3786\u001b[0m       \u001b[32m0.8493\u001b[0m    \u001b[35m0.2436\u001b[0m        \u001b[31m0.9713\u001b[0m        \u001b[94m1.0558\u001b[0m        \u001b[36m0.9433\u001b[0m  0.0100  3.7906\n",
            "      2  \u001b[36m0.4333\u001b[0m       \u001b[32m0.8519\u001b[0m    \u001b[35m0.2905\u001b[0m        \u001b[31m0.9580\u001b[0m        \u001b[94m0.8662\u001b[0m        \u001b[36m0.9177\u001b[0m  0.0100  4.3646\n",
            "      3  \u001b[36m0.4350\u001b[0m       \u001b[32m0.8543\u001b[0m    \u001b[35m0.2918\u001b[0m        \u001b[31m0.9516\u001b[0m        \u001b[94m0.8485\u001b[0m        \u001b[36m0.9056\u001b[0m  0.0100  3.8955\n",
            "      4  \u001b[36m0.4955\u001b[0m       0.8439    \u001b[35m0.3508\u001b[0m        \u001b[31m0.9454\u001b[0m        \u001b[94m0.8325\u001b[0m        \u001b[36m0.8938\u001b[0m  0.0100  3.7011\n",
            "      5  0.3959       \u001b[32m0.8631\u001b[0m    0.2568        0.9494        \u001b[94m0.8190\u001b[0m        0.9013  0.0100  4.0928\n",
            "      6  0.4329       \u001b[32m0.8655\u001b[0m    0.2886        0.9532        \u001b[94m0.8032\u001b[0m        0.9086  0.0100  4.2914\n",
            "      7  \u001b[36m0.5866\u001b[0m       0.8387    \u001b[35m0.4510\u001b[0m        \u001b[31m0.9238\u001b[0m        \u001b[94m0.7908\u001b[0m        \u001b[36m0.8535\u001b[0m  0.0100  3.7479\n",
            "      8  0.5272       0.8582    0.3805        \u001b[31m0.9188\u001b[0m        \u001b[94m0.7305\u001b[0m        \u001b[36m0.8442\u001b[0m  0.0010  3.8574\n",
            "      9  0.5374       0.8558    0.3917        \u001b[31m0.9160\u001b[0m        \u001b[94m0.7150\u001b[0m        \u001b[36m0.8390\u001b[0m  0.0010  4.3718\n",
            "     10  0.5119       0.8639    0.3637        \u001b[31m0.9149\u001b[0m        \u001b[94m0.7072\u001b[0m        \u001b[36m0.8371\u001b[0m  0.0010  3.7177\n",
            "     11  0.5315       0.8628    0.3841        \u001b[31m0.9108\u001b[0m        \u001b[94m0.6984\u001b[0m        \u001b[36m0.8295\u001b[0m  0.0010  3.7914\n",
            "     12  0.5470       0.8564    0.4019        \u001b[31m0.9090\u001b[0m        \u001b[94m0.6946\u001b[0m        \u001b[36m0.8262\u001b[0m  0.0010  4.3175\n",
            "     13  0.5420       0.8598    0.3957        \u001b[31m0.9077\u001b[0m        \u001b[94m0.6873\u001b[0m        \u001b[36m0.8239\u001b[0m  0.0010  3.8438\n",
            "     14  0.5465       0.8608    0.4004        \u001b[31m0.9060\u001b[0m        \u001b[94m0.6828\u001b[0m        \u001b[36m0.8209\u001b[0m  0.0010  3.6410\n",
            "     15  0.5464       0.8608    0.4003        \u001b[31m0.9051\u001b[0m        \u001b[94m0.6728\u001b[0m        \u001b[36m0.8191\u001b[0m  0.0001  4.5216\n",
            "     16  0.5415       0.8637    0.3944        \u001b[31m0.9050\u001b[0m        \u001b[94m0.6722\u001b[0m        \u001b[36m0.8190\u001b[0m  0.0001  3.6824\n",
            "     17  0.5474       0.8622    0.4010        \u001b[31m0.9045\u001b[0m        \u001b[94m0.6708\u001b[0m        \u001b[36m0.8182\u001b[0m  0.0001  3.7279\n",
            "     18  0.5414       0.8637    0.3943        0.9045        \u001b[94m0.6695\u001b[0m        0.8182  0.0001  4.4681\n",
            "     19  0.5413       0.8638    0.3941        \u001b[31m0.9043\u001b[0m        \u001b[94m0.6686\u001b[0m        \u001b[36m0.8178\u001b[0m  0.0001  3.6545\n",
            "     20  0.5533       0.8599    0.4079        \u001b[31m0.9039\u001b[0m        0.6698        \u001b[36m0.8170\u001b[0m  0.0001  3.6930\n",
            "     21  0.5469       0.8628    0.4004        \u001b[31m0.9038\u001b[0m        0.6705        \u001b[36m0.8169\u001b[0m  0.0001  4.0831\n",
            "     22  0.5460       0.8627    0.3994        \u001b[31m0.9038\u001b[0m        \u001b[94m0.6671\u001b[0m        \u001b[36m0.8168\u001b[0m  0.0000  4.0820\n",
            "     23  0.5451       0.8630    0.3983        \u001b[31m0.9038\u001b[0m        \u001b[94m0.6667\u001b[0m        \u001b[36m0.8168\u001b[0m  0.0000  3.6955\n",
            "     24  0.5451       0.8631    0.3984        \u001b[31m0.9038\u001b[0m        0.6686        \u001b[36m0.8168\u001b[0m  0.0000  3.8462\n",
            "     25  0.5453       0.8634    0.3985        \u001b[31m0.9037\u001b[0m        0.6672        \u001b[36m0.8168\u001b[0m  0.0000  4.3708\n",
            "     26  0.5451       0.8634    0.3983        \u001b[31m0.9037\u001b[0m        0.6685        \u001b[36m0.8167\u001b[0m  0.0000  3.7371\n",
            "     27  0.5462       0.8635    0.3994        \u001b[31m0.9037\u001b[0m        0.6668        \u001b[36m0.8166\u001b[0m  0.0000  3.7369\n",
            "     28  0.5457       0.8635    0.3988        0.9037        0.6673        0.8167  0.0000  4.5195\n",
            "     29  0.5457       0.8636    0.3989        0.9037        0.6672        0.8167  0.0000  3.6770\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 3/3; 7/36] END lr=0.01, module__dropout=0.3, module__linear_size=600, module__size_emb=30;, score=-1.120 total time= 2.0min\n",
            "[CV 1/3; 8/36] START lr=0.01, module__dropout=0.3, module__linear_size=600, module__size_emb=60\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4572\u001b[0m       \u001b[32m0.8381\u001b[0m    \u001b[35m0.3143\u001b[0m        \u001b[31m0.9932\u001b[0m        \u001b[94m1.0278\u001b[0m        \u001b[36m0.9865\u001b[0m  0.0100  4.0811\n",
            "      2  \u001b[36m0.6120\u001b[0m       0.7916    \u001b[35m0.4988\u001b[0m        0.9933        \u001b[94m0.8811\u001b[0m        0.9866  0.0100  3.7587\n",
            "      3  0.3185       \u001b[32m0.8783\u001b[0m    0.1945        \u001b[31m0.9765\u001b[0m        \u001b[94m0.8609\u001b[0m        \u001b[36m0.9536\u001b[0m  0.0100  3.7288\n",
            "      4  0.3079       0.8768    0.1868        0.9888        \u001b[94m0.8496\u001b[0m        0.9778  0.0100  4.4731\n",
            "      5  0.5259       0.8358    0.3836        \u001b[31m0.9674\u001b[0m        \u001b[94m0.8426\u001b[0m        \u001b[36m0.9359\u001b[0m  0.0100  3.6122\n",
            "      6  0.5144       0.8377    0.3712        0.9777        \u001b[94m0.8278\u001b[0m        0.9560  0.0100  3.7066\n",
            "      7  0.4535       0.8541    0.3087        \u001b[31m0.9571\u001b[0m        \u001b[94m0.8133\u001b[0m        \u001b[36m0.9160\u001b[0m  0.0100  4.6517\n",
            "      8  0.5003       0.8513    0.3543        \u001b[31m0.9457\u001b[0m        \u001b[94m0.7630\u001b[0m        \u001b[36m0.8943\u001b[0m  0.0010  3.7133\n",
            "      9  0.5135       0.8505    0.3678        \u001b[31m0.9407\u001b[0m        \u001b[94m0.7477\u001b[0m        \u001b[36m0.8849\u001b[0m  0.0010  3.6856\n",
            "     10  0.5561       0.8412    0.4153        \u001b[31m0.9366\u001b[0m        \u001b[94m0.7383\u001b[0m        \u001b[36m0.8772\u001b[0m  0.0010  4.5106\n",
            "     11  0.5300       0.8489    0.3852        \u001b[31m0.9350\u001b[0m        \u001b[94m0.7284\u001b[0m        \u001b[36m0.8742\u001b[0m  0.0010  3.7471\n",
            "     12  0.5619       0.8425    0.4215        \u001b[31m0.9328\u001b[0m        \u001b[94m0.7214\u001b[0m        \u001b[36m0.8701\u001b[0m  0.0010  3.7943\n",
            "     13  0.5479       0.8488    0.4045        \u001b[31m0.9307\u001b[0m        \u001b[94m0.7144\u001b[0m        \u001b[36m0.8663\u001b[0m  0.0010  4.4307\n",
            "     14  0.5311       0.8558    0.3850        \u001b[31m0.9263\u001b[0m        \u001b[94m0.7096\u001b[0m        \u001b[36m0.8581\u001b[0m  0.0010  3.8309\n",
            "     15  0.5632       0.8449    0.4224        \u001b[31m0.9256\u001b[0m        \u001b[94m0.6973\u001b[0m        \u001b[36m0.8566\u001b[0m  0.0001  3.7195\n",
            "     16  0.5567       0.8459    0.4149        \u001b[31m0.9252\u001b[0m        \u001b[94m0.6972\u001b[0m        \u001b[36m0.8560\u001b[0m  0.0001  4.0710\n",
            "     17  0.5516       0.8473    0.4089        \u001b[31m0.9248\u001b[0m        \u001b[94m0.6951\u001b[0m        \u001b[36m0.8553\u001b[0m  0.0001  4.2969\n",
            "     18  0.5577       0.8465    0.4158        \u001b[31m0.9246\u001b[0m        \u001b[94m0.6946\u001b[0m        \u001b[36m0.8548\u001b[0m  0.0001  3.8228\n",
            "     19  0.5581       0.8462    0.4163        \u001b[31m0.9241\u001b[0m        \u001b[94m0.6925\u001b[0m        \u001b[36m0.8540\u001b[0m  0.0001  3.8085\n",
            "     20  0.5645       0.8442    0.4240        \u001b[31m0.9239\u001b[0m        0.6929        \u001b[36m0.8537\u001b[0m  0.0001  4.2519\n",
            "     21  0.5612       0.8447    0.4202        \u001b[31m0.9237\u001b[0m        \u001b[94m0.6918\u001b[0m        \u001b[36m0.8531\u001b[0m  0.0001  3.7699\n",
            "     22  0.5601       0.8453    0.4188        0.9237        \u001b[94m0.6902\u001b[0m        0.8532  0.0000  3.8054\n",
            "     23  0.5600       0.8457    0.4186        \u001b[31m0.9236\u001b[0m        \u001b[94m0.6893\u001b[0m        \u001b[36m0.8531\u001b[0m  0.0000  4.6629\n",
            "     24  0.5589       0.8461    0.4173        \u001b[31m0.9236\u001b[0m        0.6898        \u001b[36m0.8530\u001b[0m  0.0000  3.6859\n",
            "     25  0.5593       0.8462    0.4176        \u001b[31m0.9235\u001b[0m        0.6896        \u001b[36m0.8529\u001b[0m  0.0000  3.7694\n",
            "     26  0.5597       0.8456    0.4183        \u001b[31m0.9235\u001b[0m        0.6924        \u001b[36m0.8528\u001b[0m  0.0000  4.5931\n",
            "     27  0.5601       0.8454    0.4187        0.9235        0.6914        0.8529  0.0000  3.7189\n",
            "     28  0.5595       0.8463    0.4179        \u001b[31m0.9235\u001b[0m        0.6908        \u001b[36m0.8528\u001b[0m  0.0000  3.8535\n",
            "     29  0.5597       0.8460    0.4182        \u001b[31m0.9234\u001b[0m        0.6897        \u001b[36m0.8528\u001b[0m  0.0000  4.5577\n",
            "     30  0.5597       0.8460    0.4182        0.9235        0.6896        0.8528  0.0000  3.7085\n",
            "[CV 1/3; 8/36] END lr=0.01, module__dropout=0.3, module__linear_size=600, module__size_emb=60;, score=-1.187 total time= 2.0min\n",
            "[CV 2/3; 8/36] START lr=0.01, module__dropout=0.3, module__linear_size=600, module__size_emb=60\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4795\u001b[0m       \u001b[32m0.8444\u001b[0m    \u001b[35m0.3348\u001b[0m        \u001b[31m0.9673\u001b[0m        \u001b[94m1.0405\u001b[0m        \u001b[36m0.9358\u001b[0m  0.0100  3.7781\n",
            "      2  0.4029       \u001b[32m0.8606\u001b[0m    0.2630        \u001b[31m0.9559\u001b[0m        \u001b[94m0.8870\u001b[0m        \u001b[36m0.9137\u001b[0m  0.0100  4.5787\n",
            "      3  0.4052       \u001b[32m0.8658\u001b[0m    0.2645        \u001b[31m0.9441\u001b[0m        \u001b[94m0.8631\u001b[0m        \u001b[36m0.8913\u001b[0m  0.0100  3.7153\n",
            "      4  0.4681       0.8602    0.3216        \u001b[31m0.9330\u001b[0m        \u001b[94m0.8449\u001b[0m        \u001b[36m0.8705\u001b[0m  0.0100  3.6515\n",
            "      5  \u001b[36m0.4973\u001b[0m       0.8540    \u001b[35m0.3508\u001b[0m        \u001b[31m0.9293\u001b[0m        \u001b[94m0.8317\u001b[0m        \u001b[36m0.8635\u001b[0m  0.0100  4.5069\n",
            "      6  \u001b[36m0.5613\u001b[0m       0.8397    \u001b[35m0.4216\u001b[0m        0.9373        \u001b[94m0.8094\u001b[0m        0.8786  0.0100  3.7571\n",
            "      7  \u001b[36m0.6325\u001b[0m       0.8244    \u001b[35m0.5131\u001b[0m        \u001b[31m0.9220\u001b[0m        \u001b[94m0.7961\u001b[0m        \u001b[36m0.8501\u001b[0m  0.0100  3.7692\n",
            "      8  0.5262       0.8640    0.3783        \u001b[31m0.9045\u001b[0m        \u001b[94m0.7408\u001b[0m        \u001b[36m0.8182\u001b[0m  0.0010  4.2716\n",
            "      9  0.5205       \u001b[32m0.8694\u001b[0m    0.3714        \u001b[31m0.9012\u001b[0m        \u001b[94m0.7239\u001b[0m        \u001b[36m0.8122\u001b[0m  0.0010  3.6624\n",
            "     10  0.5163       \u001b[32m0.8699\u001b[0m    0.3671        \u001b[31m0.8996\u001b[0m        \u001b[94m0.7131\u001b[0m        \u001b[36m0.8092\u001b[0m  0.0010  3.7389\n",
            "     11  0.5422       0.8681    0.3942        \u001b[31m0.8968\u001b[0m        \u001b[94m0.7076\u001b[0m        \u001b[36m0.8042\u001b[0m  0.0010  4.1966\n",
            "     12  0.5154       \u001b[32m0.8725\u001b[0m    0.3657        \u001b[31m0.8961\u001b[0m        \u001b[94m0.7003\u001b[0m        \u001b[36m0.8029\u001b[0m  0.0010  4.0607\n",
            "     13  0.5228       0.8706    0.3736        \u001b[31m0.8950\u001b[0m        \u001b[94m0.6961\u001b[0m        \u001b[36m0.8010\u001b[0m  0.0010  3.7308\n",
            "     14  0.5581       0.8652    0.4119        \u001b[31m0.8917\u001b[0m        \u001b[94m0.6894\u001b[0m        \u001b[36m0.7951\u001b[0m  0.0010  3.8669\n",
            "     15  0.5407       0.8682    0.3927        \u001b[31m0.8917\u001b[0m        \u001b[94m0.6787\u001b[0m        \u001b[36m0.7951\u001b[0m  0.0001  4.2419\n",
            "     16  0.5462       0.8673    0.3986        \u001b[31m0.8912\u001b[0m        \u001b[94m0.6771\u001b[0m        \u001b[36m0.7942\u001b[0m  0.0001  3.6922\n",
            "     17  0.5458       0.8674    0.3982        \u001b[31m0.8909\u001b[0m        \u001b[94m0.6770\u001b[0m        \u001b[36m0.7937\u001b[0m  0.0001  3.6728\n",
            "     18  0.5462       0.8667    0.3987        \u001b[31m0.8907\u001b[0m        \u001b[94m0.6762\u001b[0m        \u001b[36m0.7934\u001b[0m  0.0001  4.6694\n",
            "     19  0.5448       0.8669    0.3972        \u001b[31m0.8906\u001b[0m        \u001b[94m0.6746\u001b[0m        \u001b[36m0.7932\u001b[0m  0.0001  3.6763\n",
            "     20  0.5409       0.8677    0.3929        \u001b[31m0.8905\u001b[0m        0.6752        \u001b[36m0.7930\u001b[0m  0.0001  3.6361\n",
            "     21  0.5413       0.8682    0.3932        \u001b[31m0.8903\u001b[0m        \u001b[94m0.6734\u001b[0m        \u001b[36m0.7926\u001b[0m  0.0001  4.5894\n",
            "     22  0.5425       0.8679    0.3945        \u001b[31m0.8902\u001b[0m        \u001b[94m0.6714\u001b[0m        \u001b[36m0.7925\u001b[0m  0.0000  3.6894\n",
            "     23  0.5440       0.8669    0.3964        \u001b[31m0.8901\u001b[0m        0.6732        \u001b[36m0.7924\u001b[0m  0.0000  3.7297\n",
            "     24  0.5443       0.8666    0.3968        \u001b[31m0.8901\u001b[0m        0.6730        \u001b[36m0.7923\u001b[0m  0.0000  4.5438\n",
            "     25  0.5442       0.8669    0.3966        0.8901        0.6715        0.7923  0.0000  3.7276\n",
            "     26  0.5432       0.8676    0.3953        0.8901        0.6717        0.7923  0.0000  3.7669\n",
            "     27  0.5447       0.8669    0.3971        \u001b[31m0.8900\u001b[0m        0.6716        \u001b[36m0.7922\u001b[0m  0.0000  4.2589\n",
            "     28  0.5451       0.8668    0.3975        \u001b[31m0.8900\u001b[0m        0.6718        \u001b[36m0.7921\u001b[0m  0.0000  4.0959\n",
            "     29  0.5451       0.8668    0.3975        0.8900        0.6731        0.7921  0.0000  3.8269\n",
            "     30  0.5451       0.8668    0.3975        \u001b[31m0.8900\u001b[0m        0.6727        \u001b[36m0.7921\u001b[0m  0.0000  3.9823\n",
            "[CV 2/3; 8/36] END lr=0.01, module__dropout=0.3, module__linear_size=600, module__size_emb=60;, score=-1.034 total time= 2.0min\n",
            "[CV 3/3; 8/36] START lr=0.01, module__dropout=0.3, module__linear_size=600, module__size_emb=60\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4485\u001b[0m       \u001b[32m0.8415\u001b[0m    \u001b[35m0.3057\u001b[0m        \u001b[31m0.9632\u001b[0m        \u001b[94m1.0193\u001b[0m        \u001b[36m0.9277\u001b[0m  0.0100  3.7701\n",
            "      2  \u001b[36m0.4965\u001b[0m       0.8361    \u001b[35m0.3531\u001b[0m        \u001b[31m0.9560\u001b[0m        \u001b[94m0.8665\u001b[0m        \u001b[36m0.9139\u001b[0m  0.0100  3.8128\n",
            "      3  0.4370       \u001b[32m0.8610\u001b[0m    0.2928        \u001b[31m0.9520\u001b[0m        \u001b[94m0.8507\u001b[0m        \u001b[36m0.9063\u001b[0m  0.0100  4.5734\n",
            "      4  0.3117       \u001b[32m0.8885\u001b[0m    0.1890        0.9584        \u001b[94m0.8348\u001b[0m        0.9185  0.0100  3.6906\n",
            "      5  \u001b[36m0.5088\u001b[0m       0.8423    \u001b[35m0.3645\u001b[0m        \u001b[31m0.9429\u001b[0m        \u001b[94m0.8233\u001b[0m        \u001b[36m0.8890\u001b[0m  0.0100  3.4847\n",
            "      6  \u001b[36m0.5548\u001b[0m       0.8385    \u001b[35m0.4146\u001b[0m        0.9446        \u001b[94m0.8123\u001b[0m        0.8923  0.0100  4.1770\n",
            "      7  \u001b[36m0.5594\u001b[0m       0.8399    \u001b[35m0.4193\u001b[0m        0.9454        \u001b[94m0.8002\u001b[0m        0.8938  0.0100  4.0496\n",
            "      8  0.5131       0.8589    0.3658        \u001b[31m0.9237\u001b[0m        \u001b[94m0.7443\u001b[0m        \u001b[36m0.8532\u001b[0m  0.0010  3.7779\n",
            "      9  0.5320       0.8548    0.3862        \u001b[31m0.9187\u001b[0m        \u001b[94m0.7251\u001b[0m        \u001b[36m0.8440\u001b[0m  0.0010  3.9330\n",
            "     10  0.5488       0.8534    0.4045        \u001b[31m0.9146\u001b[0m        \u001b[94m0.7155\u001b[0m        \u001b[36m0.8365\u001b[0m  0.0010  4.3608\n",
            "     11  0.5286       0.8608    0.3814        \u001b[31m0.9123\u001b[0m        \u001b[94m0.7059\u001b[0m        \u001b[36m0.8324\u001b[0m  0.0010  3.7159\n",
            "     12  0.5518       0.8587    0.4065        \u001b[31m0.9096\u001b[0m        \u001b[94m0.7015\u001b[0m        \u001b[36m0.8274\u001b[0m  0.0010  3.6782\n",
            "     13  0.5145       0.8710    0.3651        \u001b[31m0.9091\u001b[0m        \u001b[94m0.6948\u001b[0m        \u001b[36m0.8264\u001b[0m  0.0010  4.5659\n",
            "     14  \u001b[36m0.5694\u001b[0m       0.8575    \u001b[35m0.4262\u001b[0m        \u001b[31m0.9050\u001b[0m        \u001b[94m0.6887\u001b[0m        \u001b[36m0.8191\u001b[0m  0.0010  3.8089\n",
            "     15  0.5464       0.8657    0.3992        0.9053        \u001b[94m0.6776\u001b[0m        0.8195  0.0001  3.6720\n",
            "     16  0.5460       0.8663    0.3987        \u001b[31m0.9046\u001b[0m        0.6777        \u001b[36m0.8183\u001b[0m  0.0001  4.5699\n",
            "     17  0.5446       0.8658    0.3972        \u001b[31m0.9044\u001b[0m        0.6777        \u001b[36m0.8179\u001b[0m  0.0001  3.6841\n",
            "     18  0.5480       0.8656    0.4009        \u001b[31m0.9040\u001b[0m        \u001b[94m0.6757\u001b[0m        \u001b[36m0.8172\u001b[0m  0.0001  3.7325\n",
            "     19  0.5529       0.8632    0.4067        \u001b[31m0.9035\u001b[0m        \u001b[94m0.6747\u001b[0m        \u001b[36m0.8164\u001b[0m  0.0001  4.7617\n",
            "     20  0.5501       0.8652    0.4032        \u001b[31m0.9034\u001b[0m        \u001b[94m0.6743\u001b[0m        \u001b[36m0.8161\u001b[0m  0.0001  3.7114\n",
            "     21  0.5465       0.8660    0.3992        0.9035        \u001b[94m0.6734\u001b[0m        0.8163  0.0001  3.7943\n",
            "     22  0.5478       0.8657    0.4006        0.9034        \u001b[94m0.6701\u001b[0m        0.8161  0.0000  4.4130\n",
            "     23  0.5485       0.8655    0.4014        \u001b[31m0.9033\u001b[0m        0.6708        \u001b[36m0.8160\u001b[0m  0.0000  3.8765\n",
            "     24  0.5482       0.8657    0.4011        \u001b[31m0.9032\u001b[0m        0.6734        \u001b[36m0.8158\u001b[0m  0.0000  3.7598\n",
            "     25  0.5493       0.8659    0.4022        \u001b[31m0.9032\u001b[0m        0.6712        \u001b[36m0.8157\u001b[0m  0.0000  3.9395\n",
            "     26  0.5491       0.8658    0.4021        \u001b[31m0.9031\u001b[0m        0.6718        \u001b[36m0.8157\u001b[0m  0.0000  4.2084\n",
            "     27  0.5495       0.8656    0.4025        \u001b[31m0.9031\u001b[0m        0.6703        \u001b[36m0.8156\u001b[0m  0.0000  3.6272\n",
            "     28  0.5493       0.8657    0.4022        \u001b[31m0.9031\u001b[0m        0.6701        \u001b[36m0.8155\u001b[0m  0.0000  3.7261\n",
            "     29  0.5491       0.8658    0.4021        0.9031        0.6706        0.8155  0.0000  4.5527\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 3/3; 8/36] END lr=0.01, module__dropout=0.3, module__linear_size=600, module__size_emb=60;, score=-1.126 total time= 2.0min\n",
            "[CV 1/3; 9/36] START lr=0.01, module__dropout=0.3, module__linear_size=600, module__size_emb=120\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4545\u001b[0m       \u001b[32m0.8335\u001b[0m    \u001b[35m0.3124\u001b[0m        \u001b[31m0.9837\u001b[0m        \u001b[94m1.0056\u001b[0m        \u001b[36m0.9678\u001b[0m  0.0100  4.3752\n",
            "      2  \u001b[36m0.5362\u001b[0m       0.8254    \u001b[35m0.3970\u001b[0m        \u001b[31m0.9733\u001b[0m        \u001b[94m0.8821\u001b[0m        \u001b[36m0.9473\u001b[0m  0.0100  3.8985\n",
            "      3  \u001b[36m0.5409\u001b[0m       0.8172    \u001b[35m0.4042\u001b[0m        0.9769        \u001b[94m0.8661\u001b[0m        0.9543  0.0100  3.7227\n",
            "      4  0.3260       \u001b[32m0.8782\u001b[0m    0.2002        \u001b[31m0.9728\u001b[0m        \u001b[94m0.8483\u001b[0m        \u001b[36m0.9463\u001b[0m  0.0100  4.1861\n",
            "      5  0.5235       0.8385    0.3806        \u001b[31m0.9707\u001b[0m        \u001b[94m0.8455\u001b[0m        \u001b[36m0.9423\u001b[0m  0.0100  4.1589\n",
            "      6  0.4671       0.8447    0.3228        \u001b[31m0.9591\u001b[0m        \u001b[94m0.8301\u001b[0m        \u001b[36m0.9198\u001b[0m  0.0100  3.7560\n",
            "      7  0.3231       0.8714    0.1983        0.9756        \u001b[94m0.8174\u001b[0m        0.9517  0.0100  3.7799\n",
            "      8  0.4897       0.8559    0.3430        \u001b[31m0.9439\u001b[0m        \u001b[94m0.7646\u001b[0m        \u001b[36m0.8909\u001b[0m  0.0010  4.4836\n",
            "      9  0.5104       0.8560    0.3636        \u001b[31m0.9401\u001b[0m        \u001b[94m0.7464\u001b[0m        \u001b[36m0.8839\u001b[0m  0.0010  3.8263\n",
            "     10  0.5075       0.8563    0.3606        \u001b[31m0.9363\u001b[0m        \u001b[94m0.7341\u001b[0m        \u001b[36m0.8767\u001b[0m  0.0010  3.8125\n",
            "     11  0.4660       0.8714    0.3180        \u001b[31m0.9358\u001b[0m        \u001b[94m0.7295\u001b[0m        \u001b[36m0.8758\u001b[0m  0.0010  4.3404\n",
            "     12  0.4939       0.8631    0.3459        \u001b[31m0.9344\u001b[0m        \u001b[94m0.7210\u001b[0m        \u001b[36m0.8731\u001b[0m  0.0010  3.7042\n",
            "     13  0.5377       0.8545    0.3923        \u001b[31m0.9295\u001b[0m        \u001b[94m0.7167\u001b[0m        \u001b[36m0.8641\u001b[0m  0.0010  3.7022\n",
            "     14  0.5139       0.8611    0.3662        0.9297        \u001b[94m0.7102\u001b[0m        0.8643  0.0010  4.6123\n",
            "     15  0.5209       0.8591    0.3738        \u001b[31m0.9286\u001b[0m        \u001b[94m0.6969\u001b[0m        \u001b[36m0.8623\u001b[0m  0.0001  3.6952\n",
            "     16  0.5102       0.8622    0.3623        \u001b[31m0.9280\u001b[0m        0.6974        \u001b[36m0.8612\u001b[0m  0.0001  3.6880\n",
            "     17  0.5068       0.8648    0.3585        \u001b[31m0.9275\u001b[0m        \u001b[94m0.6968\u001b[0m        \u001b[36m0.8603\u001b[0m  0.0001  4.4370\n",
            "     18  0.5137       0.8617    0.3659        \u001b[31m0.9271\u001b[0m        \u001b[94m0.6937\u001b[0m        \u001b[36m0.8595\u001b[0m  0.0001  3.7101\n",
            "     19  0.5220       0.8587    0.3749        \u001b[31m0.9268\u001b[0m        \u001b[94m0.6926\u001b[0m        \u001b[36m0.8590\u001b[0m  0.0001  3.7037\n",
            "     20  0.5244       0.8572    0.3777        \u001b[31m0.9266\u001b[0m        \u001b[94m0.6919\u001b[0m        \u001b[36m0.8586\u001b[0m  0.0001  4.2122\n",
            "     21  0.5147       0.8612    0.3671        \u001b[31m0.9264\u001b[0m        \u001b[94m0.6912\u001b[0m        \u001b[36m0.8582\u001b[0m  0.0001  4.1617\n",
            "     22  0.5185       0.8593    0.3713        \u001b[31m0.9263\u001b[0m        \u001b[94m0.6907\u001b[0m        \u001b[36m0.8580\u001b[0m  0.0000  3.6850\n",
            "     23  0.5194       0.8594    0.3722        \u001b[31m0.9262\u001b[0m        0.6915        \u001b[36m0.8579\u001b[0m  0.0000  3.8008\n",
            "     24  0.5214       0.8586    0.3744        \u001b[31m0.9261\u001b[0m        \u001b[94m0.6894\u001b[0m        \u001b[36m0.8577\u001b[0m  0.0000  4.4302\n",
            "     25  0.5217       0.8586    0.3747        \u001b[31m0.9261\u001b[0m        0.6904        \u001b[36m0.8577\u001b[0m  0.0000  3.7903\n",
            "     26  0.5225       0.8584    0.3756        \u001b[31m0.9261\u001b[0m        0.6915        \u001b[36m0.8576\u001b[0m  0.0000  3.7236\n",
            "     27  0.5222       0.8586    0.3752        \u001b[31m0.9260\u001b[0m        0.6910        \u001b[36m0.8576\u001b[0m  0.0000  4.5306\n",
            "     28  0.5214       0.8594    0.3742        0.9261        0.6916        0.8577  0.0000  3.4712\n",
            "     29  0.5216       0.8591    0.3745        0.9261        0.6908        0.8577  0.0000  3.7473\n",
            "     30  0.5216       0.8588    0.3746        0.9261        0.6900        0.8577  0.0000  4.6613\n",
            "[CV 1/3; 9/36] END lr=0.01, module__dropout=0.3, module__linear_size=600, module__size_emb=120;, score=-1.189 total time= 2.0min\n",
            "[CV 2/3; 9/36] START lr=0.01, module__dropout=0.3, module__linear_size=600, module__size_emb=120\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.5016\u001b[0m       \u001b[32m0.8336\u001b[0m    \u001b[35m0.3587\u001b[0m        \u001b[31m0.9583\u001b[0m        \u001b[94m1.0511\u001b[0m        \u001b[36m0.9183\u001b[0m  0.0100  3.7323\n",
            "      2  0.4041       \u001b[32m0.8659\u001b[0m    0.2636        \u001b[31m0.9479\u001b[0m        \u001b[94m0.8863\u001b[0m        \u001b[36m0.8985\u001b[0m  0.0100  3.8513\n",
            "      3  \u001b[36m0.5611\u001b[0m       0.8318    \u001b[35m0.4233\u001b[0m        \u001b[31m0.9458\u001b[0m        \u001b[94m0.8645\u001b[0m        \u001b[36m0.8945\u001b[0m  0.0100  4.4600\n",
            "      4  0.5543       0.8389    0.4139        0.9463        \u001b[94m0.8384\u001b[0m        0.8955  0.0100  3.7273\n",
            "      5  0.5075       0.8526    0.3613        \u001b[31m0.9234\u001b[0m        \u001b[94m0.8241\u001b[0m        \u001b[36m0.8527\u001b[0m  0.0100  3.8418\n",
            "      6  0.4693       \u001b[32m0.8729\u001b[0m    0.3209        0.9311        \u001b[94m0.8019\u001b[0m        0.8670  0.0100  4.6143\n",
            "      7  0.4993       0.8646    0.3510        \u001b[31m0.9196\u001b[0m        \u001b[94m0.7888\u001b[0m        \u001b[36m0.8456\u001b[0m  0.0100  3.7463\n",
            "      8  0.5176       0.8672    0.3689        \u001b[31m0.8998\u001b[0m        \u001b[94m0.7302\u001b[0m        \u001b[36m0.8096\u001b[0m  0.0010  3.7489\n",
            "      9  0.4968       \u001b[32m0.8758\u001b[0m    0.3467        \u001b[31m0.8991\u001b[0m        \u001b[94m0.7095\u001b[0m        \u001b[36m0.8083\u001b[0m  0.0010  4.5535\n",
            "     10  0.4947       \u001b[32m0.8768\u001b[0m    0.3446        \u001b[31m0.8929\u001b[0m        \u001b[94m0.6988\u001b[0m        \u001b[36m0.7973\u001b[0m  0.0010  3.7866\n",
            "     11  0.5404       0.8682    0.3923        \u001b[31m0.8890\u001b[0m        \u001b[94m0.6927\u001b[0m        \u001b[36m0.7903\u001b[0m  0.0010  3.7839\n",
            "     12  0.4789       \u001b[32m0.8816\u001b[0m    0.3287        0.8897        \u001b[94m0.6840\u001b[0m        0.7916  0.0010  4.5442\n",
            "     13  0.5280       0.8703    0.3790        \u001b[31m0.8848\u001b[0m        \u001b[94m0.6776\u001b[0m        \u001b[36m0.7829\u001b[0m  0.0010  3.7055\n",
            "     14  0.5171       0.8740    0.3671        \u001b[31m0.8831\u001b[0m        \u001b[94m0.6713\u001b[0m        \u001b[36m0.7798\u001b[0m  0.0010  3.7565\n",
            "     15  0.5304       0.8703    0.3815        \u001b[31m0.8822\u001b[0m        \u001b[94m0.6605\u001b[0m        \u001b[36m0.7783\u001b[0m  0.0001  4.1741\n",
            "     16  0.5192       0.8757    0.3690        \u001b[31m0.8821\u001b[0m        0.6612        \u001b[36m0.7781\u001b[0m  0.0001  4.1948\n",
            "     17  0.5263       0.8728    0.3767        \u001b[31m0.8814\u001b[0m        \u001b[94m0.6586\u001b[0m        \u001b[36m0.7769\u001b[0m  0.0001  3.8008\n",
            "     18  0.5238       0.8746    0.3739        \u001b[31m0.8812\u001b[0m        0.6592        \u001b[36m0.7765\u001b[0m  0.0001  3.8512\n",
            "     19  0.5199       0.8764    0.3696        \u001b[31m0.8811\u001b[0m        \u001b[94m0.6581\u001b[0m        \u001b[36m0.7763\u001b[0m  0.0001  4.1503\n",
            "     20  0.5215       0.8757    0.3714        \u001b[31m0.8809\u001b[0m        \u001b[94m0.6534\u001b[0m        \u001b[36m0.7759\u001b[0m  0.0001  3.7863\n",
            "     21  0.5206       0.8763    0.3703        \u001b[31m0.8804\u001b[0m        0.6568        \u001b[36m0.7752\u001b[0m  0.0001  4.0734\n",
            "     22  0.5239       0.8742    0.3740        \u001b[31m0.8802\u001b[0m        0.6538        \u001b[36m0.7748\u001b[0m  0.0000  4.3767\n",
            "     23  0.5234       0.8741    0.3735        \u001b[31m0.8802\u001b[0m        0.6535        \u001b[36m0.7747\u001b[0m  0.0000  3.6825\n",
            "     24  0.5252       0.8741    0.3754        \u001b[31m0.8801\u001b[0m        0.6539        \u001b[36m0.7746\u001b[0m  0.0000  3.6767\n",
            "     25  0.5254       0.8740    0.3756        \u001b[31m0.8801\u001b[0m        \u001b[94m0.6524\u001b[0m        \u001b[36m0.7746\u001b[0m  0.0000  4.6274\n",
            "     26  0.5255       0.8735    0.3758        \u001b[31m0.8801\u001b[0m        0.6534        \u001b[36m0.7745\u001b[0m  0.0000  3.7375\n",
            "     27  0.5266       0.8734    0.3769        \u001b[31m0.8800\u001b[0m        0.6543        \u001b[36m0.7744\u001b[0m  0.0000  3.6744\n",
            "     28  0.5252       0.8736    0.3755        0.8800        0.6528        0.7744  0.0000  4.5903\n",
            "     29  0.5251       0.8735    0.3754        0.8800        0.6526        0.7744  0.0000  3.6849\n",
            "     30  0.5253       0.8736    0.3756        \u001b[31m0.8800\u001b[0m        0.6545        \u001b[36m0.7744\u001b[0m  0.0000  3.7260\n",
            "[CV 2/3; 9/36] END lr=0.01, module__dropout=0.3, module__linear_size=600, module__size_emb=120;, score=-1.039 total time= 2.0min\n",
            "[CV 3/3; 9/36] START lr=0.01, module__dropout=0.3, module__linear_size=600, module__size_emb=120\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.2962\u001b[0m       \u001b[32m0.8763\u001b[0m    \u001b[35m0.1782\u001b[0m        \u001b[31m0.9739\u001b[0m        \u001b[94m1.0067\u001b[0m        \u001b[36m0.9485\u001b[0m  0.0100  4.3993\n",
            "      2  \u001b[36m0.3317\u001b[0m       \u001b[32m0.8794\u001b[0m    \u001b[35m0.2044\u001b[0m        \u001b[31m0.9640\u001b[0m        \u001b[94m0.8668\u001b[0m        \u001b[36m0.9293\u001b[0m  0.0100  3.7340\n",
            "      3  \u001b[36m0.4252\u001b[0m       0.8628    \u001b[35m0.2821\u001b[0m        \u001b[31m0.9477\u001b[0m        \u001b[94m0.8462\u001b[0m        \u001b[36m0.8981\u001b[0m  0.0100  3.6883\n",
            "      4  \u001b[36m0.5002\u001b[0m       0.8453    \u001b[35m0.3551\u001b[0m        \u001b[31m0.9422\u001b[0m        \u001b[94m0.8230\u001b[0m        \u001b[36m0.8878\u001b[0m  0.0100  4.6111\n",
            "      5  \u001b[36m0.5103\u001b[0m       0.8450    \u001b[35m0.3655\u001b[0m        \u001b[31m0.9326\u001b[0m        \u001b[94m0.8032\u001b[0m        \u001b[36m0.8697\u001b[0m  0.0100  3.4591\n",
            "      6  \u001b[36m0.5140\u001b[0m       0.8512    \u001b[35m0.3681\u001b[0m        \u001b[31m0.9288\u001b[0m        \u001b[94m0.7870\u001b[0m        \u001b[36m0.8626\u001b[0m  0.0100  3.7859\n",
            "      7  \u001b[36m0.5953\u001b[0m       0.8342    \u001b[35m0.4628\u001b[0m        \u001b[31m0.9227\u001b[0m        \u001b[94m0.7781\u001b[0m        \u001b[36m0.8515\u001b[0m  0.0100  4.5434\n",
            "      8  0.4986       0.8676    0.3498        \u001b[31m0.9149\u001b[0m        \u001b[94m0.7202\u001b[0m        \u001b[36m0.8370\u001b[0m  0.0010  3.6973\n",
            "      9  0.5014       0.8660    0.3528        \u001b[31m0.9117\u001b[0m        \u001b[94m0.6991\u001b[0m        \u001b[36m0.8312\u001b[0m  0.0010  3.7190\n",
            "     10  0.4863       0.8731    0.3370        \u001b[31m0.9080\u001b[0m        \u001b[94m0.6907\u001b[0m        \u001b[36m0.8245\u001b[0m  0.0010  4.6365\n",
            "     11  0.5061       0.8697    0.3568        \u001b[31m0.9043\u001b[0m        \u001b[94m0.6820\u001b[0m        \u001b[36m0.8177\u001b[0m  0.0010  3.7602\n",
            "     12  0.5214       0.8675    0.3727        \u001b[31m0.9015\u001b[0m        \u001b[94m0.6751\u001b[0m        \u001b[36m0.8128\u001b[0m  0.0010  3.7517\n",
            "     13  0.4646       \u001b[32m0.8829\u001b[0m    0.3152        0.9019        \u001b[94m0.6719\u001b[0m        0.8134  0.0010  4.3000\n",
            "     14  0.5256       0.8714    0.3763        \u001b[31m0.8949\u001b[0m        \u001b[94m0.6665\u001b[0m        \u001b[36m0.8009\u001b[0m  0.0010  3.8849\n",
            "     15  0.5078       0.8744    0.3577        0.8958        \u001b[94m0.6541\u001b[0m        0.8025  0.0001  3.6826\n",
            "     16  0.5120       0.8731    0.3622        \u001b[31m0.8949\u001b[0m        \u001b[94m0.6534\u001b[0m        \u001b[36m0.8008\u001b[0m  0.0001  4.0419\n",
            "     17  0.5245       0.8722    0.3750        \u001b[31m0.8942\u001b[0m        \u001b[94m0.6526\u001b[0m        \u001b[36m0.7996\u001b[0m  0.0001  4.3300\n",
            "     18  0.5152       0.8735    0.3654        0.8943        \u001b[94m0.6516\u001b[0m        0.7997  0.0001  3.6408\n",
            "     19  0.5125       0.8743    0.3625        0.8943        \u001b[94m0.6509\u001b[0m        0.7997  0.0001  3.6440\n",
            "     20  0.5160       0.8727    0.3663        \u001b[31m0.8936\u001b[0m        \u001b[94m0.6504\u001b[0m        \u001b[36m0.7985\u001b[0m  0.0001  4.5881\n",
            "     21  0.5137       0.8738    0.3638        0.8937        \u001b[94m0.6498\u001b[0m        0.7987  0.0001  3.7779\n",
            "     22  0.5145       0.8738    0.3645        0.8937        \u001b[94m0.6485\u001b[0m        0.7986  0.0000  3.7148\n",
            "     23  0.5153       0.8730    0.3655        \u001b[31m0.8936\u001b[0m        \u001b[94m0.6480\u001b[0m        \u001b[36m0.7985\u001b[0m  0.0000  4.5478\n",
            "     24  0.5142       0.8737    0.3643        0.8936        \u001b[94m0.6474\u001b[0m        0.7986  0.0000  3.4821\n",
            "     25  0.5148       0.8737    0.3649        \u001b[31m0.8936\u001b[0m        \u001b[94m0.6468\u001b[0m        \u001b[36m0.7985\u001b[0m  0.0000  3.7763\n",
            "     26  0.5138       0.8743    0.3637        0.8936        0.6476        0.7985  0.0000  4.5156\n",
            "     27  0.5146       0.8740    0.3646        \u001b[31m0.8935\u001b[0m        0.6469        \u001b[36m0.7983\u001b[0m  0.0000  3.8369\n",
            "     28  0.5149       0.8741    0.3649        \u001b[31m0.8934\u001b[0m        0.6482        \u001b[36m0.7982\u001b[0m  0.0000  3.7148\n",
            "     29  0.5150       0.8740    0.3651        \u001b[31m0.8934\u001b[0m        0.6477        \u001b[36m0.7982\u001b[0m  0.0000  4.4681\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 3/3; 9/36] END lr=0.01, module__dropout=0.3, module__linear_size=600, module__size_emb=120;, score=-1.120 total time= 2.0min\n",
            "[CV 1/3; 10/36] START lr=0.01, module__dropout=0.5, module__linear_size=400, module__size_emb=30\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4832\u001b[0m       \u001b[32m0.8365\u001b[0m    \u001b[35m0.3397\u001b[0m        \u001b[31m0.9881\u001b[0m        \u001b[94m1.0218\u001b[0m        \u001b[36m0.9763\u001b[0m  0.0100  3.6455\n",
            "      2  0.3653       \u001b[32m0.8708\u001b[0m    0.2312        \u001b[31m0.9772\u001b[0m        \u001b[94m0.8998\u001b[0m        \u001b[36m0.9549\u001b[0m  0.0100  4.5656\n",
            "      3  0.3392       0.8662    0.2109        0.9785        \u001b[94m0.8830\u001b[0m        0.9574  0.0100  3.6526\n",
            "      4  0.4791       0.8265    0.3373        0.9788        \u001b[94m0.8700\u001b[0m        0.9581  0.0100  3.6859\n",
            "      5  0.4109       0.8446    0.2715        0.9803        \u001b[94m0.8657\u001b[0m        0.9609  0.0100  4.4961\n",
            "      6  0.3721       0.8437    0.2387        0.9813        \u001b[94m0.8608\u001b[0m        0.9629  0.0100  3.6424\n",
            "      7  0.4316       0.8381    0.2906        \u001b[31m0.9761\u001b[0m        \u001b[94m0.8557\u001b[0m        \u001b[36m0.9528\u001b[0m  0.0100  3.6622\n",
            "      8  0.3511       0.8612    0.2205        \u001b[31m0.9668\u001b[0m        \u001b[94m0.8122\u001b[0m        \u001b[36m0.9347\u001b[0m  0.0010  4.0574\n",
            "      9  0.3616       0.8609    0.2288        \u001b[31m0.9632\u001b[0m        \u001b[94m0.7957\u001b[0m        \u001b[36m0.9277\u001b[0m  0.0010  4.0098\n",
            "     10  0.3505       0.8624    0.2200        \u001b[31m0.9631\u001b[0m        \u001b[94m0.7869\u001b[0m        \u001b[36m0.9275\u001b[0m  0.0010  3.5752\n",
            "     11  0.4131       0.8547    0.2723        \u001b[31m0.9576\u001b[0m        \u001b[94m0.7838\u001b[0m        \u001b[36m0.9170\u001b[0m  0.0010  3.6790\n",
            "     12  0.3695       0.8627    0.2351        0.9589        \u001b[94m0.7764\u001b[0m        0.9195  0.0010  4.5121\n",
            "     13  0.3866       0.8611    0.2492        \u001b[31m0.9564\u001b[0m        \u001b[94m0.7720\u001b[0m        \u001b[36m0.9148\u001b[0m  0.0010  3.6016\n",
            "     14  0.4140       0.8537    0.2732        \u001b[31m0.9520\u001b[0m        \u001b[94m0.7668\u001b[0m        \u001b[36m0.9062\u001b[0m  0.0010  3.6213\n",
            "     15  0.3872       0.8611    0.2498        0.9541        \u001b[94m0.7603\u001b[0m        0.9104  0.0001  4.4482\n",
            "     16  0.3850       0.8615    0.2479        0.9536        \u001b[94m0.7573\u001b[0m        0.9093  0.0001  3.5693\n",
            "     17  0.3890       0.8598    0.2514        0.9532        \u001b[94m0.7572\u001b[0m        0.9086  0.0001  3.4509\n",
            "     18  0.3924       0.8603    0.2542        0.9524        \u001b[94m0.7568\u001b[0m        0.9070  0.0001  4.4563\n",
            "     19  0.3860       0.8606    0.2488        0.9525        \u001b[94m0.7535\u001b[0m        0.9073  0.0001  3.7086\n",
            "     20  0.3886       0.8609    0.2509        0.9522        0.7556        0.9066  0.0001  3.6542\n",
            "     21  0.3913       0.8596    0.2533        0.9521        0.7550        0.9064  0.0001  4.0440\n",
            "     22  0.3911       0.8598    0.2531        0.9520        0.7540        0.9063  0.0000  4.3816\n",
            "     23  0.3910       0.8598    0.2530        \u001b[31m0.9519\u001b[0m        \u001b[94m0.7532\u001b[0m        \u001b[36m0.9062\u001b[0m  0.0000  3.7258\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 1/3; 10/36] END lr=0.01, module__dropout=0.5, module__linear_size=400, module__size_emb=30;, score=-1.175 total time= 1.6min\n",
            "[CV 2/3; 10/36] START lr=0.01, module__dropout=0.5, module__linear_size=400, module__size_emb=30\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.5691\u001b[0m       \u001b[32m0.8113\u001b[0m    \u001b[35m0.4383\u001b[0m        \u001b[31m0.9712\u001b[0m        \u001b[94m1.0848\u001b[0m        \u001b[36m0.9432\u001b[0m  0.0100  3.6168\n",
            "      2  0.4314       \u001b[32m0.8529\u001b[0m    0.2887        \u001b[31m0.9578\u001b[0m        \u001b[94m0.9163\u001b[0m        \u001b[36m0.9173\u001b[0m  0.0100  3.6425\n",
            "      3  0.4347       \u001b[32m0.8574\u001b[0m    0.2911        \u001b[31m0.9494\u001b[0m        \u001b[94m0.8832\u001b[0m        \u001b[36m0.9014\u001b[0m  0.0100  3.9025\n",
            "      4  0.5322       0.8330    0.3910        \u001b[31m0.9478\u001b[0m        \u001b[94m0.8752\u001b[0m        \u001b[36m0.8984\u001b[0m  0.0100  4.1746\n",
            "      5  0.4385       \u001b[32m0.8579\u001b[0m    0.2945        \u001b[31m0.9469\u001b[0m        \u001b[94m0.8617\u001b[0m        \u001b[36m0.8966\u001b[0m  0.0100  3.6009\n",
            "      6  0.2771       \u001b[32m0.8958\u001b[0m    0.1639        0.9540        \u001b[94m0.8602\u001b[0m        0.9101  0.0100  3.5636\n",
            "      7  0.5243       0.8459    0.3799        \u001b[31m0.9370\u001b[0m        \u001b[94m0.8543\u001b[0m        \u001b[36m0.8779\u001b[0m  0.0100  4.5204\n",
            "      8  0.4271       0.8717    0.2828        0.9387        \u001b[94m0.8142\u001b[0m        0.8811  0.0010  3.6446\n",
            "      9  0.4611       0.8596    0.3150        \u001b[31m0.9329\u001b[0m        \u001b[94m0.7956\u001b[0m        \u001b[36m0.8703\u001b[0m  0.0010  3.6744\n",
            "     10  0.4219       0.8730    0.2782        0.9346        \u001b[94m0.7891\u001b[0m        0.8734  0.0010  4.3759\n",
            "     11  0.4334       0.8706    0.2885        \u001b[31m0.9312\u001b[0m        \u001b[94m0.7816\u001b[0m        \u001b[36m0.8672\u001b[0m  0.0010  3.5985\n",
            "     12  0.4313       0.8710    0.2866        \u001b[31m0.9306\u001b[0m        \u001b[94m0.7742\u001b[0m        \u001b[36m0.8661\u001b[0m  0.0010  3.5781\n",
            "     13  0.4241       0.8747    0.2799        \u001b[31m0.9287\u001b[0m        \u001b[94m0.7685\u001b[0m        \u001b[36m0.8625\u001b[0m  0.0010  4.2813\n",
            "     14  0.4247       0.8763    0.2803        \u001b[31m0.9271\u001b[0m        \u001b[94m0.7676\u001b[0m        \u001b[36m0.8595\u001b[0m  0.0010  3.9468\n",
            "     15  0.4507       0.8680    0.3044        \u001b[31m0.9244\u001b[0m        \u001b[94m0.7591\u001b[0m        \u001b[36m0.8545\u001b[0m  0.0001  3.6117\n",
            "     16  0.4492       0.8688    0.3029        0.9244        \u001b[94m0.7575\u001b[0m        0.8545  0.0001  3.8094\n",
            "     17  0.4474       0.8701    0.3011        \u001b[31m0.9243\u001b[0m        \u001b[94m0.7572\u001b[0m        \u001b[36m0.8543\u001b[0m  0.0001  4.2330\n",
            "     18  0.4445       0.8698    0.2986        0.9244        \u001b[94m0.7546\u001b[0m        0.8545  0.0001  3.6650\n",
            "     19  0.4427       0.8712    0.2968        0.9244        0.7560        0.8545  0.0001  3.6622\n",
            "     20  0.4370       0.8731    0.2914        0.9246        0.7564        0.8548  0.0001  4.6397\n",
            "     21  0.4497       0.8688    0.3034        \u001b[31m0.9234\u001b[0m        \u001b[94m0.7524\u001b[0m        \u001b[36m0.8527\u001b[0m  0.0001  3.5737\n",
            "     22  0.4494       0.8687    0.3031        \u001b[31m0.9234\u001b[0m        0.7533        \u001b[36m0.8527\u001b[0m  0.0000  3.6359\n",
            "     23  0.4496       0.8687    0.3033        \u001b[31m0.9234\u001b[0m        0.7536        \u001b[36m0.8526\u001b[0m  0.0000  4.6615\n",
            "     24  0.4492       0.8688    0.3029        0.9234        0.7529        0.8526  0.0000  3.8945\n",
            "     25  0.4495       0.8689    0.3031        \u001b[31m0.9234\u001b[0m        0.7538        \u001b[36m0.8526\u001b[0m  0.0000  3.7459\n",
            "     26  0.4492       0.8688    0.3029        0.9234        0.7529        0.8526  0.0000  4.6383\n",
            "     27  0.4488       0.8687    0.3026        0.9234        0.7528        0.8526  0.0000  3.3980\n",
            "     28  0.4493       0.8688    0.3030        \u001b[31m0.9233\u001b[0m        0.7527        \u001b[36m0.8526\u001b[0m  0.0000  3.6518\n",
            "     29  0.4493       0.8688    0.3030        \u001b[31m0.9233\u001b[0m        0.7525        \u001b[36m0.8526\u001b[0m  0.0000  4.1544\n",
            "     30  0.4493       0.8688    0.3030        0.9233        0.7527        0.8526  0.0000  4.1344\n",
            "[CV 2/3; 10/36] END lr=0.01, module__dropout=0.5, module__linear_size=400, module__size_emb=30;, score=-1.030 total time= 2.0min\n",
            "[CV 3/3; 10/36] START lr=0.01, module__dropout=0.5, module__linear_size=400, module__size_emb=30\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4140\u001b[0m       \u001b[32m0.8443\u001b[0m    \u001b[35m0.2742\u001b[0m        \u001b[31m0.9674\u001b[0m        \u001b[94m1.0367\u001b[0m        \u001b[36m0.9359\u001b[0m  0.0100  3.7289\n",
            "      2  \u001b[36m0.4200\u001b[0m       \u001b[32m0.8535\u001b[0m    \u001b[35m0.2785\u001b[0m        \u001b[31m0.9582\u001b[0m        \u001b[94m0.8871\u001b[0m        \u001b[36m0.9182\u001b[0m  0.0100  4.4618\n",
            "      3  \u001b[36m0.5514\u001b[0m       0.8241    \u001b[35m0.4143\u001b[0m        \u001b[31m0.9535\u001b[0m        \u001b[94m0.8663\u001b[0m        \u001b[36m0.9093\u001b[0m  0.0100  3.6761\n",
            "      4  0.4583       0.8507    0.3136        \u001b[31m0.9500\u001b[0m        \u001b[94m0.8564\u001b[0m        \u001b[36m0.9025\u001b[0m  0.0100  3.6464\n",
            "      5  0.3241       \u001b[32m0.8675\u001b[0m    0.1993        0.9601        \u001b[94m0.8439\u001b[0m        0.9218  0.0100  4.2962\n",
            "      6  0.3562       \u001b[32m0.8711\u001b[0m    0.2239        0.9572        \u001b[94m0.8397\u001b[0m        0.9162  0.0100  3.9115\n",
            "      7  \u001b[36m0.5802\u001b[0m       0.8260    \u001b[35m0.4472\u001b[0m        \u001b[31m0.9460\u001b[0m        \u001b[94m0.8350\u001b[0m        \u001b[36m0.8948\u001b[0m  0.0100  3.5228\n",
            "      8  0.3705       \u001b[32m0.8770\u001b[0m    0.2348        \u001b[31m0.9456\u001b[0m        \u001b[94m0.7917\u001b[0m        \u001b[36m0.8942\u001b[0m  0.0010  3.6965\n",
            "      9  0.3550       \u001b[32m0.8857\u001b[0m    0.2220        \u001b[31m0.9446\u001b[0m        \u001b[94m0.7730\u001b[0m        \u001b[36m0.8922\u001b[0m  0.0010  4.4026\n",
            "     10  0.3867       0.8745    0.2483        \u001b[31m0.9387\u001b[0m        \u001b[94m0.7632\u001b[0m        \u001b[36m0.8812\u001b[0m  0.0010  3.7115\n",
            "     11  0.4384       0.8696    0.2931        \u001b[31m0.9332\u001b[0m        \u001b[94m0.7558\u001b[0m        \u001b[36m0.8708\u001b[0m  0.0010  3.6438\n",
            "     12  0.3765       0.8836    0.2392        0.9379        \u001b[94m0.7519\u001b[0m        0.8797  0.0010  4.4472\n",
            "     13  0.3689       \u001b[32m0.8877\u001b[0m    0.2329        0.9385        \u001b[94m0.7456\u001b[0m        0.8807  0.0010  3.5013\n",
            "     14  0.3988       0.8781    0.2580        \u001b[31m0.9330\u001b[0m        \u001b[94m0.7412\u001b[0m        \u001b[36m0.8706\u001b[0m  0.0010  3.6131\n",
            "     15  0.3950       0.8797    0.2547        0.9333        \u001b[94m0.7362\u001b[0m        0.8710  0.0001  4.5191\n",
            "     16  0.4009       0.8796    0.2596        \u001b[31m0.9326\u001b[0m        \u001b[94m0.7359\u001b[0m        \u001b[36m0.8698\u001b[0m  0.0001  3.6289\n",
            "     17  0.4009       0.8801    0.2595        \u001b[31m0.9326\u001b[0m        \u001b[94m0.7307\u001b[0m        \u001b[36m0.8697\u001b[0m  0.0001  3.3826\n",
            "     18  0.3994       0.8799    0.2584        \u001b[31m0.9325\u001b[0m        0.7319        \u001b[36m0.8696\u001b[0m  0.0001  4.0689\n",
            "     19  0.4071       0.8778    0.2650        \u001b[31m0.9317\u001b[0m        0.7308        \u001b[36m0.8680\u001b[0m  0.0001  3.9521\n",
            "     20  0.4044       0.8787    0.2627        0.9318        \u001b[94m0.7287\u001b[0m        0.8682  0.0001  3.6018\n",
            "     21  0.4056       0.8801    0.2636        0.9318        0.7290        0.8682  0.0001  3.6562\n",
            "     22  0.4063       0.8801    0.2641        0.9317        \u001b[94m0.7286\u001b[0m        0.8681  0.0000  4.4853\n",
            "     23  0.4054       0.8800    0.2634        0.9318        0.7293        0.8682  0.0000  3.5988\n",
            "     24  0.4052       0.8802    0.2632        0.9318        0.7291        0.8682  0.0000  3.5901\n",
            "     25  0.4064       0.8801    0.2642        \u001b[31m0.9317\u001b[0m        0.7288        \u001b[36m0.8680\u001b[0m  0.0000  4.5454\n",
            "     26  0.4064       0.8801    0.2642        0.9317        \u001b[94m0.7279\u001b[0m        0.8680  0.0000  3.7583\n",
            "     27  0.4076       0.8795    0.2653        \u001b[31m0.9315\u001b[0m        \u001b[94m0.7278\u001b[0m        \u001b[36m0.8678\u001b[0m  0.0000  3.7266\n",
            "     28  0.4073       0.8796    0.2650        0.9316        0.7289        0.8678  0.0000  4.4309\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 3/3; 10/36] END lr=0.01, module__dropout=0.5, module__linear_size=400, module__size_emb=30;, score=-1.113 total time= 1.9min\n",
            "[CV 1/3; 11/36] START lr=0.01, module__dropout=0.5, module__linear_size=400, module__size_emb=60\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.5040\u001b[0m       \u001b[32m0.8257\u001b[0m    \u001b[35m0.3627\u001b[0m        \u001b[31m0.9871\u001b[0m        \u001b[94m1.0102\u001b[0m        \u001b[36m0.9744\u001b[0m  0.0100  3.6362\n",
            "      2  0.3805       \u001b[32m0.8577\u001b[0m    0.2445        0.9897        \u001b[94m0.9001\u001b[0m        0.9794  0.0100  4.5019\n",
            "      3  0.3563       0.8573    0.2249        0.9915        \u001b[94m0.8818\u001b[0m        0.9830  0.0100  3.4603\n",
            "      4  0.4568       0.8374    0.3141        0.9951        \u001b[94m0.8789\u001b[0m        0.9903  0.0100  3.8711\n",
            "      5  0.2749       0.8559    0.1637        0.9958        \u001b[94m0.8732\u001b[0m        0.9915  0.0100  4.1778\n",
            "      6  0.2294       \u001b[32m0.8703\u001b[0m    0.1321        \u001b[31m0.9826\u001b[0m        \u001b[94m0.8715\u001b[0m        \u001b[36m0.9656\u001b[0m  0.0100  3.7218\n",
            "      7  0.3933       0.8464    0.2561        0.9856        0.8733        0.9714  0.0100  3.6568\n",
            "      8  0.3943       0.8544    0.2563        \u001b[31m0.9780\u001b[0m        \u001b[94m0.8316\u001b[0m        \u001b[36m0.9564\u001b[0m  0.0010  4.1951\n",
            "      9  0.3689       0.8603    0.2348        \u001b[31m0.9764\u001b[0m        \u001b[94m0.8148\u001b[0m        \u001b[36m0.9533\u001b[0m  0.0010  3.9332\n",
            "     10  0.3099       0.8654    0.1887        0.9792        \u001b[94m0.8048\u001b[0m        0.9588  0.0010  3.6535\n",
            "     11  0.3804       0.8618    0.2440        \u001b[31m0.9705\u001b[0m        \u001b[94m0.8015\u001b[0m        \u001b[36m0.9419\u001b[0m  0.0010  3.8267\n",
            "     12  0.3427       0.8664    0.2136        0.9783        \u001b[94m0.7951\u001b[0m        0.9570  0.0010  4.5446\n",
            "     13  0.3260       0.8673    0.2007        0.9774        \u001b[94m0.7930\u001b[0m        0.9552  0.0010  3.6420\n",
            "     14  0.3600       0.8637    0.2274        \u001b[31m0.9668\u001b[0m        \u001b[94m0.7870\u001b[0m        \u001b[36m0.9347\u001b[0m  0.0010  3.6446\n",
            "     15  0.3590       0.8627    0.2267        0.9692        \u001b[94m0.7799\u001b[0m        0.9394  0.0001  4.4129\n",
            "     16  0.3537       0.8655    0.2223        0.9695        \u001b[94m0.7788\u001b[0m        0.9399  0.0001  3.5901\n",
            "     17  0.3545       0.8652    0.2229        0.9694        \u001b[94m0.7750\u001b[0m        0.9398  0.0001  3.6198\n",
            "     18  0.3525       0.8655    0.2213        0.9690        0.7768        0.9390  0.0001  4.7756\n",
            "     19  0.3514       0.8666    0.2204        0.9694        0.7769        0.9397  0.0001  3.7143\n",
            "     20  0.3562       0.8650    0.2243        0.9687        0.7754        0.9384  0.0001  3.6312\n",
            "     21  0.3545       0.8658    0.2229        0.9680        \u001b[94m0.7734\u001b[0m        0.9370  0.0001  4.1069\n",
            "     22  0.3544       0.8657    0.2228        0.9680        \u001b[94m0.7729\u001b[0m        0.9371  0.0000  4.2497\n",
            "     23  0.3531       0.8655    0.2218        0.9682        0.7756        0.9375  0.0000  3.6611\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 1/3; 11/36] END lr=0.01, module__dropout=0.5, module__linear_size=400, module__size_emb=60;, score=-1.191 total time= 1.6min\n",
            "[CV 2/3; 11/36] START lr=0.01, module__dropout=0.5, module__linear_size=400, module__size_emb=60\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.2490\u001b[0m       \u001b[32m0.8856\u001b[0m    \u001b[35m0.1449\u001b[0m        \u001b[31m0.9795\u001b[0m        \u001b[94m1.0379\u001b[0m        \u001b[36m0.9595\u001b[0m  0.0100  3.6072\n",
            "      2  \u001b[36m0.5563\u001b[0m       0.8249    \u001b[35m0.4196\u001b[0m        \u001b[31m0.9492\u001b[0m        \u001b[94m0.9113\u001b[0m        \u001b[36m0.9011\u001b[0m  0.0100  3.7087\n",
            "      3  0.3932       0.8713    0.2539        0.9506        \u001b[94m0.8890\u001b[0m        0.9036  0.0100  4.3934\n",
            "      4  0.5246       0.8446    0.3805        \u001b[31m0.9386\u001b[0m        \u001b[94m0.8738\u001b[0m        \u001b[36m0.8811\u001b[0m  0.0100  3.8309\n",
            "      5  0.5471       0.8406    0.4056        \u001b[31m0.9380\u001b[0m        \u001b[94m0.8679\u001b[0m        \u001b[36m0.8799\u001b[0m  0.0100  3.7583\n",
            "      6  0.2825       \u001b[32m0.9008\u001b[0m    0.1675        0.9509        \u001b[94m0.8633\u001b[0m        0.9042  0.0100  4.1262\n",
            "      7  0.3533       0.8917    0.2203        0.9478        \u001b[94m0.8563\u001b[0m        0.8984  0.0100  4.0591\n",
            "      8  0.3567       0.8934    0.2228        0.9382        \u001b[94m0.8085\u001b[0m        0.8803  0.0010  3.7043\n",
            "      9  0.3643       0.8934    0.2288        \u001b[31m0.9341\u001b[0m        \u001b[94m0.7899\u001b[0m        \u001b[36m0.8725\u001b[0m  0.0010  3.6821\n",
            "     10  0.4035       0.8820    0.2616        \u001b[31m0.9274\u001b[0m        \u001b[94m0.7774\u001b[0m        \u001b[36m0.8601\u001b[0m  0.0010  4.4833\n",
            "     11  0.3877       0.8875    0.2480        \u001b[31m0.9272\u001b[0m        \u001b[94m0.7691\u001b[0m        \u001b[36m0.8598\u001b[0m  0.0010  3.6163\n",
            "     12  0.4283       0.8779    0.2833        \u001b[31m0.9217\u001b[0m        \u001b[94m0.7651\u001b[0m        \u001b[36m0.8495\u001b[0m  0.0010  3.6041\n",
            "     13  0.4023       0.8881    0.2601        0.9219        \u001b[94m0.7610\u001b[0m        0.8499  0.0010  4.4867\n",
            "     14  0.3475       0.8988    0.2154        0.9277        \u001b[94m0.7552\u001b[0m        0.8607  0.0010  3.6772\n",
            "     15  0.3947       0.8889    0.2536        \u001b[31m0.9203\u001b[0m        \u001b[94m0.7460\u001b[0m        \u001b[36m0.8470\u001b[0m  0.0001  3.7045\n",
            "     16  0.3929       0.8897    0.2521        \u001b[31m0.9202\u001b[0m        \u001b[94m0.7439\u001b[0m        \u001b[36m0.8467\u001b[0m  0.0001  4.4715\n",
            "     17  0.3889       0.8910    0.2487        0.9204        \u001b[94m0.7434\u001b[0m        0.8471  0.0001  3.7518\n",
            "     18  0.3819       0.8919    0.2430        0.9203        \u001b[94m0.7427\u001b[0m        0.8469  0.0001  3.6484\n",
            "     19  0.3971       0.8878    0.2558        \u001b[31m0.9191\u001b[0m        \u001b[94m0.7415\u001b[0m        \u001b[36m0.8448\u001b[0m  0.0001  4.1383\n",
            "     20  0.3784       0.8931    0.2400        0.9203        \u001b[94m0.7396\u001b[0m        0.8470  0.0001  4.0560\n",
            "     21  0.3928       0.8911    0.2519        \u001b[31m0.9188\u001b[0m        0.7416        \u001b[36m0.8443\u001b[0m  0.0001  3.4985\n",
            "     22  0.3910       0.8916    0.2504        0.9190        0.7411        0.8446  0.0000  3.7634\n",
            "     23  0.3904       0.8914    0.2500        0.9190        \u001b[94m0.7394\u001b[0m        0.8446  0.0000  4.4445\n",
            "     24  0.3894       0.8917    0.2491        0.9192        \u001b[94m0.7386\u001b[0m        0.8449  0.0000  3.7112\n",
            "     25  0.3893       0.8922    0.2490        0.9192        0.7392        0.8449  0.0000  3.7191\n",
            "     26  0.3900       0.8913    0.2496        0.9191        \u001b[94m0.7373\u001b[0m        0.8448  0.0000  4.5686\n",
            "     27  0.3893       0.8919    0.2490        0.9192        0.7389        0.8449  0.0000  3.6865\n",
            "     28  0.3887       0.8926    0.2484        0.9192        0.7407        0.8450  0.0000  3.6972\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 2/3; 11/36] END lr=0.01, module__dropout=0.5, module__linear_size=400, module__size_emb=60;, score=-1.025 total time= 2.0min\n",
            "[CV 3/3; 11/36] START lr=0.01, module__dropout=0.5, module__linear_size=400, module__size_emb=60\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4142\u001b[0m       \u001b[32m0.8464\u001b[0m    \u001b[35m0.2742\u001b[0m        \u001b[31m0.9681\u001b[0m        \u001b[94m1.0255\u001b[0m        \u001b[36m0.9373\u001b[0m  0.0100  3.8151\n",
            "      2  \u001b[36m0.4868\u001b[0m       0.8424    \u001b[35m0.3423\u001b[0m        \u001b[31m0.9581\u001b[0m        \u001b[94m0.8871\u001b[0m        \u001b[36m0.9179\u001b[0m  0.0100  4.0080\n",
            "      3  0.2425       \u001b[32m0.8815\u001b[0m    0.1406        0.9771        \u001b[94m0.8727\u001b[0m        0.9547  0.0100  4.4978\n",
            "      4  0.3132       0.8741    0.1908        0.9628        \u001b[94m0.8594\u001b[0m        0.9270  0.0100  3.7608\n",
            "      5  0.4797       0.8440    0.3351        \u001b[31m0.9570\u001b[0m        \u001b[94m0.8570\u001b[0m        \u001b[36m0.9159\u001b[0m  0.0100  3.7849\n",
            "      6  0.4406       0.8561    0.2966        \u001b[31m0.9531\u001b[0m        \u001b[94m0.8495\u001b[0m        \u001b[36m0.9083\u001b[0m  0.0100  4.6365\n",
            "      7  0.3897       0.8685    0.2512        \u001b[31m0.9471\u001b[0m        \u001b[94m0.8393\u001b[0m        \u001b[36m0.8970\u001b[0m  0.0100  3.6719\n",
            "      8  0.3667       0.8796    0.2316        \u001b[31m0.9452\u001b[0m        \u001b[94m0.7898\u001b[0m        \u001b[36m0.8933\u001b[0m  0.0010  3.6623\n",
            "      9  0.3921       0.8764    0.2526        \u001b[31m0.9403\u001b[0m        \u001b[94m0.7726\u001b[0m        \u001b[36m0.8842\u001b[0m  0.0010  4.4619\n",
            "     10  0.3675       0.8805    0.2322        0.9423        \u001b[94m0.7630\u001b[0m        0.8879  0.0010  3.3910\n",
            "     11  0.3661       \u001b[32m0.8859\u001b[0m    0.2307        \u001b[31m0.9402\u001b[0m        \u001b[94m0.7573\u001b[0m        \u001b[36m0.8839\u001b[0m  0.0010  3.6647\n",
            "     12  0.3859       0.8850    0.2467        \u001b[31m0.9347\u001b[0m        \u001b[94m0.7502\u001b[0m        \u001b[36m0.8736\u001b[0m  0.0010  4.4090\n",
            "     13  0.3745       \u001b[32m0.8875\u001b[0m    0.2373        \u001b[31m0.9346\u001b[0m        \u001b[94m0.7420\u001b[0m        \u001b[36m0.8734\u001b[0m  0.0010  3.7482\n",
            "     14  0.4114       0.8802    0.2684        \u001b[31m0.9300\u001b[0m        \u001b[94m0.7410\u001b[0m        \u001b[36m0.8649\u001b[0m  0.0010  3.5628\n",
            "     15  0.3974       0.8826    0.2564        0.9305        \u001b[94m0.7324\u001b[0m        0.8658  0.0001  3.8717\n",
            "     16  0.3947       0.8828    0.2542        0.9308        0.7340        0.8663  0.0001  4.1251\n",
            "     17  0.3975       0.8829    0.2565        0.9303        \u001b[94m0.7309\u001b[0m        0.8654  0.0001  3.6976\n",
            "     18  0.3908       0.8836    0.2509        0.9309        0.7326        0.8665  0.0001  3.7553\n",
            "     19  0.3939       0.8836    0.2534        0.9304        \u001b[94m0.7296\u001b[0m        0.8657  0.0001  4.4919\n",
            "     20  0.3911       0.8837    0.2511        0.9308        0.7297        0.8664  0.0001  3.6141\n",
            "     21  0.3931       0.8838    0.2527        0.9303        \u001b[94m0.7284\u001b[0m        0.8654  0.0001  3.6034\n",
            "     22  0.3908       0.8842    0.2509        0.9305        \u001b[94m0.7257\u001b[0m        0.8658  0.0000  4.4623\n",
            "     23  0.3900       0.8839    0.2502        0.9304        0.7268        0.8657  0.0000  3.7222\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 3/3; 11/36] END lr=0.01, module__dropout=0.5, module__linear_size=400, module__size_emb=60;, score=-1.111 total time= 1.6min\n",
            "[CV 1/3; 12/36] START lr=0.01, module__dropout=0.5, module__linear_size=400, module__size_emb=120\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.5866\u001b[0m       \u001b[32m0.7955\u001b[0m    \u001b[35m0.4646\u001b[0m        \u001b[31m1.0024\u001b[0m        \u001b[94m1.0074\u001b[0m        \u001b[36m1.0048\u001b[0m  0.0100  4.5899\n",
            "      2  0.2607       \u001b[32m0.8765\u001b[0m    0.1531        \u001b[31m1.0015\u001b[0m        \u001b[94m0.9066\u001b[0m        \u001b[36m1.0030\u001b[0m  0.0100  3.6858\n",
            "      3  0.3534       0.8598    0.2224        \u001b[31m0.9880\u001b[0m        \u001b[94m0.8955\u001b[0m        \u001b[36m0.9762\u001b[0m  0.0100  3.6707\n",
            "      4  0.3671       0.8525    0.2339        0.9889        \u001b[94m0.8924\u001b[0m        0.9779  0.0100  4.3518\n",
            "      5  0.5225       0.8265    0.3820        0.9908        \u001b[94m0.8920\u001b[0m        0.9816  0.0100  3.7918\n",
            "      6  0.3490       0.8505    0.2195        \u001b[31m0.9807\u001b[0m        \u001b[94m0.8876\u001b[0m        \u001b[36m0.9618\u001b[0m  0.0100  3.6893\n",
            "      7  0.4573       0.8363    0.3147        \u001b[31m0.9788\u001b[0m        \u001b[94m0.8818\u001b[0m        \u001b[36m0.9580\u001b[0m  0.0100  4.5999\n",
            "      8  0.3890       0.8439    0.2527        \u001b[31m0.9783\u001b[0m        \u001b[94m0.8376\u001b[0m        \u001b[36m0.9571\u001b[0m  0.0010  3.7885\n",
            "      9  0.3862       0.8499    0.2499        \u001b[31m0.9781\u001b[0m        \u001b[94m0.8215\u001b[0m        \u001b[36m0.9566\u001b[0m  0.0010  3.8627\n",
            "     10  0.4160       0.8495    0.2755        \u001b[31m0.9734\u001b[0m        \u001b[94m0.8139\u001b[0m        \u001b[36m0.9475\u001b[0m  0.0010  4.3763\n",
            "     11  0.3762       0.8581    0.2409        \u001b[31m0.9717\u001b[0m        \u001b[94m0.8086\u001b[0m        \u001b[36m0.9443\u001b[0m  0.0010  3.7939\n",
            "     12  0.3773       0.8593    0.2417        0.9725        \u001b[94m0.8028\u001b[0m        0.9457  0.0010  3.7173\n",
            "     13  0.4095       0.8522    0.2695        \u001b[31m0.9689\u001b[0m        \u001b[94m0.7983\u001b[0m        \u001b[36m0.9387\u001b[0m  0.0010  4.0781\n",
            "     14  0.3749       0.8608    0.2397        \u001b[31m0.9687\u001b[0m        \u001b[94m0.7924\u001b[0m        \u001b[36m0.9383\u001b[0m  0.0010  4.1278\n",
            "     15  0.3845       0.8585    0.2477        \u001b[31m0.9674\u001b[0m        \u001b[94m0.7854\u001b[0m        \u001b[36m0.9358\u001b[0m  0.0001  3.6604\n",
            "     16  0.3904       0.8575    0.2527        \u001b[31m0.9669\u001b[0m        \u001b[94m0.7831\u001b[0m        \u001b[36m0.9348\u001b[0m  0.0001  3.6708\n",
            "     17  0.3659       0.8593    0.2324        0.9690        0.7844        0.9389  0.0001  4.4580\n",
            "     18  0.3771       0.8582    0.2416        0.9673        0.7832        0.9358  0.0001  3.6851\n",
            "     19  0.3988       0.8563    0.2599        \u001b[31m0.9661\u001b[0m        \u001b[94m0.7799\u001b[0m        \u001b[36m0.9334\u001b[0m  0.0001  3.6790\n",
            "     20  0.3826       0.8588    0.2461        0.9673        0.7819        0.9357  0.0001  4.6251\n",
            "     21  0.3935       0.8592    0.2551        \u001b[31m0.9657\u001b[0m        0.7803        \u001b[36m0.9327\u001b[0m  0.0001  3.6306\n",
            "     22  0.3914       0.8587    0.2534        0.9660        0.7809        0.9331  0.0000  3.6483\n",
            "     23  0.3893       0.8584    0.2517        0.9661        \u001b[94m0.7791\u001b[0m        0.9333  0.0000  4.2500\n",
            "     24  0.3883       0.8580    0.2509        0.9662        0.7799        0.9336  0.0000  3.7759\n",
            "     25  0.3879       0.8578    0.2506        0.9662        0.7809        0.9336  0.0000  3.6804\n",
            "     26  0.3879       0.8581    0.2506        0.9662        0.7793        0.9335  0.0000  4.1786\n",
            "     27  0.3876       0.8579    0.2503        0.9662        0.7803        0.9335  0.0000  3.9245\n",
            "     28  0.3869       0.8587    0.2497        0.9663        0.7792        0.9337  0.0000  3.6396\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 1/3; 12/36] END lr=0.01, module__dropout=0.5, module__linear_size=400, module__size_emb=120;, score=-1.191 total time= 2.0min\n",
            "[CV 2/3; 12/36] START lr=0.01, module__dropout=0.5, module__linear_size=400, module__size_emb=120\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.5136\u001b[0m       \u001b[32m0.8368\u001b[0m    \u001b[35m0.3705\u001b[0m        \u001b[31m0.9569\u001b[0m        \u001b[94m1.0309\u001b[0m        \u001b[36m0.9157\u001b[0m  0.0100  3.6709\n",
            "      2  0.2987       \u001b[32m0.8853\u001b[0m    0.1797        0.9709        \u001b[94m0.9147\u001b[0m        0.9427  0.0100  3.6832\n",
            "      3  \u001b[36m0.6063\u001b[0m       0.8080    \u001b[35m0.4851\u001b[0m        0.9663        \u001b[94m0.8949\u001b[0m        0.9338  0.0100  4.1742\n",
            "      4  0.4562       0.8638    0.3099        \u001b[31m0.9452\u001b[0m        \u001b[94m0.8941\u001b[0m        \u001b[36m0.8935\u001b[0m  0.0100  3.8767\n",
            "      5  0.3856       0.8733    0.2474        0.9522        \u001b[94m0.8924\u001b[0m        0.9066  0.0100  3.7398\n",
            "      6  0.3723       0.8839    0.2358        0.9490        0.8931        0.9006  0.0100  3.9412\n",
            "      7  0.3212       0.8849    0.1962        0.9544        \u001b[94m0.8752\u001b[0m        0.9110  0.0100  4.2516\n",
            "      8  0.3760       0.8800    0.2390        \u001b[31m0.9421\u001b[0m        \u001b[94m0.8290\u001b[0m        \u001b[36m0.8875\u001b[0m  0.0010  3.6690\n",
            "      9  0.3936       0.8803    0.2534        \u001b[31m0.9381\u001b[0m        \u001b[94m0.8118\u001b[0m        \u001b[36m0.8801\u001b[0m  0.0010  3.6603\n",
            "     10  0.3993       0.8803    0.2582        \u001b[31m0.9342\u001b[0m        \u001b[94m0.7999\u001b[0m        \u001b[36m0.8728\u001b[0m  0.0010  4.7088\n",
            "     11  0.4138       0.8771    0.2708        \u001b[31m0.9309\u001b[0m        \u001b[94m0.7956\u001b[0m        \u001b[36m0.8665\u001b[0m  0.0010  3.3893\n",
            "     12  0.3798       \u001b[32m0.8861\u001b[0m    0.2417        0.9334        \u001b[94m0.7888\u001b[0m        0.8713  0.0010  3.6924\n",
            "     13  0.4231       0.8793    0.2786        \u001b[31m0.9279\u001b[0m        \u001b[94m0.7825\u001b[0m        \u001b[36m0.8609\u001b[0m  0.0010  4.5152\n",
            "     14  0.3956       \u001b[32m0.8879\u001b[0m    0.2545        0.9295        \u001b[94m0.7809\u001b[0m        0.8640  0.0010  3.6655\n",
            "     15  0.3955       \u001b[32m0.8884\u001b[0m    0.2543        0.9287        \u001b[94m0.7684\u001b[0m        0.8624  0.0001  3.6862\n",
            "     16  0.4044       0.8867    0.2620        0.9280        0.7706        0.8612  0.0001  4.2074\n",
            "     17  0.4076       0.8864    0.2646        \u001b[31m0.9275\u001b[0m        0.7686        \u001b[36m0.8603\u001b[0m  0.0001  3.7991\n",
            "     18  0.4037       0.8867    0.2613        0.9279        \u001b[94m0.7684\u001b[0m        0.8611  0.0001  3.7433\n",
            "     19  0.3979       0.8873    0.2565        0.9282        \u001b[94m0.7672\u001b[0m        0.8615  0.0001  4.0083\n",
            "     20  0.4041       0.8876    0.2616        \u001b[31m0.9272\u001b[0m        \u001b[94m0.7658\u001b[0m        \u001b[36m0.8598\u001b[0m  0.0001  4.2524\n",
            "     21  0.4007       0.8879    0.2587        0.9275        0.7677        0.8603  0.0001  3.6978\n",
            "     22  0.4002       0.8877    0.2584        0.9275        \u001b[94m0.7650\u001b[0m        0.8603  0.0000  3.6077\n",
            "     23  0.4001       0.8877    0.2583        0.9275        0.7659        0.8603  0.0000  4.5273\n",
            "     24  0.4009       0.8879    0.2589        0.9275        0.7660        0.8602  0.0000  3.6186\n",
            "     25  0.3988       0.8878    0.2571        0.9276        0.7653        0.8605  0.0000  3.7209\n",
            "     26  0.3976       0.8877    0.2561        0.9278        \u001b[94m0.7649\u001b[0m        0.8608  0.0000  4.5886\n",
            "     27  0.3972       0.8879    0.2559        0.9277        \u001b[94m0.7645\u001b[0m        0.8607  0.0000  3.6845\n",
            "     28  0.3972       0.8879    0.2559        0.9277        0.7647        0.8607  0.0000  3.7402\n",
            "     29  0.3974       0.8879    0.2560        0.9277        \u001b[94m0.7643\u001b[0m        0.8607  0.0000  4.5370\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 2/3; 12/36] END lr=0.01, module__dropout=0.5, module__linear_size=400, module__size_emb=120;, score=-1.019 total time= 2.0min\n",
            "[CV 3/3; 12/36] START lr=0.01, module__dropout=0.5, module__linear_size=400, module__size_emb=120\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.3651\u001b[0m       \u001b[32m0.8618\u001b[0m    \u001b[35m0.2316\u001b[0m        \u001b[31m0.9710\u001b[0m        \u001b[94m1.0063\u001b[0m        \u001b[36m0.9428\u001b[0m  0.0100  3.7121\n",
            "      2  0.3245       \u001b[32m0.8831\u001b[0m    0.1987        \u001b[31m0.9645\u001b[0m        \u001b[94m0.8898\u001b[0m        \u001b[36m0.9303\u001b[0m  0.0100  4.6087\n",
            "      3  \u001b[36m0.4039\u001b[0m       0.8612    \u001b[35m0.2638\u001b[0m        \u001b[31m0.9580\u001b[0m        \u001b[94m0.8774\u001b[0m        \u001b[36m0.9179\u001b[0m  0.0100  3.6279\n",
            "      4  0.2843       0.8797    0.1696        0.9691        \u001b[94m0.8696\u001b[0m        0.9391  0.0100  3.5454\n",
            "      5  0.3815       0.8670    0.2446        0.9665        \u001b[94m0.8664\u001b[0m        0.9341  0.0100  4.5164\n",
            "      6  0.3182       0.8785    0.1943        0.9732        0.8676        0.9471  0.0100  3.6712\n",
            "      7  \u001b[36m0.4243\u001b[0m       0.8578    \u001b[35m0.2818\u001b[0m        \u001b[31m0.9518\u001b[0m        \u001b[94m0.8627\u001b[0m        \u001b[36m0.9059\u001b[0m  0.0100  3.5922\n",
            "      8  0.3865       0.8681    0.2486        0.9519        \u001b[94m0.8206\u001b[0m        0.9061  0.0010  3.9984\n",
            "      9  0.4227       0.8656    0.2796        \u001b[31m0.9473\u001b[0m        \u001b[94m0.8010\u001b[0m        \u001b[36m0.8974\u001b[0m  0.0010  4.0011\n",
            "     10  0.3841       0.8754    0.2460        0.9481        \u001b[94m0.7903\u001b[0m        0.8989  0.0010  3.6706\n",
            "     11  0.3876       0.8772    0.2488        \u001b[31m0.9465\u001b[0m        \u001b[94m0.7831\u001b[0m        \u001b[36m0.8959\u001b[0m  0.0010  3.5984\n",
            "     12  0.4115       0.8748    0.2690        \u001b[31m0.9443\u001b[0m        \u001b[94m0.7769\u001b[0m        \u001b[36m0.8918\u001b[0m  0.0010  4.4930\n",
            "     13  0.3914       0.8812    0.2516        0.9449        \u001b[94m0.7702\u001b[0m        0.8927  0.0010  3.6235\n",
            "     14  0.3694       \u001b[32m0.8844\u001b[0m    0.2335        0.9457        \u001b[94m0.7667\u001b[0m        0.8944  0.0010  3.5824\n",
            "     15  0.3945       0.8774    0.2544        \u001b[31m0.9418\u001b[0m        \u001b[94m0.7614\u001b[0m        \u001b[36m0.8870\u001b[0m  0.0001  4.4976\n",
            "     16  0.3903       0.8792    0.2509        0.9419        \u001b[94m0.7568\u001b[0m        0.8872  0.0001  3.5936\n",
            "     17  0.3861       0.8786    0.2474        0.9419        0.7581        0.8872  0.0001  3.5707\n",
            "     18  0.3996       0.8776    0.2587        \u001b[31m0.9404\u001b[0m        \u001b[94m0.7561\u001b[0m        \u001b[36m0.8844\u001b[0m  0.0001  4.0999\n",
            "     19  0.3962       0.8785    0.2558        0.9406        \u001b[94m0.7553\u001b[0m        0.8847  0.0001  3.7462\n",
            "     20  0.3958       0.8784    0.2554        0.9406        \u001b[94m0.7551\u001b[0m        0.8847  0.0001  3.5887\n",
            "     21  0.4009       0.8788    0.2597        \u001b[31m0.9399\u001b[0m        0.7570        \u001b[36m0.8834\u001b[0m  0.0001  3.8233\n",
            "     22  0.3987       0.8786    0.2578        0.9401        \u001b[94m0.7547\u001b[0m        0.8837  0.0000  4.2759\n",
            "     23  0.3955       0.8794    0.2551        0.9404        \u001b[94m0.7545\u001b[0m        0.8843  0.0000  3.6291\n",
            "     24  0.3954       0.8796    0.2551        0.9404        \u001b[94m0.7532\u001b[0m        0.8843  0.0000  3.6573\n",
            "     25  0.3939       0.8801    0.2537        0.9405        0.7541        0.8845  0.0000  4.5648\n",
            "     26  0.3939       0.8801    0.2537        0.9405        0.7532        0.8845  0.0000  3.7445\n",
            "     27  0.3941       0.8802    0.2539        0.9405        0.7551        0.8845  0.0000  3.8440\n",
            "     28  0.3942       0.8802    0.2540        0.9405        0.7542        0.8846  0.0000  4.5691\n",
            "     29  0.3940       0.8804    0.2538        0.9405        0.7549        0.8846  0.0000  3.5788\n",
            "     30  0.3940       0.8804    0.2538        0.9405        0.7533        0.8846  0.0000  3.6751\n",
            "[CV 3/3; 12/36] END lr=0.01, module__dropout=0.5, module__linear_size=400, module__size_emb=120;, score=-1.114 total time= 2.0min\n",
            "[CV 1/3; 13/36] START lr=0.01, module__dropout=0.5, module__linear_size=500, module__size_emb=30\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4608\u001b[0m       \u001b[32m0.8330\u001b[0m    \u001b[35m0.3184\u001b[0m        \u001b[31m0.9979\u001b[0m        \u001b[94m1.0464\u001b[0m        \u001b[36m0.9959\u001b[0m  0.0100  4.4485\n",
            "      2  0.1788       \u001b[32m0.8853\u001b[0m    0.0995        1.0011        \u001b[94m0.9005\u001b[0m        1.0023  0.0100  3.6201\n",
            "      3  0.3570       0.8636    0.2250        \u001b[31m0.9835\u001b[0m        \u001b[94m0.8804\u001b[0m        \u001b[36m0.9674\u001b[0m  0.0100  3.6479\n",
            "      4  0.4344       0.8278    0.2944        \u001b[31m0.9758\u001b[0m        \u001b[94m0.8724\u001b[0m        \u001b[36m0.9521\u001b[0m  0.0100  4.5022\n",
            "      5  0.2682       0.8770    0.1583        0.9974        \u001b[94m0.8719\u001b[0m        0.9949  0.0100  3.4085\n",
            "      6  0.2082       0.8499    0.1186        0.9950        \u001b[94m0.8642\u001b[0m        0.9899  0.0100  3.6886\n",
            "      7  0.4300       0.8385    0.2892        \u001b[31m0.9697\u001b[0m        \u001b[94m0.8606\u001b[0m        \u001b[36m0.9403\u001b[0m  0.0100  4.2969\n",
            "      8  0.3402       0.8518    0.2125        0.9700        \u001b[94m0.8185\u001b[0m        0.9409  0.0010  3.7566\n",
            "      9  0.3085       0.8592    0.1880        0.9700        \u001b[94m0.8042\u001b[0m        0.9408  0.0010  3.6072\n",
            "     10  0.3133       0.8538    0.1919        \u001b[31m0.9678\u001b[0m        \u001b[94m0.7933\u001b[0m        \u001b[36m0.9366\u001b[0m  0.0010  3.9322\n",
            "     11  0.3235       0.8518    0.1996        \u001b[31m0.9655\u001b[0m        \u001b[94m0.7881\u001b[0m        \u001b[36m0.9322\u001b[0m  0.0010  4.3482\n",
            "     12  0.3652       0.8501    0.2326        \u001b[31m0.9604\u001b[0m        \u001b[94m0.7822\u001b[0m        \u001b[36m0.9225\u001b[0m  0.0010  3.6720\n",
            "     13  0.3704       0.8505    0.2368        \u001b[31m0.9603\u001b[0m        \u001b[94m0.7790\u001b[0m        \u001b[36m0.9222\u001b[0m  0.0010  3.5267\n",
            "     14  0.3827       0.8457    0.2473        \u001b[31m0.9590\u001b[0m        \u001b[94m0.7740\u001b[0m        \u001b[36m0.9198\u001b[0m  0.0010  4.4616\n",
            "     15  0.3694       0.8474    0.2362        \u001b[31m0.9580\u001b[0m        \u001b[94m0.7665\u001b[0m        \u001b[36m0.9177\u001b[0m  0.0001  3.6322\n",
            "     16  0.3709       0.8470    0.2374        \u001b[31m0.9577\u001b[0m        \u001b[94m0.7643\u001b[0m        \u001b[36m0.9173\u001b[0m  0.0001  3.6361\n",
            "     17  0.3727       0.8491    0.2388        \u001b[31m0.9573\u001b[0m        \u001b[94m0.7634\u001b[0m        \u001b[36m0.9165\u001b[0m  0.0001  4.5620\n",
            "     18  0.3721       0.8473    0.2384        \u001b[31m0.9573\u001b[0m        0.7643        \u001b[36m0.9163\u001b[0m  0.0001  3.6994\n",
            "     19  0.3790       0.8474    0.2440        \u001b[31m0.9565\u001b[0m        \u001b[94m0.7625\u001b[0m        \u001b[36m0.9149\u001b[0m  0.0001  3.6258\n",
            "     20  0.3775       0.8457    0.2430        \u001b[31m0.9564\u001b[0m        0.7637        \u001b[36m0.9147\u001b[0m  0.0001  4.3094\n",
            "     21  0.3863       0.8457    0.2503        \u001b[31m0.9561\u001b[0m        \u001b[94m0.7616\u001b[0m        \u001b[36m0.9141\u001b[0m  0.0001  3.8444\n",
            "     22  0.3859       0.8453    0.2500        0.9561        \u001b[94m0.7609\u001b[0m        0.9142  0.0000  3.3790\n",
            "     23  0.3842       0.8461    0.2485        0.9562        0.7612        0.9144  0.0000  3.8354\n",
            "     24  0.3847       0.8458    0.2490        0.9562        0.7610        0.9142  0.0000  4.1820\n",
            "     25  0.3839       0.8462    0.2483        0.9562        0.7614        0.9143  0.0000  3.6120\n",
            "     26  0.3846       0.8460    0.2489        0.9561        0.7610        0.9142  0.0000  3.6625\n",
            "     27  0.3846       0.8460    0.2489        \u001b[31m0.9561\u001b[0m        0.7612        \u001b[36m0.9141\u001b[0m  0.0000  4.5639\n",
            "     28  0.3850       0.8462    0.2491        \u001b[31m0.9560\u001b[0m        \u001b[94m0.7599\u001b[0m        \u001b[36m0.9140\u001b[0m  0.0000  3.6647\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 1/3; 13/36] END lr=0.01, module__dropout=0.5, module__linear_size=500, module__size_emb=30;, score=-1.188 total time= 1.9min\n",
            "[CV 2/3; 13/36] START lr=0.01, module__dropout=0.5, module__linear_size=500, module__size_emb=30\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.3117\u001b[0m       \u001b[32m0.8597\u001b[0m    \u001b[35m0.1903\u001b[0m        \u001b[31m0.9696\u001b[0m        \u001b[94m1.0867\u001b[0m        \u001b[36m0.9401\u001b[0m  0.0100  4.2099\n",
            "      2  0.1841       \u001b[32m0.8885\u001b[0m    0.1027        0.9781        \u001b[94m0.9087\u001b[0m        0.9567  0.0100  3.5785\n",
            "      3  0.2406       0.8653    0.1397        \u001b[31m0.9637\u001b[0m        \u001b[94m0.8880\u001b[0m        \u001b[36m0.9287\u001b[0m  0.0100  3.7264\n",
            "      4  \u001b[36m0.4630\u001b[0m       0.8335    \u001b[35m0.3205\u001b[0m        \u001b[31m0.9467\u001b[0m        \u001b[94m0.8731\u001b[0m        \u001b[36m0.8963\u001b[0m  0.0100  4.5493\n",
            "      5  0.2552       0.8563    0.1500        0.9636        \u001b[94m0.8667\u001b[0m        0.9286  0.0100  3.5737\n",
            "      6  0.2388       \u001b[32m0.8907\u001b[0m    0.1379        0.9615        \u001b[94m0.8654\u001b[0m        0.9245  0.0100  3.5474\n",
            "      7  0.4442       0.8547    0.3001        \u001b[31m0.9414\u001b[0m        \u001b[94m0.8596\u001b[0m        \u001b[36m0.8863\u001b[0m  0.0100  4.4763\n",
            "      8  0.3963       0.8670    0.2568        \u001b[31m0.9346\u001b[0m        \u001b[94m0.8149\u001b[0m        \u001b[36m0.8735\u001b[0m  0.0010  3.6423\n",
            "      9  0.3747       0.8712    0.2387        0.9359        \u001b[94m0.7936\u001b[0m        0.8759  0.0010  3.6532\n",
            "     10  0.3836       0.8662    0.2464        \u001b[31m0.9327\u001b[0m        \u001b[94m0.7855\u001b[0m        \u001b[36m0.8700\u001b[0m  0.0010  4.0789\n",
            "     11  0.3025       0.8838    0.1825        0.9374        \u001b[94m0.7802\u001b[0m        0.8787  0.0010  3.9080\n",
            "     12  0.4061       0.8732    0.2645        \u001b[31m0.9273\u001b[0m        \u001b[94m0.7749\u001b[0m        \u001b[36m0.8599\u001b[0m  0.0010  3.6110\n",
            "     13  0.3858       0.8755    0.2474        \u001b[31m0.9270\u001b[0m        \u001b[94m0.7710\u001b[0m        \u001b[36m0.8594\u001b[0m  0.0010  3.7109\n",
            "     14  0.3738       0.8742    0.2377        0.9273        \u001b[94m0.7676\u001b[0m        0.8599  0.0010  4.2004\n",
            "     15  0.3840       0.8733    0.2461        \u001b[31m0.9263\u001b[0m        \u001b[94m0.7561\u001b[0m        \u001b[36m0.8581\u001b[0m  0.0001  3.6047\n",
            "     16  0.3904       0.8734    0.2514        \u001b[31m0.9255\u001b[0m        0.7566        \u001b[36m0.8566\u001b[0m  0.0001  3.5847\n",
            "     17  0.3866       0.8739    0.2482        \u001b[31m0.9254\u001b[0m        0.7567        \u001b[36m0.8564\u001b[0m  0.0001  4.4738\n",
            "     18  0.3800       0.8748    0.2427        0.9257        \u001b[94m0.7545\u001b[0m        0.8569  0.0001  3.6707\n",
            "     19  0.3865       0.8744    0.2481        \u001b[31m0.9250\u001b[0m        \u001b[94m0.7539\u001b[0m        \u001b[36m0.8556\u001b[0m  0.0001  3.6868\n",
            "     20  0.3950       0.8729    0.2552        \u001b[31m0.9243\u001b[0m        \u001b[94m0.7534\u001b[0m        \u001b[36m0.8544\u001b[0m  0.0001  4.4837\n",
            "     21  0.3918       0.8744    0.2525        0.9244        \u001b[94m0.7509\u001b[0m        0.8546  0.0001  3.5424\n",
            "     22  0.3920       0.8740    0.2526        0.9244        0.7533        0.8545  0.0000  3.5670\n",
            "     23  0.3915       0.8743    0.2522        0.9244        \u001b[94m0.7507\u001b[0m        0.8545  0.0000  3.9553\n",
            "     24  0.3906       0.8743    0.2515        0.9244        0.7527        0.8546  0.0000  4.1784\n",
            "     25  0.3918       0.8744    0.2525        0.9244        0.7531        0.8545  0.0000  3.7036\n",
            "     26  0.3914       0.8740    0.2522        0.9244        0.7522        0.8545  0.0000  3.5946\n",
            "     27  0.3907       0.8743    0.2516        0.9244        0.7528        0.8546  0.0000  4.4343\n",
            "     28  0.3900       0.8748    0.2509        0.9244        0.7534        0.8546  0.0000  3.6245\n",
            "     29  0.3901       0.8749    0.2510        0.9244        0.7527        0.8546  0.0000  3.6419\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 2/3; 13/36] END lr=0.01, module__dropout=0.5, module__linear_size=500, module__size_emb=30;, score=-1.023 total time= 2.0min\n",
            "[CV 3/3; 13/36] START lr=0.01, module__dropout=0.5, module__linear_size=500, module__size_emb=30\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.2755\u001b[0m       \u001b[32m0.8473\u001b[0m    \u001b[35m0.1645\u001b[0m        \u001b[31m0.9852\u001b[0m        \u001b[94m1.0971\u001b[0m        \u001b[36m0.9707\u001b[0m  0.0100  3.5432\n",
            "      2  \u001b[36m0.4197\u001b[0m       \u001b[32m0.8485\u001b[0m    \u001b[35m0.2788\u001b[0m        \u001b[31m0.9628\u001b[0m        \u001b[94m0.9025\u001b[0m        \u001b[36m0.9270\u001b[0m  0.0100  3.5676\n",
            "      3  0.3857       \u001b[32m0.8634\u001b[0m    0.2483        \u001b[31m0.9582\u001b[0m        \u001b[94m0.8727\u001b[0m        \u001b[36m0.9182\u001b[0m  0.0100  4.6459\n",
            "      4  \u001b[36m0.4736\u001b[0m       0.8360    \u001b[35m0.3303\u001b[0m        \u001b[31m0.9532\u001b[0m        \u001b[94m0.8654\u001b[0m        \u001b[36m0.9087\u001b[0m  0.0100  3.6890\n",
            "      5  0.4527       0.8441    0.3093        \u001b[31m0.9517\u001b[0m        \u001b[94m0.8554\u001b[0m        \u001b[36m0.9057\u001b[0m  0.0100  3.6785\n",
            "      6  0.4713       0.8412    0.3273        \u001b[31m0.9479\u001b[0m        \u001b[94m0.8482\u001b[0m        \u001b[36m0.8985\u001b[0m  0.0100  4.6502\n",
            "      7  \u001b[36m0.4762\u001b[0m       0.8423    \u001b[35m0.3320\u001b[0m        0.9531        \u001b[94m0.8452\u001b[0m        0.9084  0.0100  3.8135\n",
            "      8  0.4289       0.8525    0.2866        \u001b[31m0.9449\u001b[0m        \u001b[94m0.8014\u001b[0m        \u001b[36m0.8928\u001b[0m  0.0010  3.7098\n",
            "      9  0.4391       0.8495    0.2961        \u001b[31m0.9418\u001b[0m        \u001b[94m0.7911\u001b[0m        \u001b[36m0.8870\u001b[0m  0.0010  4.5045\n",
            "     10  0.4457       0.8482    0.3022        \u001b[31m0.9396\u001b[0m        \u001b[94m0.7817\u001b[0m        \u001b[36m0.8829\u001b[0m  0.0010  3.7851\n",
            "     11  0.4032       0.8588    0.2635        0.9407        \u001b[94m0.7750\u001b[0m        0.8849  0.0010  3.6851\n",
            "     12  0.4088       0.8620    0.2679        \u001b[31m0.9383\u001b[0m        \u001b[94m0.7723\u001b[0m        \u001b[36m0.8804\u001b[0m  0.0010  4.3082\n",
            "     13  0.4216       0.8566    0.2796        0.9389        \u001b[94m0.7665\u001b[0m        0.8815  0.0010  3.9908\n",
            "     14  0.4145       0.8587    0.2731        \u001b[31m0.9359\u001b[0m        \u001b[94m0.7601\u001b[0m        \u001b[36m0.8759\u001b[0m  0.0010  3.7542\n",
            "     15  0.4463       0.8530    0.3022        \u001b[31m0.9342\u001b[0m        \u001b[94m0.7567\u001b[0m        \u001b[36m0.8727\u001b[0m  0.0001  4.1405\n",
            "     16  0.4389       0.8553    0.2952        0.9346        0.7568        0.8734  0.0001  4.2142\n",
            "     17  0.4371       0.8557    0.2936        0.9346        \u001b[94m0.7540\u001b[0m        0.8734  0.0001  3.7460\n",
            "     18  0.4333       0.8551    0.2902        0.9346        0.7551        0.8736  0.0001  3.5039\n",
            "     19  0.4450       0.8541    0.3009        \u001b[31m0.9339\u001b[0m        \u001b[94m0.7528\u001b[0m        \u001b[36m0.8721\u001b[0m  0.0001  4.4680\n",
            "     20  0.4471       0.8536    0.3029        \u001b[31m0.9336\u001b[0m        0.7538        \u001b[36m0.8716\u001b[0m  0.0001  3.6540\n",
            "     21  0.4414       0.8555    0.2974        0.9338        \u001b[94m0.7525\u001b[0m        0.8719  0.0001  3.6378\n",
            "     22  0.4424       0.8551    0.2984        0.9337        0.7529        0.8718  0.0000  4.5594\n",
            "     23  0.4419       0.8553    0.2979        0.9337        \u001b[94m0.7519\u001b[0m        0.8718  0.0000  3.6744\n",
            "     24  0.4422       0.8552    0.2982        0.9337        \u001b[94m0.7518\u001b[0m        0.8718  0.0000  3.6568\n",
            "     25  0.4423       0.8552    0.2983        0.9337        \u001b[94m0.7499\u001b[0m        0.8718  0.0000  4.5139\n",
            "     26  0.4426       0.8553    0.2986        0.9337        0.7514        0.8717  0.0000  3.6216\n",
            "     27  0.4417       0.8552    0.2978        0.9337        0.7521        0.8719  0.0000  3.6139\n",
            "     28  0.4429       0.8552    0.2988        0.9336        0.7528        0.8717  0.0000  4.1403\n",
            "     29  0.4430       0.8551    0.2989        0.9336        0.7506        0.8716  0.0000  3.9908\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 3/3; 13/36] END lr=0.01, module__dropout=0.5, module__linear_size=500, module__size_emb=30;, score=-1.101 total time= 2.0min\n",
            "[CV 1/3; 14/36] START lr=0.01, module__dropout=0.5, module__linear_size=500, module__size_emb=60\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4390\u001b[0m       \u001b[32m0.8431\u001b[0m    \u001b[35m0.2968\u001b[0m        \u001b[31m0.9890\u001b[0m        \u001b[94m1.0196\u001b[0m        \u001b[36m0.9782\u001b[0m  0.0100  4.4782\n",
            "      2  0.3226       \u001b[32m0.8623\u001b[0m    0.1984        1.0040        \u001b[94m0.9033\u001b[0m        1.0081  0.0100  3.7305\n",
            "      3  \u001b[36m0.4505\u001b[0m       0.8435    \u001b[35m0.3073\u001b[0m        \u001b[31m0.9815\u001b[0m        \u001b[94m0.8862\u001b[0m        \u001b[36m0.9633\u001b[0m  0.0100  3.7013\n",
            "      4  0.4365       0.8360    0.2953        0.9932        \u001b[94m0.8780\u001b[0m        0.9864  0.0100  4.2405\n",
            "      5  \u001b[36m0.4723\u001b[0m       0.8389    \u001b[35m0.3286\u001b[0m        \u001b[31m0.9700\u001b[0m        \u001b[94m0.8778\u001b[0m        \u001b[36m0.9408\u001b[0m  0.0100  3.9682\n",
            "      6  0.2014       0.8591    0.1141        1.0005        0.8851        1.0011  0.0100  3.6767\n",
            "      7  0.2691       \u001b[32m0.8641\u001b[0m    0.1594        0.9896        \u001b[94m0.8760\u001b[0m        0.9793  0.0100  4.0135\n",
            "      8  0.3221       \u001b[32m0.8680\u001b[0m    0.1978        0.9749        \u001b[94m0.8274\u001b[0m        0.9504  0.0010  4.3525\n",
            "      9  0.3016       \u001b[32m0.8691\u001b[0m    0.1825        0.9743        \u001b[94m0.8114\u001b[0m        0.9492  0.0010  3.8395\n",
            "     10  0.3626       0.8650    0.2294        \u001b[31m0.9686\u001b[0m        \u001b[94m0.8038\u001b[0m        \u001b[36m0.9381\u001b[0m  0.0010  3.8760\n",
            "     11  0.3117       \u001b[32m0.8721\u001b[0m    0.1898        0.9716        \u001b[94m0.7993\u001b[0m        0.9441  0.0010  4.1341\n",
            "     12  0.3035       0.8710    0.1838        \u001b[31m0.9680\u001b[0m        \u001b[94m0.7932\u001b[0m        \u001b[36m0.9370\u001b[0m  0.0010  3.6267\n",
            "     13  0.3203       0.8702    0.1962        \u001b[31m0.9668\u001b[0m        \u001b[94m0.7866\u001b[0m        \u001b[36m0.9347\u001b[0m  0.0010  3.6959\n",
            "     14  0.3448       0.8690    0.2150        \u001b[31m0.9643\u001b[0m        \u001b[94m0.7832\u001b[0m        \u001b[36m0.9298\u001b[0m  0.0010  4.6528\n",
            "     15  0.3396       0.8675    0.2111        \u001b[31m0.9636\u001b[0m        \u001b[94m0.7783\u001b[0m        \u001b[36m0.9284\u001b[0m  0.0001  3.6319\n",
            "     16  0.3353       0.8667    0.2079        0.9642        \u001b[94m0.7759\u001b[0m        0.9296  0.0001  3.6382\n",
            "     17  0.3256       0.8688    0.2004        0.9647        \u001b[94m0.7749\u001b[0m        0.9306  0.0001  4.5000\n",
            "     18  0.3239       0.8681    0.1991        0.9644        \u001b[94m0.7739\u001b[0m        0.9300  0.0001  3.6202\n",
            "     19  0.3206       0.8687    0.1966        0.9650        \u001b[94m0.7732\u001b[0m        0.9312  0.0001  3.6804\n",
            "     20  0.3338       0.8693    0.2065        0.9641        \u001b[94m0.7708\u001b[0m        0.9295  0.0001  4.2896\n",
            "     21  0.3257       0.8678    0.2004        0.9641        0.7716        0.9295  0.0001  4.0908\n",
            "     22  0.3252       0.8680    0.2001        0.9641        \u001b[94m0.7704\u001b[0m        0.9295  0.0000  3.7524\n",
            "     23  0.3272       0.8682    0.2016        0.9641        0.7730        0.9294  0.0000  3.9351\n",
            "     24  0.3276       0.8683    0.2019        0.9640        0.7713        0.9293  0.0000  4.4419\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 1/3; 14/36] END lr=0.01, module__dropout=0.5, module__linear_size=500, module__size_emb=60;, score=-1.195 total time= 1.7min\n",
            "[CV 2/3; 14/36] START lr=0.01, module__dropout=0.5, module__linear_size=500, module__size_emb=60\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4457\u001b[0m       \u001b[32m0.8379\u001b[0m    \u001b[35m0.3036\u001b[0m        \u001b[31m0.9601\u001b[0m        \u001b[94m1.0634\u001b[0m        \u001b[36m0.9219\u001b[0m  0.0100  4.5201\n",
            "      2  0.4047       \u001b[32m0.8671\u001b[0m    0.2639        \u001b[31m0.9597\u001b[0m        \u001b[94m0.9032\u001b[0m        \u001b[36m0.9211\u001b[0m  0.0100  3.6618\n",
            "      3  0.3859       \u001b[32m0.8678\u001b[0m    0.2481        \u001b[31m0.9492\u001b[0m        \u001b[94m0.8874\u001b[0m        \u001b[36m0.9011\u001b[0m  0.0100  3.7248\n",
            "      4  \u001b[36m0.5016\u001b[0m       0.8448    \u001b[35m0.3567\u001b[0m        \u001b[31m0.9439\u001b[0m        \u001b[94m0.8805\u001b[0m        \u001b[36m0.8909\u001b[0m  0.0100  4.3351\n",
            "      5  0.2600       \u001b[32m0.9003\u001b[0m    0.1519        0.9819        \u001b[94m0.8669\u001b[0m        0.9642  0.0100  4.0253\n",
            "      6  0.3129       0.8922    0.1897        0.9524        \u001b[94m0.8623\u001b[0m        0.9071  0.0100  3.7277\n",
            "      7  0.4629       0.8654    0.3159        \u001b[31m0.9383\u001b[0m        \u001b[94m0.8526\u001b[0m        \u001b[36m0.8804\u001b[0m  0.0100  4.1062\n",
            "      8  0.4258       0.8778    0.2811        \u001b[31m0.9320\u001b[0m        \u001b[94m0.8026\u001b[0m        \u001b[36m0.8685\u001b[0m  0.0010  3.9533\n",
            "      9  0.3898       0.8923    0.2493        0.9320        \u001b[94m0.7840\u001b[0m        0.8686  0.0010  3.7084\n",
            "     10  0.3946       0.8930    0.2533        \u001b[31m0.9295\u001b[0m        \u001b[94m0.7745\u001b[0m        \u001b[36m0.8640\u001b[0m  0.0010  3.7228\n",
            "     11  0.4514       0.8786    0.3038        \u001b[31m0.9216\u001b[0m        \u001b[94m0.7682\u001b[0m        \u001b[36m0.8493\u001b[0m  0.0010  4.5942\n",
            "     12  0.4354       0.8846    0.2888        0.9232        \u001b[94m0.7592\u001b[0m        0.8523  0.0010  3.6424\n",
            "     13  0.4170       0.8912    0.2722        0.9242        \u001b[94m0.7565\u001b[0m        0.8541  0.0010  3.7691\n",
            "     14  0.3584       \u001b[32m0.9027\u001b[0m    0.2235        0.9274        \u001b[94m0.7493\u001b[0m        0.8602  0.0010  4.6003\n",
            "     15  0.3974       0.8949    0.2554        0.9228        \u001b[94m0.7416\u001b[0m        0.8515  0.0001  3.6649\n",
            "     16  0.4021       0.8928    0.2594        0.9217        \u001b[94m0.7403\u001b[0m        0.8495  0.0001  3.5858\n",
            "     17  0.4027       0.8935    0.2599        0.9219        \u001b[94m0.7380\u001b[0m        0.8499  0.0001  4.4497\n",
            "     18  0.3966       0.8958    0.2547        0.9220        \u001b[94m0.7368\u001b[0m        0.8501  0.0001  3.7696\n",
            "     19  0.3993       0.8944    0.2570        0.9219        0.7374        0.8500  0.0001  3.8421\n",
            "     20  0.3966       0.8955    0.2547        0.9225        \u001b[94m0.7351\u001b[0m        0.8511  0.0001  4.5937\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 2/3; 14/36] END lr=0.01, module__dropout=0.5, module__linear_size=500, module__size_emb=60;, score=-1.025 total time= 1.4min\n",
            "[CV 3/3; 14/36] START lr=0.01, module__dropout=0.5, module__linear_size=500, module__size_emb=60\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.3947\u001b[0m       \u001b[32m0.8550\u001b[0m    \u001b[35m0.2566\u001b[0m        \u001b[31m0.9776\u001b[0m        \u001b[94m1.0394\u001b[0m        \u001b[36m0.9556\u001b[0m  0.0100  3.6931\n",
            "      2  \u001b[36m0.4714\u001b[0m       0.8415    \u001b[35m0.3274\u001b[0m        \u001b[31m0.9604\u001b[0m        \u001b[94m0.8896\u001b[0m        \u001b[36m0.9224\u001b[0m  0.0100  4.4771\n",
            "      3  \u001b[36m0.5000\u001b[0m       0.8327    \u001b[35m0.3573\u001b[0m        0.9728        \u001b[94m0.8662\u001b[0m        0.9462  0.0100  3.6976\n",
            "      4  0.4388       0.8498    0.2958        \u001b[31m0.9583\u001b[0m        \u001b[94m0.8622\u001b[0m        \u001b[36m0.9184\u001b[0m  0.0100  3.7362\n",
            "      5  0.4276       \u001b[32m0.8563\u001b[0m    0.2850        0.9663        \u001b[94m0.8534\u001b[0m        0.9337  0.0100  4.6040\n",
            "      6  \u001b[36m0.5267\u001b[0m       0.8420    \u001b[35m0.3832\u001b[0m        \u001b[31m0.9553\u001b[0m        0.8555        \u001b[36m0.9127\u001b[0m  0.0100  3.7187\n",
            "      7  0.4009       \u001b[32m0.8677\u001b[0m    0.2607        0.9611        \u001b[94m0.8476\u001b[0m        0.9237  0.0100  3.6888\n",
            "      8  0.4320       \u001b[32m0.8733\u001b[0m    0.2870        \u001b[31m0.9446\u001b[0m        \u001b[94m0.7992\u001b[0m        \u001b[36m0.8924\u001b[0m  0.0010  4.1972\n",
            "      9  0.4113       \u001b[32m0.8755\u001b[0m    0.2688        0.9451        \u001b[94m0.7802\u001b[0m        0.8932  0.0010  3.8682\n",
            "     10  0.3893       \u001b[32m0.8804\u001b[0m    0.2499        \u001b[31m0.9440\u001b[0m        \u001b[94m0.7701\u001b[0m        \u001b[36m0.8911\u001b[0m  0.0010  3.7024\n",
            "     11  0.4062       \u001b[32m0.8811\u001b[0m    0.2639        \u001b[31m0.9412\u001b[0m        \u001b[94m0.7673\u001b[0m        \u001b[36m0.8858\u001b[0m  0.0010  4.0380\n",
            "     12  0.4189       0.8775    0.2751        \u001b[31m0.9398\u001b[0m        \u001b[94m0.7609\u001b[0m        \u001b[36m0.8832\u001b[0m  0.0010  4.2176\n",
            "     13  0.3892       \u001b[32m0.8864\u001b[0m    0.2493        0.9404        \u001b[94m0.7535\u001b[0m        0.8844  0.0010  3.7137\n",
            "     14  0.4221       0.8788    0.2778        \u001b[31m0.9346\u001b[0m        \u001b[94m0.7481\u001b[0m        \u001b[36m0.8735\u001b[0m  0.0010  3.8041\n",
            "     15  0.4038       0.8844    0.2616        0.9364        \u001b[94m0.7413\u001b[0m        0.8768  0.0001  4.5910\n",
            "     16  0.4131       0.8818    0.2697        0.9351        \u001b[94m0.7406\u001b[0m        0.8745  0.0001  3.7424\n",
            "     17  0.4035       0.8851    0.2613        0.9361        \u001b[94m0.7393\u001b[0m        0.8763  0.0001  3.8278\n",
            "     18  0.4033       0.8858    0.2611        0.9359        \u001b[94m0.7368\u001b[0m        0.8759  0.0001  4.6503\n",
            "     19  0.4086       0.8854    0.2656        0.9352        0.7386        0.8746  0.0001  3.8166\n",
            "     20  0.4110       0.8844    0.2677        0.9347        \u001b[94m0.7366\u001b[0m        0.8736  0.0001  3.7747\n",
            "     21  0.4094       0.8839    0.2664        0.9346        0.7366        0.8735  0.0001  4.6159\n",
            "     22  0.4086       0.8844    0.2657        0.9347        0.7383        0.8737  0.0000  3.7128\n",
            "     23  0.4079       0.8844    0.2651        0.9348        \u001b[94m0.7359\u001b[0m        0.8738  0.0000  3.6699\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 3/3; 14/36] END lr=0.01, module__dropout=0.5, module__linear_size=500, module__size_emb=60;, score=-1.120 total time= 1.6min\n",
            "[CV 1/3; 15/36] START lr=0.01, module__dropout=0.5, module__linear_size=500, module__size_emb=120\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4736\u001b[0m       \u001b[32m0.8353\u001b[0m    \u001b[35m0.3305\u001b[0m        \u001b[31m0.9879\u001b[0m        \u001b[94m1.0205\u001b[0m        \u001b[36m0.9760\u001b[0m  0.0100  3.5502\n",
            "      2  \u001b[36m0.5766\u001b[0m       0.8034    \u001b[35m0.4497\u001b[0m        0.9946        \u001b[94m0.9084\u001b[0m        0.9893  0.0100  3.8336\n",
            "      3  0.3654       \u001b[32m0.8542\u001b[0m    0.2324        0.9951        \u001b[94m0.8917\u001b[0m        0.9902  0.0100  4.5657\n",
            "      4  0.4930       0.8342    0.3499        0.9885        \u001b[94m0.8868\u001b[0m        0.9772  0.0100  3.6573\n",
            "      5  0.5016       0.8312    0.3592        0.9908        \u001b[94m0.8798\u001b[0m        0.9817  0.0100  3.7139\n",
            "      6  0.4985       0.8302    0.3562        \u001b[31m0.9817\u001b[0m        0.8886        \u001b[36m0.9638\u001b[0m  0.0100  4.6513\n",
            "      7  0.3399       \u001b[32m0.8658\u001b[0m    0.2115        0.9839        0.8838        0.9680  0.0100  3.7333\n",
            "      8  0.3842       0.8610    0.2473        \u001b[31m0.9746\u001b[0m        \u001b[94m0.8357\u001b[0m        \u001b[36m0.9499\u001b[0m  0.0010  3.7163\n",
            "      9  0.3919       0.8626    0.2535        \u001b[31m0.9681\u001b[0m        \u001b[94m0.8158\u001b[0m        \u001b[36m0.9371\u001b[0m  0.0010  4.6598\n",
            "     10  0.4071       0.8609    0.2666        \u001b[31m0.9668\u001b[0m        \u001b[94m0.8047\u001b[0m        \u001b[36m0.9346\u001b[0m  0.0010  3.8630\n",
            "     11  0.3774       \u001b[32m0.8668\u001b[0m    0.2412        \u001b[31m0.9634\u001b[0m        \u001b[94m0.7961\u001b[0m        \u001b[36m0.9282\u001b[0m  0.0010  3.7556\n",
            "     12  0.3786       \u001b[32m0.8670\u001b[0m    0.2422        0.9650        \u001b[94m0.7916\u001b[0m        0.9312  0.0010  4.5151\n",
            "     13  0.4116       0.8597    0.2705        \u001b[31m0.9575\u001b[0m        \u001b[94m0.7855\u001b[0m        \u001b[36m0.9169\u001b[0m  0.0010  3.8189\n",
            "     14  0.3907       \u001b[32m0.8678\u001b[0m    0.2521        0.9582        \u001b[94m0.7784\u001b[0m        0.9182  0.0010  3.7336\n",
            "     15  0.4072       0.8646    0.2663        \u001b[31m0.9573\u001b[0m        \u001b[94m0.7726\u001b[0m        \u001b[36m0.9164\u001b[0m  0.0001  4.2746\n",
            "     16  0.3916       \u001b[32m0.8681\u001b[0m    0.2528        0.9575        \u001b[94m0.7690\u001b[0m        0.9169  0.0001  4.0636\n",
            "     17  0.3974       0.8674    0.2577        \u001b[31m0.9567\u001b[0m        0.7705        \u001b[36m0.9152\u001b[0m  0.0001  3.8356\n",
            "     18  0.3938       \u001b[32m0.8689\u001b[0m    0.2546        \u001b[31m0.9566\u001b[0m        \u001b[94m0.7679\u001b[0m        \u001b[36m0.9152\u001b[0m  0.0001  4.1087\n",
            "     19  0.3924       0.8679    0.2535        0.9568        0.7691        0.9155  0.0001  4.1795\n",
            "     20  0.3879       0.8686    0.2497        0.9568        \u001b[94m0.7664\u001b[0m        0.9154  0.0001  3.7254\n",
            "     21  0.3996       0.8657    0.2597        \u001b[31m0.9558\u001b[0m        0.7679        \u001b[36m0.9136\u001b[0m  0.0001  3.6489\n",
            "     22  0.3986       0.8661    0.2589        0.9559        \u001b[94m0.7660\u001b[0m        0.9137  0.0000  4.5650\n",
            "     23  0.3979       0.8679    0.2581        0.9559        \u001b[94m0.7650\u001b[0m        0.9137  0.0000  3.4388\n",
            "     24  0.3972       0.8679    0.2576        0.9559        0.7657        0.9137  0.0000  3.6388\n",
            "     25  0.3985       0.8676    0.2586        \u001b[31m0.9558\u001b[0m        0.7682        \u001b[36m0.9135\u001b[0m  0.0000  4.6658\n",
            "     26  0.3972       0.8676    0.2576        0.9558        0.7661        0.9135  0.0000  3.7274\n",
            "     27  0.3972       0.8676    0.2576        \u001b[31m0.9558\u001b[0m        0.7657        \u001b[36m0.9135\u001b[0m  0.0000  3.6978\n",
            "     28  0.3962       0.8675    0.2568        \u001b[31m0.9557\u001b[0m        0.7656        \u001b[36m0.9134\u001b[0m  0.0000  4.6008\n",
            "     29  0.3964       0.8676    0.2568        \u001b[31m0.9557\u001b[0m        0.7671        \u001b[36m0.9134\u001b[0m  0.0000  3.7240\n",
            "     30  0.3964       0.8673    0.2569        \u001b[31m0.9557\u001b[0m        0.7658        \u001b[36m0.9134\u001b[0m  0.0000  3.7243\n",
            "[CV 1/3; 15/36] END lr=0.01, module__dropout=0.5, module__linear_size=500, module__size_emb=120;, score=-1.185 total time= 2.0min\n",
            "[CV 2/3; 15/36] START lr=0.01, module__dropout=0.5, module__linear_size=500, module__size_emb=120\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.3910\u001b[0m       \u001b[32m0.8643\u001b[0m    \u001b[35m0.2526\u001b[0m        \u001b[31m0.9612\u001b[0m        \u001b[94m1.0319\u001b[0m        \u001b[36m0.9240\u001b[0m  0.0100  4.5691\n",
            "      2  0.3603       \u001b[32m0.8712\u001b[0m    0.2271        \u001b[31m0.9591\u001b[0m        \u001b[94m0.9085\u001b[0m        \u001b[36m0.9199\u001b[0m  0.0100  3.7325\n",
            "      3  \u001b[36m0.4242\u001b[0m       0.8629    \u001b[35m0.2812\u001b[0m        \u001b[31m0.9492\u001b[0m        \u001b[94m0.8891\u001b[0m        \u001b[36m0.9010\u001b[0m  0.0100  3.7533\n",
            "      4  \u001b[36m0.5127\u001b[0m       0.8392    \u001b[35m0.3691\u001b[0m        \u001b[31m0.9487\u001b[0m        \u001b[94m0.8833\u001b[0m        \u001b[36m0.9000\u001b[0m  0.0100  4.6238\n",
            "      5  0.3605       \u001b[32m0.8751\u001b[0m    0.2270        0.9551        \u001b[94m0.8784\u001b[0m        0.9122  0.0100  3.8468\n",
            "      6  0.3435       \u001b[32m0.8835\u001b[0m    0.2132        0.9555        0.8799        0.9130  0.0100  3.7913\n",
            "      7  0.4114       0.8651    0.2698        \u001b[31m0.9434\u001b[0m        \u001b[94m0.8642\u001b[0m        \u001b[36m0.8900\u001b[0m  0.0100  4.6133\n",
            "      8  0.3346       \u001b[32m0.8871\u001b[0m    0.2062        0.9449        \u001b[94m0.8224\u001b[0m        0.8928  0.0010  3.7967\n",
            "      9  0.3110       \u001b[32m0.9012\u001b[0m    0.1879        \u001b[31m0.9427\u001b[0m        \u001b[94m0.7978\u001b[0m        \u001b[36m0.8887\u001b[0m  0.0010  3.7960\n",
            "     10  0.2810       \u001b[32m0.9103\u001b[0m    0.1662        0.9454        \u001b[94m0.7868\u001b[0m        0.8937  0.0010  4.7434\n",
            "     11  0.2764       \u001b[32m0.9163\u001b[0m    0.1628        \u001b[31m0.9395\u001b[0m        \u001b[94m0.7765\u001b[0m        \u001b[36m0.8827\u001b[0m  0.0010  3.7962\n",
            "     12  0.3007       0.9080    0.1802        \u001b[31m0.9344\u001b[0m        \u001b[94m0.7671\u001b[0m        \u001b[36m0.8732\u001b[0m  0.0010  3.5180\n",
            "     13  0.2830       0.9135    0.1674        \u001b[31m0.9344\u001b[0m        \u001b[94m0.7650\u001b[0m        \u001b[36m0.8730\u001b[0m  0.0010  4.4305\n",
            "     14  0.3434       0.8994    0.2122        \u001b[31m0.9256\u001b[0m        \u001b[94m0.7575\u001b[0m        \u001b[36m0.8568\u001b[0m  0.0010  3.9342\n",
            "     15  0.3025       0.9077    0.1815        0.9311        \u001b[94m0.7495\u001b[0m        0.8670  0.0001  3.7440\n",
            "     16  0.3201       0.9028    0.1945        0.9281        \u001b[94m0.7466\u001b[0m        0.8614  0.0001  4.1998\n",
            "     17  0.3193       0.9029    0.1939        0.9280        0.7472        0.8611  0.0001  4.2296\n",
            "     18  0.3113       0.9071    0.1879        0.9287        \u001b[94m0.7449\u001b[0m        0.8625  0.0001  3.7102\n",
            "     19  0.3159       0.9070    0.1912        0.9280        0.7450        0.8612  0.0001  3.9019\n",
            "     20  0.3108       0.9077    0.1875        0.9281        \u001b[94m0.7408\u001b[0m        0.8613  0.0001  4.4942\n",
            "     21  0.3097       0.9070    0.1868        0.9282        0.7413        0.8616  0.0001  3.7547\n",
            "     22  0.3073       0.9085    0.1850        0.9285        0.7426        0.8621  0.0000  3.8457\n",
            "     23  0.3071       0.9085    0.1848        0.9285        0.7422        0.8621  0.0000  4.6021\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 2/3; 15/36] END lr=0.01, module__dropout=0.5, module__linear_size=500, module__size_emb=120;, score=-1.034 total time= 1.7min\n",
            "[CV 3/3; 15/36] START lr=0.01, module__dropout=0.5, module__linear_size=500, module__size_emb=120\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.6170\u001b[0m       \u001b[32m0.7891\u001b[0m    \u001b[35m0.5065\u001b[0m        \u001b[31m0.9784\u001b[0m        \u001b[94m1.0112\u001b[0m        \u001b[36m0.9573\u001b[0m  0.0100  4.2668\n",
            "      2  0.5066       \u001b[32m0.8345\u001b[0m    0.3637        \u001b[31m0.9617\u001b[0m        \u001b[94m0.8865\u001b[0m        \u001b[36m0.9248\u001b[0m  0.0100  3.9222\n",
            "      3  0.3262       \u001b[32m0.8713\u001b[0m    0.2006        0.9657        \u001b[94m0.8773\u001b[0m        0.9326  0.0100  3.6330\n",
            "      4  0.3534       0.8683    0.2218        \u001b[31m0.9566\u001b[0m        \u001b[94m0.8615\u001b[0m        \u001b[36m0.9150\u001b[0m  0.0100  4.0571\n",
            "      5  0.2189       \u001b[32m0.8813\u001b[0m    0.1250        0.9736        \u001b[94m0.8602\u001b[0m        0.9479  0.0100  4.3559\n",
            "      6  0.2051       \u001b[32m0.8828\u001b[0m    0.1160        0.9866        0.8618        0.9734  0.0100  3.7716\n",
            "      7  0.3922       0.8605    0.2540        0.9571        0.8607        0.9160  0.0100  3.7507\n",
            "      8  0.2504       \u001b[32m0.8844\u001b[0m    0.1458        0.9646        \u001b[94m0.8182\u001b[0m        0.9304  0.0010  4.2817\n",
            "      9  0.3405       0.8748    0.2114        \u001b[31m0.9483\u001b[0m        \u001b[94m0.7903\u001b[0m        \u001b[36m0.8993\u001b[0m  0.0010  3.9062\n",
            "     10  0.2956       \u001b[32m0.8868\u001b[0m    0.1774        0.9516        \u001b[94m0.7807\u001b[0m        0.9055  0.0010  3.7864\n",
            "     11  0.2833       \u001b[32m0.8890\u001b[0m    0.1685        0.9518        \u001b[94m0.7732\u001b[0m        0.9060  0.0010  4.6658\n",
            "     12  0.2944       \u001b[32m0.8910\u001b[0m    0.1764        \u001b[31m0.9479\u001b[0m        \u001b[94m0.7674\u001b[0m        \u001b[36m0.8985\u001b[0m  0.0010  3.7464\n",
            "     13  0.2731       0.8899    0.1613        0.9495        \u001b[94m0.7600\u001b[0m        0.9015  0.0010  3.7238\n",
            "     14  0.3037       0.8847    0.1833        \u001b[31m0.9435\u001b[0m        \u001b[94m0.7553\u001b[0m        \u001b[36m0.8902\u001b[0m  0.0010  4.6765\n",
            "     15  0.3006       0.8849    0.1810        \u001b[31m0.9434\u001b[0m        \u001b[94m0.7460\u001b[0m        \u001b[36m0.8900\u001b[0m  0.0001  3.7887\n",
            "     16  0.2879       0.8880    0.1718        0.9448        \u001b[94m0.7432\u001b[0m        0.8927  0.0001  3.8130\n",
            "     17  0.3020       0.8847    0.1821        \u001b[31m0.9430\u001b[0m        0.7434        \u001b[36m0.8893\u001b[0m  0.0001  4.7163\n",
            "     18  0.3001       0.8859    0.1807        \u001b[31m0.9428\u001b[0m        \u001b[94m0.7425\u001b[0m        \u001b[36m0.8888\u001b[0m  0.0001  3.7372\n",
            "     19  0.2936       0.8871    0.1759        0.9435        \u001b[94m0.7412\u001b[0m        0.8903  0.0001  3.8231\n",
            "     20  0.2994       0.8860    0.1801        \u001b[31m0.9424\u001b[0m        0.7424        \u001b[36m0.8881\u001b[0m  0.0001  4.4676\n",
            "     21  0.2915       0.8875    0.1744        0.9430        \u001b[94m0.7403\u001b[0m        0.8893  0.0001  3.9453\n",
            "     22  0.2909       0.8880    0.1739        0.9431        \u001b[94m0.7399\u001b[0m        0.8895  0.0000  3.8660\n",
            "     23  0.2923       0.8878    0.1749        0.9430        0.7412        0.8893  0.0000  4.3587\n",
            "     24  0.2923       0.8874    0.1749        0.9430        \u001b[94m0.7372\u001b[0m        0.8892  0.0000  3.8439\n",
            "     25  0.2921       0.8877    0.1748        0.9430        0.7387        0.8893  0.0000  3.7168\n",
            "     26  0.2925       0.8879    0.1751        0.9430        0.7390        0.8892  0.0000  4.0487\n",
            "     27  0.2945       0.8875    0.1765        0.9427        0.7393        0.8887  0.0000  4.2653\n",
            "     28  0.2941       0.8873    0.1763        0.9427        0.7384        0.8887  0.0000  3.7299\n",
            "     29  0.2937       0.8872    0.1760        0.9427        0.7410        0.8887  0.0000  3.6911\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 3/3; 15/36] END lr=0.01, module__dropout=0.5, module__linear_size=500, module__size_emb=120;, score=-1.115 total time= 2.1min\n",
            "[CV 1/3; 16/36] START lr=0.01, module__dropout=0.5, module__linear_size=600, module__size_emb=30\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4206\u001b[0m       \u001b[32m0.8456\u001b[0m    \u001b[35m0.2799\u001b[0m        \u001b[31m0.9912\u001b[0m        \u001b[94m1.0647\u001b[0m        \u001b[36m0.9825\u001b[0m  0.0100  3.8027\n",
            "      2  \u001b[36m0.4562\u001b[0m       0.8325    \u001b[35m0.3141\u001b[0m        \u001b[31m0.9807\u001b[0m        \u001b[94m0.9054\u001b[0m        \u001b[36m0.9617\u001b[0m  0.0100  4.5792\n",
            "      3  0.2977       \u001b[32m0.8520\u001b[0m    0.1804        0.9829        \u001b[94m0.8865\u001b[0m        0.9661  0.0100  3.8789\n",
            "      4  0.3871       0.8463    0.2509        \u001b[31m0.9768\u001b[0m        \u001b[94m0.8773\u001b[0m        \u001b[36m0.9542\u001b[0m  0.0100  3.7657\n",
            "      5  0.3811       0.8386    0.2466        0.9876        \u001b[94m0.8710\u001b[0m        0.9753  0.0100  4.2258\n",
            "      6  0.2713       \u001b[32m0.8520\u001b[0m    0.1613        0.9831        \u001b[94m0.8661\u001b[0m        0.9664  0.0100  4.1885\n",
            "      7  0.2944       \u001b[32m0.8582\u001b[0m    0.1777        0.9793        \u001b[94m0.8607\u001b[0m        0.9590  0.0100  3.8445\n",
            "      8  0.3609       0.8491    0.2292        \u001b[31m0.9715\u001b[0m        \u001b[94m0.8235\u001b[0m        \u001b[36m0.9439\u001b[0m  0.0010  4.2180\n",
            "      9  0.3733       0.8447    0.2396        \u001b[31m0.9679\u001b[0m        \u001b[94m0.8068\u001b[0m        \u001b[36m0.9369\u001b[0m  0.0010  4.2708\n",
            "     10  0.3058       \u001b[32m0.8590\u001b[0m    0.1860        0.9707        \u001b[94m0.7977\u001b[0m        0.9422  0.0010  3.8337\n",
            "     11  0.3425       0.8564    0.2141        \u001b[31m0.9655\u001b[0m        \u001b[94m0.7922\u001b[0m        \u001b[36m0.9322\u001b[0m  0.0010  4.0568\n",
            "     12  0.4041       0.8459    0.2654        \u001b[31m0.9601\u001b[0m        \u001b[94m0.7865\u001b[0m        \u001b[36m0.9218\u001b[0m  0.0010  4.4940\n",
            "     13  0.3942       0.8464    0.2569        \u001b[31m0.9583\u001b[0m        \u001b[94m0.7819\u001b[0m        \u001b[36m0.9184\u001b[0m  0.0010  3.8903\n",
            "     14  0.3644       0.8544    0.2316        0.9588        \u001b[94m0.7756\u001b[0m        0.9193  0.0010  3.9330\n",
            "     15  0.3782       0.8486    0.2433        \u001b[31m0.9580\u001b[0m        \u001b[94m0.7686\u001b[0m        \u001b[36m0.9178\u001b[0m  0.0001  4.3586\n",
            "     16  0.3913       0.8481    0.2543        \u001b[31m0.9570\u001b[0m        \u001b[94m0.7684\u001b[0m        \u001b[36m0.9159\u001b[0m  0.0001  3.8629\n",
            "     17  0.3915       0.8489    0.2544        \u001b[31m0.9569\u001b[0m        \u001b[94m0.7666\u001b[0m        \u001b[36m0.9157\u001b[0m  0.0001  3.9054\n",
            "     18  0.3818       0.8498    0.2462        0.9573        0.7689        0.9163  0.0001  4.7312\n",
            "     19  0.3819       0.8496    0.2463        \u001b[31m0.9568\u001b[0m        \u001b[94m0.7638\u001b[0m        \u001b[36m0.9154\u001b[0m  0.0001  3.8505\n",
            "     20  0.3900       0.8482    0.2532        \u001b[31m0.9562\u001b[0m        0.7661        \u001b[36m0.9144\u001b[0m  0.0001  3.8816\n",
            "     21  0.3944       0.8500    0.2568        \u001b[31m0.9560\u001b[0m        0.7638        \u001b[36m0.9138\u001b[0m  0.0001  4.7148\n",
            "     22  0.3923       0.8494    0.2551        0.9560        0.7646        0.9139  0.0000  3.8822\n",
            "     23  0.3926       0.8493    0.2553        \u001b[31m0.9559\u001b[0m        \u001b[94m0.7632\u001b[0m        \u001b[36m0.9138\u001b[0m  0.0000  3.8559\n",
            "     24  0.3909       0.8491    0.2539        0.9560        \u001b[94m0.7630\u001b[0m        0.9139  0.0000  4.6821\n",
            "     25  0.3907       0.8500    0.2536        0.9560        0.7647        0.9140  0.0000  3.8413\n",
            "     26  0.3882       0.8495    0.2516        0.9562        0.7657        0.9142  0.0000  3.8659\n",
            "     27  0.3872       0.8490    0.2508        0.9562        \u001b[94m0.7618\u001b[0m        0.9142  0.0000  4.7678\n",
            "     28  0.3871       0.8490    0.2507        0.9561        0.7639        0.9142  0.0000  3.8285\n",
            "     29  0.3874       0.8491    0.2509        0.9561        0.7653        0.9142  0.0000  3.9079\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 1/3; 16/36] END lr=0.01, module__dropout=0.5, module__linear_size=600, module__size_emb=30;, score=-1.190 total time= 2.1min\n",
            "[CV 2/3; 16/36] START lr=0.01, module__dropout=0.5, module__linear_size=600, module__size_emb=30\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.1574\u001b[0m       \u001b[32m0.8718\u001b[0m    \u001b[35m0.0865\u001b[0m        \u001b[31m0.9841\u001b[0m        \u001b[94m1.0825\u001b[0m        \u001b[36m0.9684\u001b[0m  0.0100  3.8181\n",
            "      2  \u001b[36m0.2829\u001b[0m       \u001b[32m0.8839\u001b[0m    \u001b[35m0.1684\u001b[0m        \u001b[31m0.9622\u001b[0m        \u001b[94m0.9058\u001b[0m        \u001b[36m0.9258\u001b[0m  0.0100  4.0574\n",
            "      3  \u001b[36m0.3785\u001b[0m       0.8635    \u001b[35m0.2423\u001b[0m        \u001b[31m0.9487\u001b[0m        \u001b[94m0.8833\u001b[0m        \u001b[36m0.9001\u001b[0m  0.0100  4.5815\n",
            "      4  0.2016       0.8743    0.1140        0.9661        \u001b[94m0.8729\u001b[0m        0.9334  0.0100  3.8755\n",
            "      5  \u001b[36m0.4422\u001b[0m       0.8484    \u001b[35m0.2990\u001b[0m        0.9558        \u001b[94m0.8708\u001b[0m        0.9135  0.0100  3.8228\n",
            "      6  0.4018       0.8504    0.2630        \u001b[31m0.9426\u001b[0m        0.8743        \u001b[36m0.8886\u001b[0m  0.0100  4.8274\n",
            "      7  0.2480       0.8785    0.1444        0.9588        \u001b[94m0.8608\u001b[0m        0.9192  0.0100  3.9402\n",
            "      8  0.3534       0.8697    0.2218        \u001b[31m0.9376\u001b[0m        \u001b[94m0.8157\u001b[0m        \u001b[36m0.8792\u001b[0m  0.0010  3.8940\n",
            "      9  0.3468       0.8769    0.2161        \u001b[31m0.9347\u001b[0m        \u001b[94m0.7948\u001b[0m        \u001b[36m0.8737\u001b[0m  0.0010  4.8056\n",
            "     10  0.3349       0.8761    0.2070        \u001b[31m0.9341\u001b[0m        \u001b[94m0.7857\u001b[0m        \u001b[36m0.8726\u001b[0m  0.0010  3.8615\n",
            "     11  0.3990       0.8705    0.2588        \u001b[31m0.9279\u001b[0m        \u001b[94m0.7785\u001b[0m        \u001b[36m0.8610\u001b[0m  0.0010  3.8050\n",
            "     12  0.3986       0.8734    0.2582        \u001b[31m0.9252\u001b[0m        \u001b[94m0.7731\u001b[0m        \u001b[36m0.8560\u001b[0m  0.0010  4.8076\n",
            "     13  0.4009       0.8717    0.2603        \u001b[31m0.9249\u001b[0m        \u001b[94m0.7684\u001b[0m        \u001b[36m0.8554\u001b[0m  0.0010  3.8927\n",
            "     14  0.3644       0.8745    0.2302        0.9260        \u001b[94m0.7637\u001b[0m        0.8574  0.0010  3.8619\n",
            "     15  0.3936       0.8730    0.2541        \u001b[31m0.9235\u001b[0m        \u001b[94m0.7565\u001b[0m        \u001b[36m0.8529\u001b[0m  0.0001  4.7299\n",
            "     16  0.3926       0.8729    0.2533        \u001b[31m0.9234\u001b[0m        \u001b[94m0.7557\u001b[0m        \u001b[36m0.8527\u001b[0m  0.0001  3.8786\n",
            "     17  0.3971       0.8734    0.2569        \u001b[31m0.9230\u001b[0m        \u001b[94m0.7538\u001b[0m        \u001b[36m0.8520\u001b[0m  0.0001  3.6228\n",
            "     18  0.3946       0.8725    0.2550        0.9231        0.7544        0.8521  0.0001  4.7782\n",
            "     19  0.3929       0.8727    0.2535        \u001b[31m0.9229\u001b[0m        \u001b[94m0.7514\u001b[0m        \u001b[36m0.8518\u001b[0m  0.0001  3.8081\n",
            "     20  0.3918       0.8731    0.2526        \u001b[31m0.9228\u001b[0m        \u001b[94m0.7501\u001b[0m        \u001b[36m0.8516\u001b[0m  0.0001  3.9300\n",
            "     21  0.4009       0.8724    0.2603        \u001b[31m0.9221\u001b[0m        0.7520        \u001b[36m0.8502\u001b[0m  0.0001  4.7220\n",
            "     22  0.3995       0.8732    0.2590        0.9222        0.7510        0.8504  0.0000  3.8986\n",
            "     23  0.3992       0.8734    0.2587        0.9222        \u001b[94m0.7499\u001b[0m        0.8504  0.0000  3.9323\n",
            "     24  0.3996       0.8730    0.2591        0.9221        \u001b[94m0.7499\u001b[0m        0.8503  0.0000  4.7413\n",
            "     25  0.3989       0.8738    0.2585        0.9222        0.7510        0.8505  0.0000  3.8354\n",
            "     26  0.3997       0.8730    0.2592        0.9221        \u001b[94m0.7497\u001b[0m        0.8503  0.0000  3.8739\n",
            "     27  0.4000       0.8729    0.2594        \u001b[31m0.9221\u001b[0m        0.7508        \u001b[36m0.8502\u001b[0m  0.0000  4.7463\n",
            "     28  0.3992       0.8731    0.2587        0.9221        0.7512        0.8503  0.0000  3.9467\n",
            "     29  0.3992       0.8731    0.2587        0.9221        0.7515        0.8503  0.0000  3.9555\n",
            "     30  0.3992       0.8731    0.2587        0.9221        0.7515        0.8503  0.0000  4.7262\n",
            "[CV 2/3; 16/36] END lr=0.01, module__dropout=0.5, module__linear_size=600, module__size_emb=30;, score=-1.030 total time= 2.1min\n",
            "[CV 3/3; 16/36] START lr=0.01, module__dropout=0.5, module__linear_size=600, module__size_emb=30\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.2326\u001b[0m       \u001b[32m0.8796\u001b[0m    \u001b[35m0.1340\u001b[0m        \u001b[31m0.9875\u001b[0m        \u001b[94m1.0695\u001b[0m        \u001b[36m0.9751\u001b[0m  0.0100  3.8617\n",
            "      2  \u001b[36m0.3175\u001b[0m       0.8687    \u001b[35m0.1943\u001b[0m        \u001b[31m0.9690\u001b[0m        \u001b[94m0.8882\u001b[0m        \u001b[36m0.9389\u001b[0m  0.0100  3.9079\n",
            "      3  0.2966       \u001b[32m0.8802\u001b[0m    0.1783        \u001b[31m0.9678\u001b[0m        \u001b[94m0.8688\u001b[0m        \u001b[36m0.9366\u001b[0m  0.0100  4.5643\n",
            "      4  \u001b[36m0.3203\u001b[0m       0.8712    \u001b[35m0.1962\u001b[0m        \u001b[31m0.9623\u001b[0m        \u001b[94m0.8602\u001b[0m        \u001b[36m0.9260\u001b[0m  0.0100  3.8280\n",
            "      5  0.3155       0.8743    0.1925        \u001b[31m0.9617\u001b[0m        \u001b[94m0.8534\u001b[0m        \u001b[36m0.9248\u001b[0m  0.0100  3.7879\n",
            "      6  \u001b[36m0.3272\u001b[0m       0.8682    \u001b[35m0.2016\u001b[0m        \u001b[31m0.9573\u001b[0m        \u001b[94m0.8457\u001b[0m        \u001b[36m0.9165\u001b[0m  0.0100  4.6315\n",
            "      7  0.2126       0.8784    0.1209        0.9851        0.8462        0.9704  0.0100  3.5738\n",
            "      8  0.3040       0.8748    0.1840        \u001b[31m0.9562\u001b[0m        \u001b[94m0.8030\u001b[0m        \u001b[36m0.9144\u001b[0m  0.0010  3.8304\n",
            "      9  0.3117       0.8795    0.1894        \u001b[31m0.9521\u001b[0m        \u001b[94m0.7829\u001b[0m        \u001b[36m0.9066\u001b[0m  0.0010  4.6671\n",
            "     10  \u001b[36m0.3472\u001b[0m       0.8740    \u001b[35m0.2167\u001b[0m        \u001b[31m0.9456\u001b[0m        \u001b[94m0.7751\u001b[0m        \u001b[36m0.8942\u001b[0m  0.0010  3.7243\n",
            "     11  0.3180       \u001b[32m0.8826\u001b[0m    0.1939        0.9473        \u001b[94m0.7688\u001b[0m        0.8974  0.0010  3.7521\n",
            "     12  0.3244       \u001b[32m0.8851\u001b[0m    0.1986        0.9463        \u001b[94m0.7620\u001b[0m        0.8955  0.0010  4.6392\n",
            "     13  \u001b[36m0.3669\u001b[0m       0.8708    \u001b[35m0.2324\u001b[0m        \u001b[31m0.9389\u001b[0m        \u001b[94m0.7555\u001b[0m        \u001b[36m0.8816\u001b[0m  0.0010  3.7498\n",
            "     14  \u001b[36m0.3709\u001b[0m       0.8696    \u001b[35m0.2357\u001b[0m        \u001b[31m0.9377\u001b[0m        \u001b[94m0.7536\u001b[0m        \u001b[36m0.8793\u001b[0m  0.0010  3.7036\n",
            "     15  0.3577       0.8737    0.2249        0.9384        \u001b[94m0.7445\u001b[0m        0.8807  0.0001  4.4855\n",
            "     16  0.3638       0.8714    0.2299        \u001b[31m0.9376\u001b[0m        \u001b[94m0.7440\u001b[0m        \u001b[36m0.8791\u001b[0m  0.0001  3.8111\n",
            "     17  0.3623       0.8717    0.2286        \u001b[31m0.9376\u001b[0m        0.7440        \u001b[36m0.8790\u001b[0m  0.0001  3.8720\n",
            "     18  0.3686       0.8701    0.2338        \u001b[31m0.9367\u001b[0m        \u001b[94m0.7438\u001b[0m        \u001b[36m0.8774\u001b[0m  0.0001  4.3304\n",
            "     19  \u001b[36m0.3727\u001b[0m       0.8708    \u001b[35m0.2371\u001b[0m        \u001b[31m0.9362\u001b[0m        \u001b[94m0.7413\u001b[0m        \u001b[36m0.8765\u001b[0m  0.0001  4.1506\n",
            "     20  0.3616       0.8726    0.2280        0.9373        0.7425        0.8785  0.0001  3.7667\n",
            "     21  0.3684       0.8717    0.2336        0.9363        0.7414        0.8767  0.0001  4.1281\n",
            "     22  0.3681       0.8719    0.2333        0.9364        \u001b[94m0.7402\u001b[0m        0.8768  0.0000  4.2661\n",
            "     23  0.3672       0.8721    0.2326        0.9365        0.7406        0.8770  0.0000  3.8312\n",
            "     24  0.3665       0.8718    0.2321        0.9366        \u001b[94m0.7402\u001b[0m        0.8771  0.0000  3.9961\n",
            "     25  0.3678       0.8717    0.2330        0.9365        \u001b[94m0.7383\u001b[0m        0.8770  0.0000  4.5121\n",
            "     26  0.3676       0.8719    0.2329        0.9365        0.7406        0.8771  0.0000  3.8285\n",
            "     27  0.3672       0.8721    0.2326        0.9365        0.7390        0.8770  0.0000  3.8124\n",
            "     28  0.3676       0.8717    0.2329        0.9365        0.7393        0.8769  0.0000  4.3316\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 3/3; 16/36] END lr=0.01, module__dropout=0.5, module__linear_size=600, module__size_emb=30;, score=-1.113 total time= 2.0min\n",
            "[CV 1/3; 17/36] START lr=0.01, module__dropout=0.5, module__linear_size=600, module__size_emb=60\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.2784\u001b[0m       \u001b[32m0.8799\u001b[0m    \u001b[35m0.1654\u001b[0m        \u001b[31m1.0060\u001b[0m        \u001b[94m1.0430\u001b[0m        \u001b[36m1.0120\u001b[0m  0.0100  4.5428\n",
            "      2  \u001b[36m0.3049\u001b[0m       0.8666    \u001b[35m0.1850\u001b[0m        \u001b[31m0.9928\u001b[0m        \u001b[94m0.9044\u001b[0m        \u001b[36m0.9856\u001b[0m  0.0100  4.0746\n",
            "      3  0.2991       0.8647    0.1808        \u001b[31m0.9882\u001b[0m        \u001b[94m0.8905\u001b[0m        \u001b[36m0.9764\u001b[0m  0.0100  3.5679\n",
            "      4  0.1907       \u001b[32m0.8876\u001b[0m    0.1068        1.0069        \u001b[94m0.8826\u001b[0m        1.0138  0.0100  4.1282\n",
            "      5  \u001b[36m0.3890\u001b[0m       0.8459    \u001b[35m0.2526\u001b[0m        \u001b[31m0.9791\u001b[0m        0.8830        \u001b[36m0.9587\u001b[0m  0.0100  4.4298\n",
            "      6  0.2000       0.8817    0.1128        0.9963        0.8854        0.9926  0.0100  3.8037\n",
            "      7  0.2666       0.8451    0.1583        0.9977        \u001b[94m0.8793\u001b[0m        0.9954  0.0100  3.9303\n",
            "      8  0.3396       0.8608    0.2115        \u001b[31m0.9737\u001b[0m        \u001b[94m0.8353\u001b[0m        \u001b[36m0.9480\u001b[0m  0.0010  4.5675\n",
            "      9  0.3361       0.8587    0.2090        \u001b[31m0.9718\u001b[0m        \u001b[94m0.8157\u001b[0m        \u001b[36m0.9444\u001b[0m  0.0010  3.8376\n",
            "     10  0.2729       0.8648    0.1620        0.9740        \u001b[94m0.8076\u001b[0m        0.9487  0.0010  3.8093\n",
            "     11  0.2828       0.8645    0.1690        0.9728        \u001b[94m0.8019\u001b[0m        0.9463  0.0010  4.6856\n",
            "     12  0.2710       0.8671    0.1606        0.9739        \u001b[94m0.7943\u001b[0m        0.9485  0.0010  3.7635\n",
            "     13  0.2561       0.8681    0.1502        \u001b[31m0.9714\u001b[0m        \u001b[94m0.7915\u001b[0m        \u001b[36m0.9437\u001b[0m  0.0010  3.7948\n",
            "     14  0.3124       0.8587    0.1910        \u001b[31m0.9657\u001b[0m        \u001b[94m0.7865\u001b[0m        \u001b[36m0.9325\u001b[0m  0.0010  4.7901\n",
            "     15  0.3252       0.8564    0.2007        \u001b[31m0.9647\u001b[0m        \u001b[94m0.7790\u001b[0m        \u001b[36m0.9306\u001b[0m  0.0001  3.7641\n",
            "     16  0.3170       0.8591    0.1944        0.9649        0.7791        0.9311  0.0001  3.7780\n",
            "     17  0.3168       0.8580    0.1943        0.9650        \u001b[94m0.7785\u001b[0m        0.9311  0.0001  4.6147\n",
            "     18  0.3225       0.8561    0.1987        \u001b[31m0.9640\u001b[0m        \u001b[94m0.7773\u001b[0m        \u001b[36m0.9293\u001b[0m  0.0001  3.7545\n",
            "     19  0.3235       0.8556    0.1995        \u001b[31m0.9635\u001b[0m        \u001b[94m0.7756\u001b[0m        \u001b[36m0.9284\u001b[0m  0.0001  3.8328\n",
            "     20  0.3252       0.8545    0.2008        0.9637        0.7761        0.9288  0.0001  4.4010\n",
            "     21  0.3175       0.8561    0.1949        0.9640        \u001b[94m0.7750\u001b[0m        0.9294  0.0001  3.8135\n",
            "     22  0.3177       0.8552    0.1951        0.9640        \u001b[94m0.7730\u001b[0m        0.9292  0.0000  3.8175\n",
            "     23  0.3187       0.8550    0.1959        0.9638        0.7745        0.9289  0.0000  4.5893\n",
            "     24  0.3202       0.8554    0.1970        0.9636        \u001b[94m0.7727\u001b[0m        0.9285  0.0000  3.8590\n",
            "     25  0.3216       0.8554    0.1980        \u001b[31m0.9635\u001b[0m        0.7741        \u001b[36m0.9283\u001b[0m  0.0000  3.7418\n",
            "     26  0.3213       0.8555    0.1978        \u001b[31m0.9635\u001b[0m        0.7744        \u001b[36m0.9283\u001b[0m  0.0000  4.2781\n",
            "     27  0.3218       0.8555    0.1982        \u001b[31m0.9634\u001b[0m        \u001b[94m0.7715\u001b[0m        \u001b[36m0.9282\u001b[0m  0.0000  4.0327\n",
            "     28  0.3206       0.8555    0.1972        0.9635        0.7736        0.9284  0.0000  3.7298\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 1/3; 17/36] END lr=0.01, module__dropout=0.5, module__linear_size=600, module__size_emb=60;, score=-1.185 total time= 2.0min\n",
            "[CV 2/3; 17/36] START lr=0.01, module__dropout=0.5, module__linear_size=600, module__size_emb=60\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.5182\u001b[0m       \u001b[32m0.8253\u001b[0m    \u001b[35m0.3777\u001b[0m        \u001b[31m0.9616\u001b[0m        \u001b[94m1.0767\u001b[0m        \u001b[36m0.9246\u001b[0m  0.0100  3.7873\n",
            "      2  0.4121       \u001b[32m0.8562\u001b[0m    0.2714        \u001b[31m0.9536\u001b[0m        \u001b[94m0.8990\u001b[0m        \u001b[36m0.9094\u001b[0m  0.0100  3.7283\n",
            "      3  0.4747       0.8498    0.3294        \u001b[31m0.9451\u001b[0m        \u001b[94m0.8868\u001b[0m        \u001b[36m0.8932\u001b[0m  0.0100  4.6231\n",
            "      4  0.4779       0.8506    0.3323        0.9488        \u001b[94m0.8749\u001b[0m        0.9002  0.0100  3.7625\n",
            "      5  0.3302       \u001b[32m0.8798\u001b[0m    0.2032        0.9563        \u001b[94m0.8654\u001b[0m        0.9146  0.0100  3.8540\n",
            "      6  0.5128       0.8528    0.3666        \u001b[31m0.9346\u001b[0m        \u001b[94m0.8642\u001b[0m        \u001b[36m0.8734\u001b[0m  0.0100  4.5169\n",
            "      7  0.3880       \u001b[32m0.8814\u001b[0m    0.2488        0.9417        \u001b[94m0.8627\u001b[0m        0.8868  0.0100  3.8988\n",
            "      8  0.3999       \u001b[32m0.8837\u001b[0m    0.2585        0.9403        \u001b[94m0.8129\u001b[0m        0.8842  0.0010  3.7525\n",
            "      9  0.4254       0.8810    0.2804        0.9378        \u001b[94m0.7906\u001b[0m        0.8794  0.0010  3.9497\n",
            "     10  0.3920       \u001b[32m0.8881\u001b[0m    0.2515        0.9368        \u001b[94m0.7824\u001b[0m        0.8775  0.0010  4.1805\n",
            "     11  0.4122       0.8837    0.2688        \u001b[31m0.9309\u001b[0m        \u001b[94m0.7753\u001b[0m        \u001b[36m0.8667\u001b[0m  0.0010  3.7812\n",
            "     12  0.4193       0.8840    0.2748        \u001b[31m0.9299\u001b[0m        \u001b[94m0.7691\u001b[0m        \u001b[36m0.8648\u001b[0m  0.0010  4.0471\n",
            "     13  0.4280       0.8804    0.2827        \u001b[31m0.9274\u001b[0m        \u001b[94m0.7624\u001b[0m        \u001b[36m0.8601\u001b[0m  0.0010  4.3739\n",
            "     14  0.4418       0.8810    0.2948        \u001b[31m0.9251\u001b[0m        \u001b[94m0.7581\u001b[0m        \u001b[36m0.8557\u001b[0m  0.0010  3.7521\n",
            "     15  0.4205       0.8833    0.2759        0.9258        \u001b[94m0.7497\u001b[0m        0.8571  0.0001  3.7756\n",
            "     16  0.4163       0.8833    0.2723        0.9254        \u001b[94m0.7490\u001b[0m        0.8564  0.0001  4.6518\n",
            "     17  0.4213       0.8826    0.2767        \u001b[31m0.9249\u001b[0m        \u001b[94m0.7483\u001b[0m        \u001b[36m0.8554\u001b[0m  0.0001  3.7290\n",
            "     18  0.4199       0.8829    0.2755        0.9251        0.7491        0.8558  0.0001  3.7278\n",
            "     19  0.4259       0.8824    0.2807        \u001b[31m0.9239\u001b[0m        \u001b[94m0.7457\u001b[0m        \u001b[36m0.8536\u001b[0m  0.0001  4.6262\n",
            "     20  0.4197       0.8836    0.2752        0.9249        0.7457        0.8554  0.0001  3.7808\n",
            "     21  0.4159       0.8844    0.2719        0.9244        \u001b[94m0.7453\u001b[0m        0.8546  0.0001  3.8193\n",
            "     22  0.4160       0.8844    0.2720        0.9244        0.7453        0.8546  0.0000  4.6669\n",
            "     23  0.4166       0.8844    0.2725        0.9243        \u001b[94m0.7434\u001b[0m        0.8544  0.0000  3.7460\n",
            "     24  0.4174       0.8843    0.2731        0.9243        0.7441        0.8543  0.0000  3.7437\n",
            "     25  0.4165       0.8846    0.2723        0.9243        0.7446        0.8544  0.0000  4.6547\n",
            "     26  0.4161       0.8845    0.2721        0.9243        0.7458        0.8544  0.0000  3.8187\n",
            "     27  0.4166       0.8844    0.2725        0.9243        0.7451        0.8544  0.0000  3.8720\n",
            "     28  0.4169       0.8844    0.2727        0.9243        0.7468        0.8543  0.0000  4.6474\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 2/3; 17/36] END lr=0.01, module__dropout=0.5, module__linear_size=600, module__size_emb=60;, score=-1.020 total time= 2.0min\n",
            "[CV 3/3; 17/36] START lr=0.01, module__dropout=0.5, module__linear_size=600, module__size_emb=60\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.2338\u001b[0m       \u001b[32m0.8786\u001b[0m    \u001b[35m0.1348\u001b[0m        \u001b[31m0.9840\u001b[0m        \u001b[94m1.0570\u001b[0m        \u001b[36m0.9682\u001b[0m  0.0100  3.7497\n",
            "      2  \u001b[36m0.4717\u001b[0m       0.8351    \u001b[35m0.3286\u001b[0m        \u001b[31m0.9654\u001b[0m        \u001b[94m0.8909\u001b[0m        \u001b[36m0.9319\u001b[0m  0.0100  4.6695\n",
            "      3  0.1972       \u001b[32m0.8894\u001b[0m    0.1109        1.0080        \u001b[94m0.8742\u001b[0m        1.0160  0.0100  3.7418\n",
            "      4  0.1474       0.8847    0.0804        0.9934        \u001b[94m0.8685\u001b[0m        0.9867  0.0100  3.4972\n",
            "      5  0.3751       0.8606    0.2398        \u001b[31m0.9599\u001b[0m        \u001b[94m0.8653\u001b[0m        \u001b[36m0.9215\u001b[0m  0.0100  4.6284\n",
            "      6  0.2179       0.8748    0.1244        0.9760        \u001b[94m0.8611\u001b[0m        0.9526  0.0100  3.8659\n",
            "      7  0.2672       0.8834    0.1574        0.9682        \u001b[94m0.8554\u001b[0m        0.9375  0.0100  3.8343\n",
            "      8  0.2687       0.8849    0.1584        0.9619        \u001b[94m0.8101\u001b[0m        0.9253  0.0010  4.5660\n",
            "      9  0.3013       0.8829    0.1816        \u001b[31m0.9550\u001b[0m        \u001b[94m0.7884\u001b[0m        \u001b[36m0.9120\u001b[0m  0.0010  3.7662\n",
            "     10  0.2732       0.8882    0.1614        0.9570        \u001b[94m0.7810\u001b[0m        0.9158  0.0010  3.7037\n",
            "     11  0.2989       0.8812    0.1799        \u001b[31m0.9533\u001b[0m        \u001b[94m0.7749\u001b[0m        \u001b[36m0.9087\u001b[0m  0.0010  4.4979\n",
            "     12  0.2956       0.8798    0.1776        \u001b[31m0.9519\u001b[0m        \u001b[94m0.7716\u001b[0m        \u001b[36m0.9062\u001b[0m  0.0010  3.9376\n",
            "     13  0.3246       0.8780    0.1991        \u001b[31m0.9457\u001b[0m        \u001b[94m0.7666\u001b[0m        \u001b[36m0.8943\u001b[0m  0.0010  3.8218\n",
            "     14  0.3321       0.8776    0.2048        \u001b[31m0.9429\u001b[0m        \u001b[94m0.7621\u001b[0m        \u001b[36m0.8890\u001b[0m  0.0010  4.3468\n",
            "     15  0.3290       0.8794    0.2023        0.9433        \u001b[94m0.7537\u001b[0m        0.8899  0.0001  4.2016\n",
            "     16  0.3313       0.8790    0.2041        \u001b[31m0.9428\u001b[0m        \u001b[94m0.7535\u001b[0m        \u001b[36m0.8888\u001b[0m  0.0001  3.7983\n",
            "     17  0.3310       0.8802    0.2038        \u001b[31m0.9424\u001b[0m        \u001b[94m0.7519\u001b[0m        \u001b[36m0.8882\u001b[0m  0.0001  4.2004\n",
            "     18  0.3210       0.8797    0.1963        0.9433        \u001b[94m0.7501\u001b[0m        0.8899  0.0001  4.2381\n",
            "     19  0.3362       0.8795    0.2078        \u001b[31m0.9415\u001b[0m        \u001b[94m0.7498\u001b[0m        \u001b[36m0.8864\u001b[0m  0.0001  3.7040\n",
            "     20  0.3298       0.8794    0.2030        0.9421        \u001b[94m0.7497\u001b[0m        0.8875  0.0001  3.8522\n",
            "     21  0.3270       0.8796    0.2008        0.9423        0.7501        0.8879  0.0001  4.5393\n",
            "     22  0.3274       0.8795    0.2012        0.9422        0.7502        0.8878  0.0000  3.7969\n",
            "     23  0.3290       0.8794    0.2023        0.9420        \u001b[94m0.7478\u001b[0m        0.8874  0.0000  3.7629\n",
            "     24  0.3320       0.8792    0.2047        0.9417        \u001b[94m0.7463\u001b[0m        0.8868  0.0000  4.3093\n",
            "     25  0.3323       0.8793    0.2048        0.9416        0.7494        0.8866  0.0000  3.7447\n",
            "     26  0.3310       0.8792    0.2038        0.9418        0.7479        0.8869  0.0000  3.8460\n",
            "     27  0.3297       0.8793    0.2029        0.9418        0.7469        0.8871  0.0000  4.6731\n",
            "     28  0.3294       0.8792    0.2027        0.9419        0.7482        0.8871  0.0000  3.8144\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 3/3; 17/36] END lr=0.01, module__dropout=0.5, module__linear_size=600, module__size_emb=60;, score=-1.108 total time= 2.0min\n",
            "[CV 1/3; 18/36] START lr=0.01, module__dropout=0.5, module__linear_size=600, module__size_emb=120\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.3204\u001b[0m       \u001b[32m0.8716\u001b[0m    \u001b[35m0.1962\u001b[0m        \u001b[31m1.0073\u001b[0m        \u001b[94m1.0206\u001b[0m        \u001b[36m1.0146\u001b[0m  0.0100  4.3378\n",
            "      2  \u001b[36m0.5558\u001b[0m       0.8146    \u001b[35m0.4218\u001b[0m        \u001b[31m1.0020\u001b[0m        \u001b[94m0.9055\u001b[0m        \u001b[36m1.0041\u001b[0m  0.0100  3.8739\n",
            "      3  0.3931       0.8596    0.2548        \u001b[31m0.9880\u001b[0m        \u001b[94m0.8933\u001b[0m        \u001b[36m0.9761\u001b[0m  0.0100  4.0206\n",
            "      4  0.3920       0.8486    0.2549        0.9987        \u001b[94m0.8858\u001b[0m        0.9974  0.0100  4.6020\n",
            "      5  0.4198       0.8528    0.2784        \u001b[31m0.9788\u001b[0m        \u001b[94m0.8852\u001b[0m        \u001b[36m0.9580\u001b[0m  0.0100  3.8519\n",
            "      6  0.1636       0.8510    0.0905        1.0170        \u001b[94m0.8850\u001b[0m        1.0343  0.0100  3.8853\n",
            "      7  0.5025       0.8313    0.3601        0.9879        \u001b[94m0.8841\u001b[0m        0.9760  0.0100  4.6657\n",
            "      8  0.3099       \u001b[32m0.8780\u001b[0m    0.1882        0.9836        \u001b[94m0.8373\u001b[0m        0.9674  0.0010  3.8402\n",
            "      9  0.3275       0.8768    0.2013        \u001b[31m0.9765\u001b[0m        \u001b[94m0.8170\u001b[0m        \u001b[36m0.9535\u001b[0m  0.0010  3.8626\n",
            "     10  0.3343       0.8762    0.2065        \u001b[31m0.9752\u001b[0m        \u001b[94m0.8075\u001b[0m        \u001b[36m0.9510\u001b[0m  0.0010  4.6552\n",
            "     11  0.3150       0.8776    0.1919        \u001b[31m0.9747\u001b[0m        \u001b[94m0.7998\u001b[0m        \u001b[36m0.9500\u001b[0m  0.0010  3.8256\n",
            "     12  0.3021       \u001b[32m0.8792\u001b[0m    0.1824        0.9752        \u001b[94m0.7932\u001b[0m        0.9511  0.0010  3.9114\n",
            "     13  0.2743       \u001b[32m0.8814\u001b[0m    0.1624        0.9790        \u001b[94m0.7861\u001b[0m        0.9584  0.0010  4.7897\n",
            "     14  0.3224       0.8704    0.1979        \u001b[31m0.9709\u001b[0m        \u001b[94m0.7815\u001b[0m        \u001b[36m0.9427\u001b[0m  0.0010  3.8916\n",
            "     15  0.3080       0.8744    0.1869        \u001b[31m0.9697\u001b[0m        \u001b[94m0.7753\u001b[0m        \u001b[36m0.9404\u001b[0m  0.0001  3.8351\n",
            "     16  0.3055       0.8747    0.1850        0.9701        \u001b[94m0.7733\u001b[0m        0.9411  0.0001  4.6592\n",
            "     17  0.3093       0.8731    0.1879        \u001b[31m0.9693\u001b[0m        \u001b[94m0.7719\u001b[0m        \u001b[36m0.9395\u001b[0m  0.0001  3.6048\n",
            "     18  0.3010       0.8754    0.1817        0.9697        \u001b[94m0.7697\u001b[0m        0.9403  0.0001  3.8536\n",
            "     19  0.3028       0.8754    0.1831        0.9699        0.7718        0.9407  0.0001  4.7360\n",
            "     20  0.3113       0.8740    0.1893        \u001b[31m0.9687\u001b[0m        \u001b[94m0.7693\u001b[0m        \u001b[36m0.9384\u001b[0m  0.0001  3.8598\n",
            "     21  0.2960       0.8774    0.1781        0.9698        \u001b[94m0.7685\u001b[0m        0.9405  0.0001  3.9153\n",
            "     22  0.2983       0.8772    0.1797        0.9696        \u001b[94m0.7672\u001b[0m        0.9401  0.0000  4.7283\n",
            "     23  0.2991       0.8753    0.1804        0.9694        \u001b[94m0.7664\u001b[0m        0.9397  0.0000  3.9108\n",
            "     24  0.3018       0.8761    0.1823        0.9692        0.7671        0.9394  0.0000  3.9342\n",
            "     25  0.3021       0.8762    0.1825        0.9692        0.7681        0.9393  0.0000  4.7173\n",
            "     26  0.3014       0.8759    0.1820        0.9693        \u001b[94m0.7663\u001b[0m        0.9396  0.0000  4.1065\n",
            "     27  0.3015       0.8760    0.1821        0.9693        0.7683        0.9396  0.0000  3.8624\n",
            "     28  0.3026       0.8764    0.1829        0.9691        0.7670        0.9391  0.0000  4.7597\n",
            "     29  0.3026       0.8764    0.1829        0.9691        0.7670        0.9391  0.0000  3.9273\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 1/3; 18/36] END lr=0.01, module__dropout=0.5, module__linear_size=600, module__size_emb=120;, score=-1.205 total time= 2.1min\n",
            "[CV 2/3; 18/36] START lr=0.01, module__dropout=0.5, module__linear_size=600, module__size_emb=120\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.5464\u001b[0m       \u001b[32m0.8183\u001b[0m    \u001b[35m0.4101\u001b[0m        \u001b[31m0.9593\u001b[0m        \u001b[94m1.0321\u001b[0m        \u001b[36m0.9203\u001b[0m  0.0100  4.5747\n",
            "      2  0.3517       \u001b[32m0.8778\u001b[0m    0.2199        \u001b[31m0.9533\u001b[0m        \u001b[94m0.9029\u001b[0m        \u001b[36m0.9088\u001b[0m  0.0100  3.8617\n",
            "      3  \u001b[36m0.5559\u001b[0m       0.8315    \u001b[35m0.4175\u001b[0m        \u001b[31m0.9444\u001b[0m        \u001b[94m0.8853\u001b[0m        \u001b[36m0.8918\u001b[0m  0.0100  3.8615\n",
            "      4  0.3111       \u001b[32m0.8875\u001b[0m    0.1886        0.9702        \u001b[94m0.8790\u001b[0m        0.9413  0.0100  4.7500\n",
            "      5  0.5447       0.8408    0.4029        0.9492        \u001b[94m0.8744\u001b[0m        0.9009  0.0100  3.8948\n",
            "      6  0.4005       0.8779    0.2594        0.9446        \u001b[94m0.8719\u001b[0m        0.8922  0.0100  3.5854\n",
            "      7  0.3063       \u001b[32m0.8966\u001b[0m    0.1847        0.9535        \u001b[94m0.8717\u001b[0m        0.9092  0.0100  4.6608\n",
            "      8  0.3633       0.8891    0.2283        \u001b[31m0.9410\u001b[0m        \u001b[94m0.8243\u001b[0m        \u001b[36m0.8854\u001b[0m  0.0010  3.8788\n",
            "      9  0.3687       0.8918    0.2324        \u001b[31m0.9376\u001b[0m        \u001b[94m0.7998\u001b[0m        \u001b[36m0.8790\u001b[0m  0.0010  3.8842\n",
            "     10  0.3612       \u001b[32m0.8976\u001b[0m    0.2261        \u001b[31m0.9349\u001b[0m        \u001b[94m0.7888\u001b[0m        \u001b[36m0.8741\u001b[0m  0.0010  4.7584\n",
            "     11  0.3118       \u001b[32m0.9023\u001b[0m    0.1885        0.9377        \u001b[94m0.7827\u001b[0m        0.8793  0.0010  3.8796\n",
            "     12  0.3883       0.8922    0.2482        \u001b[31m0.9283\u001b[0m        \u001b[94m0.7739\u001b[0m        \u001b[36m0.8617\u001b[0m  0.0010  3.8258\n",
            "     13  0.3121       \u001b[32m0.9035\u001b[0m    0.1886        0.9349        \u001b[94m0.7695\u001b[0m        0.8740  0.0010  4.8300\n",
            "     14  0.2966       0.9026    0.1774        0.9344        \u001b[94m0.7622\u001b[0m        0.8731  0.0010  3.9013\n",
            "     15  0.3327       0.9004    0.2040        0.9306        \u001b[94m0.7547\u001b[0m        0.8659  0.0001  3.8281\n",
            "     16  0.3403       0.8995    0.2098        0.9293        \u001b[94m0.7543\u001b[0m        0.8637  0.0001  4.6604\n",
            "     17  0.3377       0.8997    0.2079        0.9294        \u001b[94m0.7531\u001b[0m        0.8637  0.0001  3.8869\n",
            "     18  0.3369       0.9004    0.2073        0.9291        \u001b[94m0.7491\u001b[0m        0.8633  0.0001  3.8524\n",
            "     19  0.3247       0.8996    0.1981        0.9302        \u001b[94m0.7490\u001b[0m        0.8652  0.0001  4.7955\n",
            "     20  0.3257       0.8999    0.1988        0.9296        0.7497        0.8642  0.0001  3.8295\n",
            "     21  0.3212       0.8999    0.1955        0.9300        \u001b[94m0.7484\u001b[0m        0.8649  0.0001  3.8273\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 2/3; 18/36] END lr=0.01, module__dropout=0.5, module__linear_size=600, module__size_emb=120;, score=-1.033 total time= 1.6min\n",
            "[CV 3/3; 18/36] START lr=0.01, module__dropout=0.5, module__linear_size=600, module__size_emb=120\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4959\u001b[0m       \u001b[32m0.8260\u001b[0m    \u001b[35m0.3543\u001b[0m        \u001b[31m0.9616\u001b[0m        \u001b[94m1.0245\u001b[0m        \u001b[36m0.9246\u001b[0m  0.0100  3.9355\n",
            "      2  \u001b[36m0.6269\u001b[0m       0.7953    \u001b[35m0.5174\u001b[0m        0.9658        \u001b[94m0.8816\u001b[0m        0.9327  0.0100  3.9966\n",
            "      3  0.4927       \u001b[32m0.8408\u001b[0m    0.3484        \u001b[31m0.9493\u001b[0m        \u001b[94m0.8752\u001b[0m        \u001b[36m0.9011\u001b[0m  0.0100  4.6401\n",
            "      4  0.2921       \u001b[32m0.8933\u001b[0m    0.1746        0.9665        \u001b[94m0.8664\u001b[0m        0.9342  0.0100  3.6639\n",
            "      5  0.4968       0.8416    0.3525        \u001b[31m0.9478\u001b[0m        \u001b[94m0.8635\u001b[0m        \u001b[36m0.8984\u001b[0m  0.0100  3.8113\n",
            "      6  0.2104       0.8916    0.1192        0.9810        \u001b[94m0.8550\u001b[0m        0.9625  0.0100  4.6977\n",
            "      7  0.4888       0.8490    0.3432        0.9517        0.8561        0.9057  0.0100  3.7887\n",
            "      8  0.3651       0.8823    0.2302        0.9490        \u001b[94m0.8097\u001b[0m        0.9005  0.0010  3.7790\n",
            "      9  0.3293       0.8869    0.2021        \u001b[31m0.9471\u001b[0m        \u001b[94m0.7843\u001b[0m        \u001b[36m0.8970\u001b[0m  0.0010  4.6826\n",
            "     10  0.3538       0.8850    0.2211        \u001b[31m0.9418\u001b[0m        \u001b[94m0.7750\u001b[0m        \u001b[36m0.8870\u001b[0m  0.0010  3.7511\n",
            "     11  0.3469       0.8917    0.2153        \u001b[31m0.9412\u001b[0m        \u001b[94m0.7679\u001b[0m        \u001b[36m0.8859\u001b[0m  0.0010  3.8175\n",
            "     12  0.3777       0.8846    0.2401        \u001b[31m0.9353\u001b[0m        \u001b[94m0.7632\u001b[0m        \u001b[36m0.8747\u001b[0m  0.0010  4.6382\n",
            "     13  0.2933       \u001b[32m0.8978\u001b[0m    0.1753        0.9438        \u001b[94m0.7532\u001b[0m        0.8908  0.0010  3.6291\n",
            "     14  0.3258       0.8929    0.1993        0.9379        \u001b[94m0.7473\u001b[0m        0.8797  0.0010  3.6455\n",
            "     15  0.3371       0.8910    0.2079        0.9359        \u001b[94m0.7380\u001b[0m        0.8760  0.0001  4.4298\n",
            "     16  0.3461       0.8905    0.2148        \u001b[31m0.9347\u001b[0m        0.7382        \u001b[36m0.8737\u001b[0m  0.0001  3.8750\n",
            "     17  0.3495       0.8910    0.2174        \u001b[31m0.9342\u001b[0m        \u001b[94m0.7361\u001b[0m        \u001b[36m0.8727\u001b[0m  0.0001  3.8604\n",
            "     18  0.3376       0.8925    0.2081        0.9353        \u001b[94m0.7347\u001b[0m        0.8749  0.0001  4.1777\n",
            "     19  0.3545       0.8921    0.2212        \u001b[31m0.9330\u001b[0m        \u001b[94m0.7331\u001b[0m        \u001b[36m0.8705\u001b[0m  0.0001  4.1064\n",
            "     20  0.3420       0.8927    0.2115        0.9341        \u001b[94m0.7323\u001b[0m        0.8726  0.0001  3.6864\n",
            "     21  0.3505       0.8933    0.2180        \u001b[31m0.9328\u001b[0m        \u001b[94m0.7318\u001b[0m        \u001b[36m0.8702\u001b[0m  0.0001  3.6522\n",
            "     22  0.3486       0.8929    0.2166        0.9331        0.7324        0.8706  0.0000  4.4548\n",
            "     23  0.3470       0.8931    0.2153        0.9333        0.7331        0.8711  0.0000  3.6762\n",
            "     24  0.3467       0.8936    0.2150        0.9333        \u001b[94m0.7311\u001b[0m        0.8711  0.0000  3.7373\n",
            "     25  0.3462       0.8934    0.2147        0.9333        \u001b[94m0.7296\u001b[0m        0.8711  0.0000  4.5946\n",
            "     26  0.3464       0.8932    0.2149        0.9333        0.7311        0.8711  0.0000  3.7736\n",
            "     27  0.3457       0.8936    0.2143        0.9334        0.7319        0.8712  0.0000  3.7953\n",
            "     28  0.3458       0.8937    0.2144        0.9334        0.7296        0.8712  0.0000  4.5926\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 3/3; 18/36] END lr=0.01, module__dropout=0.5, module__linear_size=600, module__size_emb=120;, score=-1.118 total time= 2.0min\n",
            "[CV 1/3; 19/36] START lr=0.001, module__dropout=0.3, module__linear_size=400, module__size_emb=30\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.3700\u001b[0m       \u001b[32m0.8692\u001b[0m    \u001b[35m0.2350\u001b[0m        \u001b[31m1.0010\u001b[0m        \u001b[94m1.0913\u001b[0m        \u001b[36m1.0021\u001b[0m  0.0010  3.7284\n",
            "      2  \u001b[36m0.4766\u001b[0m       0.8343    \u001b[35m0.3336\u001b[0m        \u001b[31m0.9762\u001b[0m        \u001b[94m0.8958\u001b[0m        \u001b[36m0.9530\u001b[0m  0.0010  4.4244\n",
            "      3  \u001b[36m0.5151\u001b[0m       0.8288    \u001b[35m0.3737\u001b[0m        \u001b[31m0.9735\u001b[0m        \u001b[94m0.8656\u001b[0m        \u001b[36m0.9478\u001b[0m  0.0010  3.6715\n",
            "      4  \u001b[36m0.5248\u001b[0m       0.8248    \u001b[35m0.3848\u001b[0m        \u001b[31m0.9703\u001b[0m        \u001b[94m0.8555\u001b[0m        \u001b[36m0.9415\u001b[0m  0.0010  3.6798\n",
            "      5  0.4442       0.8501    0.3006        \u001b[31m0.9639\u001b[0m        \u001b[94m0.8460\u001b[0m        \u001b[36m0.9291\u001b[0m  0.0010  4.4755\n",
            "      6  0.4880       0.8431    0.3434        \u001b[31m0.9627\u001b[0m        \u001b[94m0.8361\u001b[0m        \u001b[36m0.9269\u001b[0m  0.0010  3.5178\n",
            "      7  0.4315       0.8635    0.2876        \u001b[31m0.9591\u001b[0m        \u001b[94m0.8272\u001b[0m        \u001b[36m0.9198\u001b[0m  0.0010  3.6454\n",
            "      8  \u001b[36m0.5259\u001b[0m       0.8373    0.3833        \u001b[31m0.9573\u001b[0m        \u001b[94m0.8017\u001b[0m        \u001b[36m0.9164\u001b[0m  0.0001  4.4981\n",
            "      9  0.5138       0.8402    0.3700        \u001b[31m0.9564\u001b[0m        \u001b[94m0.7989\u001b[0m        \u001b[36m0.9147\u001b[0m  0.0001  3.3604\n",
            "     10  0.5050       0.8421    0.3606        \u001b[31m0.9556\u001b[0m        \u001b[94m0.7969\u001b[0m        \u001b[36m0.9131\u001b[0m  0.0001  3.6168\n",
            "     11  0.5180       0.8401    0.3744        \u001b[31m0.9553\u001b[0m        \u001b[94m0.7953\u001b[0m        \u001b[36m0.9126\u001b[0m  0.0001  3.9318\n",
            "     12  0.5249       0.8363    0.3825        \u001b[31m0.9551\u001b[0m        \u001b[94m0.7939\u001b[0m        \u001b[36m0.9123\u001b[0m  0.0001  4.0665\n",
            "     13  0.5132       0.8402    0.3694        \u001b[31m0.9547\u001b[0m        \u001b[94m0.7925\u001b[0m        \u001b[36m0.9114\u001b[0m  0.0001  3.7047\n",
            "     14  0.5211       0.8384    0.3781        \u001b[31m0.9542\u001b[0m        \u001b[94m0.7918\u001b[0m        \u001b[36m0.9105\u001b[0m  0.0001  3.6442\n",
            "     15  0.5239       0.8378    0.3811        0.9542        \u001b[94m0.7886\u001b[0m        0.9106  0.0000  4.4939\n",
            "     16  0.5228       0.8377    0.3799        \u001b[31m0.9542\u001b[0m        \u001b[94m0.7876\u001b[0m        \u001b[36m0.9105\u001b[0m  0.0000  3.5877\n",
            "     17  0.5235       0.8371    0.3808        \u001b[31m0.9542\u001b[0m        0.7877        \u001b[36m0.9104\u001b[0m  0.0000  3.5193\n",
            "     18  0.5231       0.8372    0.3804        \u001b[31m0.9541\u001b[0m        0.7880        \u001b[36m0.9103\u001b[0m  0.0000  4.5186\n",
            "     19  0.5216       0.8379    0.3786        \u001b[31m0.9540\u001b[0m        0.7877        \u001b[36m0.9101\u001b[0m  0.0000  3.7025\n",
            "     20  0.5203       0.8386    0.3772        \u001b[31m0.9539\u001b[0m        \u001b[94m0.7873\u001b[0m        \u001b[36m0.9099\u001b[0m  0.0000  3.6594\n",
            "     21  0.5203       0.8379    0.3773        \u001b[31m0.9538\u001b[0m        \u001b[94m0.7870\u001b[0m        \u001b[36m0.9098\u001b[0m  0.0000  4.3853\n",
            "     22  0.5206       0.8379    0.3776        0.9538        \u001b[94m0.7869\u001b[0m        0.9098  0.0000  3.8204\n",
            "     23  0.5206       0.8379    0.3776        0.9538        \u001b[94m0.7867\u001b[0m        0.9098  0.0000  3.6458\n",
            "     24  0.5210       0.8379    0.3781        0.9538        0.7876        0.9098  0.0000  4.1666\n",
            "     25  0.5208       0.8380    0.3778        \u001b[31m0.9538\u001b[0m        \u001b[94m0.7863\u001b[0m        \u001b[36m0.9098\u001b[0m  0.0000  4.1104\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 1/3; 19/36] END lr=0.001, module__dropout=0.3, module__linear_size=400, module__size_emb=30;, score=-1.140 total time= 1.7min\n",
            "[CV 2/3; 19/36] START lr=0.001, module__dropout=0.3, module__linear_size=400, module__size_emb=30\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.5476\u001b[0m       \u001b[32m0.8168\u001b[0m    \u001b[35m0.4118\u001b[0m        \u001b[31m0.9839\u001b[0m        \u001b[94m1.1544\u001b[0m        \u001b[36m0.9681\u001b[0m  0.0010  4.3775\n",
            "      2  0.4063       \u001b[32m0.8606\u001b[0m    0.2659        \u001b[31m0.9597\u001b[0m        \u001b[94m0.9080\u001b[0m        \u001b[36m0.9209\u001b[0m  0.0010  3.6437\n",
            "      3  \u001b[36m0.5632\u001b[0m       0.8177    \u001b[35m0.4295\u001b[0m        \u001b[31m0.9496\u001b[0m        \u001b[94m0.8719\u001b[0m        \u001b[36m0.9017\u001b[0m  0.0010  3.6169\n",
            "      4  0.5527       0.8218    0.4163        \u001b[31m0.9469\u001b[0m        \u001b[94m0.8591\u001b[0m        \u001b[36m0.8967\u001b[0m  0.0010  3.7291\n",
            "      5  0.4642       0.8508    0.3192        \u001b[31m0.9450\u001b[0m        \u001b[94m0.8493\u001b[0m        \u001b[36m0.8931\u001b[0m  0.0010  4.3286\n",
            "      6  0.4943       0.8496    0.3485        \u001b[31m0.9396\u001b[0m        \u001b[94m0.8386\u001b[0m        \u001b[36m0.8829\u001b[0m  0.0010  3.3693\n",
            "      7  0.4966       0.8461    0.3515        \u001b[31m0.9377\u001b[0m        \u001b[94m0.8283\u001b[0m        \u001b[36m0.8792\u001b[0m  0.0010  3.6517\n",
            "      8  0.5332       0.8359    0.3914        \u001b[31m0.9346\u001b[0m        \u001b[94m0.7996\u001b[0m        \u001b[36m0.8736\u001b[0m  0.0001  4.4644\n",
            "      9  0.5443       0.8346    0.4038        \u001b[31m0.9343\u001b[0m        \u001b[94m0.7964\u001b[0m        \u001b[36m0.8728\u001b[0m  0.0001  3.6331\n",
            "     10  0.5404       0.8358    0.3993        \u001b[31m0.9331\u001b[0m        \u001b[94m0.7937\u001b[0m        \u001b[36m0.8706\u001b[0m  0.0001  3.6180\n",
            "     11  0.5462       0.8348    0.4059        \u001b[31m0.9321\u001b[0m        \u001b[94m0.7922\u001b[0m        \u001b[36m0.8687\u001b[0m  0.0001  4.5154\n",
            "     12  0.5372       0.8399    0.3949        \u001b[31m0.9316\u001b[0m        \u001b[94m0.7899\u001b[0m        \u001b[36m0.8678\u001b[0m  0.0001  3.5594\n",
            "     13  0.5238       0.8443    0.3797        \u001b[31m0.9311\u001b[0m        \u001b[94m0.7883\u001b[0m        \u001b[36m0.8670\u001b[0m  0.0001  3.6391\n",
            "     14  0.5320       0.8427    0.3886        \u001b[31m0.9305\u001b[0m        \u001b[94m0.7872\u001b[0m        \u001b[36m0.8658\u001b[0m  0.0001  4.2854\n",
            "     15  0.5356       0.8415    0.3927        \u001b[31m0.9305\u001b[0m        \u001b[94m0.7828\u001b[0m        \u001b[36m0.8657\u001b[0m  0.0000  3.8232\n",
            "     16  0.5355       0.8412    0.3927        0.9305        0.7832        0.8658  0.0000  3.6522\n",
            "     17  0.5327       0.8425    0.3894        \u001b[31m0.9304\u001b[0m        0.7833        \u001b[36m0.8657\u001b[0m  0.0000  3.8876\n",
            "     18  0.5355       0.8415    0.3927        \u001b[31m0.9304\u001b[0m        0.7830        \u001b[36m0.8656\u001b[0m  0.0000  4.2623\n",
            "     19  0.5346       0.8420    0.3916        0.9304        \u001b[94m0.7822\u001b[0m        0.8657  0.0000  3.7368\n",
            "     20  0.5357       0.8417    0.3928        \u001b[31m0.9304\u001b[0m        \u001b[94m0.7821\u001b[0m        \u001b[36m0.8656\u001b[0m  0.0000  3.5946\n",
            "     21  0.5344       0.8419    0.3914        \u001b[31m0.9302\u001b[0m        0.7824        \u001b[36m0.8653\u001b[0m  0.0000  4.5832\n",
            "     22  0.5342       0.8418    0.3912        0.9302        \u001b[94m0.7812\u001b[0m        0.8653  0.0000  3.6774\n",
            "     23  0.5345       0.8418    0.3916        \u001b[31m0.9302\u001b[0m        0.7816        \u001b[36m0.8653\u001b[0m  0.0000  3.5961\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 2/3; 19/36] END lr=0.001, module__dropout=0.3, module__linear_size=400, module__size_emb=30;, score=-1.015 total time= 1.6min\n",
            "[CV 3/3; 19/36] START lr=0.001, module__dropout=0.3, module__linear_size=400, module__size_emb=30\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4135\u001b[0m       \u001b[32m0.8483\u001b[0m    \u001b[35m0.2734\u001b[0m        \u001b[31m0.9882\u001b[0m        \u001b[94m1.1370\u001b[0m        \u001b[36m0.9766\u001b[0m  0.0010  3.4081\n",
            "      2  \u001b[36m0.4680\u001b[0m       0.8356    \u001b[35m0.3250\u001b[0m        \u001b[31m0.9614\u001b[0m        \u001b[94m0.8889\u001b[0m        \u001b[36m0.9242\u001b[0m  0.0010  3.6633\n",
            "      3  0.4224       0.8454    0.2816        \u001b[31m0.9578\u001b[0m        \u001b[94m0.8542\u001b[0m        \u001b[36m0.9174\u001b[0m  0.0010  4.5654\n",
            "      4  0.4264       \u001b[32m0.8489\u001b[0m    0.2847        \u001b[31m0.9542\u001b[0m        \u001b[94m0.8402\u001b[0m        \u001b[36m0.9105\u001b[0m  0.0010  3.7106\n",
            "      5  \u001b[36m0.4985\u001b[0m       0.8377    \u001b[35m0.3548\u001b[0m        \u001b[31m0.9462\u001b[0m        \u001b[94m0.8237\u001b[0m        \u001b[36m0.8952\u001b[0m  0.0010  3.6364\n",
            "      6  \u001b[36m0.5020\u001b[0m       0.8401    \u001b[35m0.3579\u001b[0m        \u001b[31m0.9408\u001b[0m        \u001b[94m0.8131\u001b[0m        \u001b[36m0.8851\u001b[0m  0.0010  4.5816\n",
            "      7  0.5011       0.8381    0.3574        \u001b[31m0.9390\u001b[0m        \u001b[94m0.8054\u001b[0m        \u001b[36m0.8817\u001b[0m  0.0010  3.5954\n",
            "      8  \u001b[36m0.5041\u001b[0m       0.8390    \u001b[35m0.3603\u001b[0m        0.9402        \u001b[94m0.7750\u001b[0m        0.8839  0.0001  3.5904\n",
            "      9  0.4958       0.8441    0.3509        \u001b[31m0.9388\u001b[0m        \u001b[94m0.7722\u001b[0m        \u001b[36m0.8814\u001b[0m  0.0001  4.3011\n",
            "     10  \u001b[36m0.5199\u001b[0m       0.8353    \u001b[35m0.3774\u001b[0m        \u001b[31m0.9371\u001b[0m        \u001b[94m0.7700\u001b[0m        \u001b[36m0.8782\u001b[0m  0.0001  3.8336\n",
            "     11  0.4984       0.8421    0.3539        0.9374        \u001b[94m0.7689\u001b[0m        0.8787  0.0001  3.7103\n",
            "     12  0.4891       0.8459    0.3440        0.9373        \u001b[94m0.7670\u001b[0m        0.8786  0.0001  4.0082\n",
            "     13  0.4956       0.8429    0.3510        \u001b[31m0.9363\u001b[0m        \u001b[94m0.7649\u001b[0m        \u001b[36m0.8766\u001b[0m  0.0001  4.1244\n",
            "     14  0.5072       0.8392    0.3634        \u001b[31m0.9354\u001b[0m        0.7649        \u001b[36m0.8750\u001b[0m  0.0001  3.6294\n",
            "     15  0.5081       0.8389    0.3645        0.9357        \u001b[94m0.7603\u001b[0m        0.8755  0.0000  3.6386\n",
            "     16  0.5059       0.8390    0.3621        0.9357        \u001b[94m0.7599\u001b[0m        0.8756  0.0000  4.5986\n",
            "     17  0.5033       0.8404    0.3592        0.9357        0.7607        0.8756  0.0000  3.6799\n",
            "     18  0.5047       0.8391    0.3609        0.9358        0.7609        0.8756  0.0000  3.5892\n",
            "     19  0.5047       0.8393    0.3609        0.9358        \u001b[94m0.7593\u001b[0m        0.8757  0.0000  4.2674\n",
            "     20  0.5066       0.8388    0.3628        0.9356        0.7606        0.8753  0.0000  3.5832\n",
            "     21  0.5020       0.8404    0.3579        0.9357        0.7600        0.8756  0.0000  3.6378\n",
            "     22  0.5020       0.8401    0.3579        0.9357        0.7598        0.8755  0.0000  4.2938\n",
            "     23  0.5025       0.8398    0.3585        0.9357        \u001b[94m0.7592\u001b[0m        0.8755  0.0000  3.8159\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 3/3; 19/36] END lr=0.001, module__dropout=0.3, module__linear_size=400, module__size_emb=30;, score=-1.095 total time= 1.6min\n",
            "[CV 1/3; 20/36] START lr=0.001, module__dropout=0.3, module__linear_size=400, module__size_emb=60\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4003\u001b[0m       \u001b[32m0.8560\u001b[0m    \u001b[35m0.2612\u001b[0m        \u001b[31m0.9973\u001b[0m        \u001b[94m1.0759\u001b[0m        \u001b[36m0.9947\u001b[0m  0.0010  4.4320\n",
            "      2  \u001b[36m0.4814\u001b[0m       0.8405    \u001b[35m0.3373\u001b[0m        \u001b[31m0.9747\u001b[0m        \u001b[94m0.8969\u001b[0m        \u001b[36m0.9500\u001b[0m  0.0010  3.5800\n",
            "      3  0.4623       0.8416    0.3187        \u001b[31m0.9708\u001b[0m        \u001b[94m0.8677\u001b[0m        \u001b[36m0.9424\u001b[0m  0.0010  3.6857\n",
            "      4  \u001b[36m0.5286\u001b[0m       0.8259    \u001b[35m0.3887\u001b[0m        \u001b[31m0.9658\u001b[0m        \u001b[94m0.8530\u001b[0m        \u001b[36m0.9328\u001b[0m  0.0010  4.2115\n",
            "      5  0.5199       0.8321    0.3781        \u001b[31m0.9637\u001b[0m        \u001b[94m0.8393\u001b[0m        \u001b[36m0.9287\u001b[0m  0.0010  3.7720\n",
            "      6  0.4368       \u001b[32m0.8598\u001b[0m    0.2927        \u001b[31m0.9613\u001b[0m        \u001b[94m0.8300\u001b[0m        \u001b[36m0.9242\u001b[0m  0.0010  3.5397\n",
            "      7  \u001b[36m0.5379\u001b[0m       0.8332    \u001b[35m0.3971\u001b[0m        \u001b[31m0.9573\u001b[0m        \u001b[94m0.8206\u001b[0m        \u001b[36m0.9164\u001b[0m  0.0010  3.8251\n",
            "      8  0.5185       0.8409    0.3748        \u001b[31m0.9536\u001b[0m        \u001b[94m0.7921\u001b[0m        \u001b[36m0.9094\u001b[0m  0.0001  4.3704\n",
            "      9  0.5151       0.8421    0.3710        \u001b[31m0.9525\u001b[0m        \u001b[94m0.7882\u001b[0m        \u001b[36m0.9072\u001b[0m  0.0001  3.6299\n",
            "     10  0.5002       0.8478    0.3547        \u001b[31m0.9517\u001b[0m        \u001b[94m0.7860\u001b[0m        \u001b[36m0.9057\u001b[0m  0.0001  3.6040\n",
            "     11  0.5210       0.8407    0.3774        0.9517        \u001b[94m0.7853\u001b[0m        0.9057  0.0001  4.4698\n",
            "     12  0.5151       0.8436    0.3707        \u001b[31m0.9510\u001b[0m        \u001b[94m0.7828\u001b[0m        \u001b[36m0.9044\u001b[0m  0.0001  3.5492\n",
            "     13  0.5242       0.8387    0.3813        \u001b[31m0.9508\u001b[0m        \u001b[94m0.7819\u001b[0m        \u001b[36m0.9040\u001b[0m  0.0001  3.6339\n",
            "     14  0.5163       0.8452    0.3716        \u001b[31m0.9501\u001b[0m        \u001b[94m0.7812\u001b[0m        \u001b[36m0.9028\u001b[0m  0.0001  4.5221\n",
            "     15  0.5229       0.8413    0.3793        0.9503        \u001b[94m0.7771\u001b[0m        0.9031  0.0000  3.6089\n",
            "     16  0.5241       0.8407    0.3808        0.9502        \u001b[94m0.7765\u001b[0m        0.9029  0.0000  3.3238\n",
            "     17  0.5231       0.8413    0.3796        0.9502        \u001b[94m0.7762\u001b[0m        0.9028  0.0000  3.9553\n",
            "     18  0.5240       0.8394    0.3808        \u001b[31m0.9501\u001b[0m        0.7763        \u001b[36m0.9028\u001b[0m  0.0000  4.0813\n",
            "     19  0.5225       0.8412    0.3790        \u001b[31m0.9500\u001b[0m        0.7765        \u001b[36m0.9026\u001b[0m  0.0000  3.6334\n",
            "     20  0.5220       0.8409    0.3785        \u001b[31m0.9500\u001b[0m        0.7765        \u001b[36m0.9026\u001b[0m  0.0000  3.6217\n",
            "     21  0.5230       0.8404    0.3796        \u001b[31m0.9500\u001b[0m        0.7764        \u001b[36m0.9025\u001b[0m  0.0000  4.4157\n",
            "     22  0.5224       0.8404    0.3790        \u001b[31m0.9500\u001b[0m        \u001b[94m0.7755\u001b[0m        \u001b[36m0.9024\u001b[0m  0.0000  3.5559\n",
            "     23  0.5231       0.8402    0.3798        0.9500        \u001b[94m0.7755\u001b[0m        0.9025  0.0000  3.6655\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 1/3; 20/36] END lr=0.001, module__dropout=0.3, module__linear_size=400, module__size_emb=60;, score=-1.143 total time= 1.6min\n",
            "[CV 2/3; 20/36] START lr=0.001, module__dropout=0.3, module__linear_size=400, module__size_emb=60\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.5052\u001b[0m       \u001b[32m0.8383\u001b[0m    \u001b[35m0.3615\u001b[0m        \u001b[31m0.9801\u001b[0m        \u001b[94m1.1363\u001b[0m        \u001b[36m0.9607\u001b[0m  0.0010  3.6156\n",
            "      2  0.4355       \u001b[32m0.8553\u001b[0m    0.2921        \u001b[31m0.9531\u001b[0m        \u001b[94m0.9032\u001b[0m        \u001b[36m0.9083\u001b[0m  0.0010  3.5777\n",
            "      3  0.4947       0.8418    0.3502        \u001b[31m0.9478\u001b[0m        \u001b[94m0.8679\u001b[0m        \u001b[36m0.8983\u001b[0m  0.0010  4.4696\n",
            "      4  0.4812       0.8495    0.3356        \u001b[31m0.9435\u001b[0m        \u001b[94m0.8565\u001b[0m        \u001b[36m0.8901\u001b[0m  0.0010  3.6225\n",
            "      5  0.4577       0.8552    0.3124        \u001b[31m0.9389\u001b[0m        \u001b[94m0.8402\u001b[0m        \u001b[36m0.8815\u001b[0m  0.0010  3.7142\n",
            "      6  0.4980       0.8497    0.3522        \u001b[31m0.9349\u001b[0m        \u001b[94m0.8286\u001b[0m        \u001b[36m0.8740\u001b[0m  0.0010  4.4471\n",
            "      7  0.4948       0.8524    0.3485        \u001b[31m0.9311\u001b[0m        \u001b[94m0.8170\u001b[0m        \u001b[36m0.8669\u001b[0m  0.0010  3.6256\n",
            "      8  \u001b[36m0.5418\u001b[0m       0.8389    \u001b[35m0.4001\u001b[0m        \u001b[31m0.9300\u001b[0m        \u001b[94m0.7839\u001b[0m        \u001b[36m0.8649\u001b[0m  0.0001  3.5814\n",
            "      9  \u001b[36m0.5491\u001b[0m       0.8375    \u001b[35m0.4084\u001b[0m        \u001b[31m0.9289\u001b[0m        \u001b[94m0.7792\u001b[0m        \u001b[36m0.8629\u001b[0m  0.0001  3.9269\n",
            "     10  0.5234       0.8442    0.3793        \u001b[31m0.9281\u001b[0m        \u001b[94m0.7759\u001b[0m        \u001b[36m0.8614\u001b[0m  0.0001  3.8820\n",
            "     11  0.5482       0.8384    0.4073        \u001b[31m0.9273\u001b[0m        \u001b[94m0.7740\u001b[0m        \u001b[36m0.8599\u001b[0m  0.0001  3.6933\n",
            "     12  0.5282       0.8427    0.3846        \u001b[31m0.9268\u001b[0m        \u001b[94m0.7722\u001b[0m        \u001b[36m0.8590\u001b[0m  0.0001  3.8042\n",
            "     13  0.5400       0.8423    0.3974        0.9269        \u001b[94m0.7700\u001b[0m        0.8591  0.0001  4.2551\n",
            "     14  0.5486       0.8401    0.4073        \u001b[31m0.9258\u001b[0m        \u001b[94m0.7688\u001b[0m        \u001b[36m0.8571\u001b[0m  0.0001  3.7609\n",
            "     15  0.5388       0.8428    0.3960        0.9258        \u001b[94m0.7659\u001b[0m        0.8571  0.0000  3.6786\n",
            "     16  0.5366       0.8431    0.3936        0.9259        \u001b[94m0.7651\u001b[0m        0.8574  0.0000  4.4055\n",
            "     17  0.5387       0.8428    0.3959        0.9260        \u001b[94m0.7641\u001b[0m        0.8574  0.0000  3.5616\n",
            "     18  0.5397       0.8424    0.3970        0.9259        0.7650        0.8573  0.0000  3.5946\n",
            "     19  0.5360       0.8426    0.3930        0.9259        0.7655        0.8572  0.0000  4.3752\n",
            "     20  0.5404       0.8428    0.3978        0.9259        \u001b[94m0.7636\u001b[0m        0.8572  0.0000  3.5976\n",
            "     21  0.5376       0.8430    0.3946        0.9258        0.7639        0.8571  0.0000  3.6344\n",
            "     22  0.5383       0.8428    0.3954        0.9258        \u001b[94m0.7629\u001b[0m        0.8571  0.0000  4.2371\n",
            "     23  0.5391       0.8428    0.3963        0.9258        0.7635        0.8571  0.0000  3.8827\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 2/3; 20/36] END lr=0.001, module__dropout=0.3, module__linear_size=400, module__size_emb=60;, score=-1.025 total time= 1.6min\n",
            "[CV 3/3; 20/36] START lr=0.001, module__dropout=0.3, module__linear_size=400, module__size_emb=60\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4592\u001b[0m       \u001b[32m0.8418\u001b[0m    \u001b[35m0.3158\u001b[0m        \u001b[31m0.9821\u001b[0m        \u001b[94m1.1150\u001b[0m        \u001b[36m0.9646\u001b[0m  0.0010  4.2384\n",
            "      2  \u001b[36m0.5056\u001b[0m       0.8254    \u001b[35m0.3644\u001b[0m        \u001b[31m0.9575\u001b[0m        \u001b[94m0.8866\u001b[0m        \u001b[36m0.9167\u001b[0m  0.0010  3.7502\n",
            "      3  \u001b[36m0.5294\u001b[0m       0.8254    \u001b[35m0.3897\u001b[0m        \u001b[31m0.9546\u001b[0m        \u001b[94m0.8523\u001b[0m        \u001b[36m0.9112\u001b[0m  0.0010  3.5904\n",
            "      4  \u001b[36m0.5349\u001b[0m       0.8262    \u001b[35m0.3954\u001b[0m        \u001b[31m0.9460\u001b[0m        \u001b[94m0.8340\u001b[0m        \u001b[36m0.8950\u001b[0m  0.0010  4.0541\n",
            "      5  \u001b[36m0.5537\u001b[0m       0.8233    \u001b[35m0.4171\u001b[0m        \u001b[31m0.9433\u001b[0m        \u001b[94m0.8203\u001b[0m        \u001b[36m0.8899\u001b[0m  0.0010  3.9937\n",
            "      6  0.5300       0.8298    0.3893        \u001b[31m0.9395\u001b[0m        \u001b[94m0.8075\u001b[0m        \u001b[36m0.8827\u001b[0m  0.0010  3.6334\n",
            "      7  0.4587       \u001b[32m0.8546\u001b[0m    0.3135        \u001b[31m0.9381\u001b[0m        \u001b[94m0.8006\u001b[0m        \u001b[36m0.8801\u001b[0m  0.0010  3.7779\n",
            "      8  0.5177       0.8403    0.3740        \u001b[31m0.9365\u001b[0m        \u001b[94m0.7688\u001b[0m        \u001b[36m0.8770\u001b[0m  0.0001  4.4085\n",
            "      9  0.5190       0.8404    0.3754        \u001b[31m0.9354\u001b[0m        \u001b[94m0.7656\u001b[0m        \u001b[36m0.8750\u001b[0m  0.0001  3.6495\n",
            "     10  0.5052       0.8446    0.3604        \u001b[31m0.9346\u001b[0m        \u001b[94m0.7635\u001b[0m        \u001b[36m0.8735\u001b[0m  0.0001  3.5697\n",
            "     11  0.4924       0.8471    0.3471        \u001b[31m0.9345\u001b[0m        \u001b[94m0.7613\u001b[0m        \u001b[36m0.8733\u001b[0m  0.0001  4.5033\n",
            "     12  0.5038       0.8452    0.3588        \u001b[31m0.9338\u001b[0m        \u001b[94m0.7586\u001b[0m        \u001b[36m0.8720\u001b[0m  0.0001  3.6287\n",
            "     13  0.5168       0.8442    0.3724        \u001b[31m0.9328\u001b[0m        \u001b[94m0.7574\u001b[0m        \u001b[36m0.8701\u001b[0m  0.0001  3.6297\n",
            "     14  0.5198       0.8419    0.3760        \u001b[31m0.9328\u001b[0m        \u001b[94m0.7563\u001b[0m        \u001b[36m0.8701\u001b[0m  0.0001  4.5103\n",
            "     15  0.5086       0.8446    0.3638        0.9329        \u001b[94m0.7531\u001b[0m        0.8703  0.0000  3.6616\n",
            "     16  0.5097       0.8441    0.3651        0.9328        \u001b[94m0.7529\u001b[0m        0.8701  0.0000  3.6286\n",
            "     17  0.5117       0.8443    0.3671        \u001b[31m0.9327\u001b[0m        \u001b[94m0.7525\u001b[0m        \u001b[36m0.8699\u001b[0m  0.0000  4.1939\n",
            "     18  0.5125       0.8435    0.3680        \u001b[31m0.9326\u001b[0m        0.7526        \u001b[36m0.8697\u001b[0m  0.0000  3.9062\n",
            "     19  0.5148       0.8433    0.3705        \u001b[31m0.9326\u001b[0m        \u001b[94m0.7517\u001b[0m        \u001b[36m0.8697\u001b[0m  0.0000  3.6421\n",
            "     20  0.5124       0.8443    0.3679        \u001b[31m0.9325\u001b[0m        \u001b[94m0.7514\u001b[0m        \u001b[36m0.8696\u001b[0m  0.0000  3.9471\n",
            "     21  0.5081       0.8446    0.3634        0.9326        0.7524        0.8697  0.0000  4.2196\n",
            "     22  0.5083       0.8446    0.3636        0.9326        \u001b[94m0.7505\u001b[0m        0.8697  0.0000  3.6713\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 3/3; 20/36] END lr=0.001, module__dropout=0.3, module__linear_size=400, module__size_emb=60;, score=-1.099 total time= 1.5min\n",
            "[CV 1/3; 21/36] START lr=0.001, module__dropout=0.3, module__linear_size=400, module__size_emb=120\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4810\u001b[0m       \u001b[32m0.8370\u001b[0m    \u001b[35m0.3374\u001b[0m        \u001b[31m0.9977\u001b[0m        \u001b[94m1.0656\u001b[0m        \u001b[36m0.9953\u001b[0m  0.0010  3.5142\n",
            "      2  0.4647       \u001b[32m0.8421\u001b[0m    0.3209        \u001b[31m0.9744\u001b[0m        \u001b[94m0.8919\u001b[0m        \u001b[36m0.9495\u001b[0m  0.0010  3.6652\n",
            "      3  0.4172       \u001b[32m0.8522\u001b[0m    0.2762        \u001b[31m0.9687\u001b[0m        \u001b[94m0.8609\u001b[0m        \u001b[36m0.9383\u001b[0m  0.0010  3.7792\n",
            "      4  \u001b[36m0.5458\u001b[0m       0.8272    \u001b[35m0.4073\u001b[0m        \u001b[31m0.9625\u001b[0m        \u001b[94m0.8429\u001b[0m        \u001b[36m0.9265\u001b[0m  0.0010  4.2835\n",
            "      5  0.5008       0.8418    0.3564        \u001b[31m0.9588\u001b[0m        \u001b[94m0.8308\u001b[0m        \u001b[36m0.9193\u001b[0m  0.0010  3.6682\n",
            "      6  0.4743       \u001b[32m0.8536\u001b[0m    0.3284        \u001b[31m0.9564\u001b[0m        \u001b[94m0.8216\u001b[0m        \u001b[36m0.9148\u001b[0m  0.0010  3.6674\n",
            "      7  0.5071       0.8430    0.3626        0.9569        \u001b[94m0.8128\u001b[0m        0.9157  0.0010  4.4855\n",
            "      8  0.5016       0.8458    0.3565        \u001b[31m0.9502\u001b[0m        \u001b[94m0.7832\u001b[0m        \u001b[36m0.9029\u001b[0m  0.0001  3.6158\n",
            "      9  0.5164       0.8418    0.3724        \u001b[31m0.9491\u001b[0m        \u001b[94m0.7774\u001b[0m        \u001b[36m0.9008\u001b[0m  0.0001  3.5898\n",
            "     10  0.5080       0.8444    0.3633        \u001b[31m0.9481\u001b[0m        \u001b[94m0.7759\u001b[0m        \u001b[36m0.8989\u001b[0m  0.0001  4.5097\n",
            "     11  0.5380       0.8388    0.3960        \u001b[31m0.9479\u001b[0m        \u001b[94m0.7731\u001b[0m        \u001b[36m0.8986\u001b[0m  0.0001  3.6876\n",
            "     12  0.4951       0.8496    0.3493        \u001b[31m0.9464\u001b[0m        \u001b[94m0.7710\u001b[0m        \u001b[36m0.8957\u001b[0m  0.0001  3.6536\n",
            "     13  0.5322       0.8403    0.3894        \u001b[31m0.9461\u001b[0m        \u001b[94m0.7680\u001b[0m        \u001b[36m0.8951\u001b[0m  0.0001  4.1880\n",
            "     14  0.5253       0.8434    0.3815        \u001b[31m0.9452\u001b[0m        \u001b[94m0.7665\u001b[0m        \u001b[36m0.8935\u001b[0m  0.0001  3.8216\n",
            "     15  0.5329       0.8441    0.3893        0.9453        \u001b[94m0.7622\u001b[0m        0.8936  0.0000  3.6512\n",
            "     16  0.5305       0.8450    0.3866        \u001b[31m0.9452\u001b[0m        \u001b[94m0.7606\u001b[0m        \u001b[36m0.8934\u001b[0m  0.0000  3.9457\n",
            "     17  0.5269       0.8451    0.3828        \u001b[31m0.9451\u001b[0m        0.7614        \u001b[36m0.8932\u001b[0m  0.0000  4.2424\n",
            "     18  0.5290       0.8450    0.3850        \u001b[31m0.9451\u001b[0m        0.7621        \u001b[36m0.8932\u001b[0m  0.0000  3.6614\n",
            "     19  0.5264       0.8450    0.3823        \u001b[31m0.9450\u001b[0m        0.7614        \u001b[36m0.8929\u001b[0m  0.0000  3.7078\n",
            "     20  0.5294       0.8448    0.3855        \u001b[31m0.9449\u001b[0m        \u001b[94m0.7595\u001b[0m        \u001b[36m0.8929\u001b[0m  0.0000  4.3232\n",
            "     21  0.5308       0.8451    0.3869        \u001b[31m0.9449\u001b[0m        0.7606        \u001b[36m0.8928\u001b[0m  0.0000  3.6632\n",
            "     22  0.5303       0.8451    0.3864        \u001b[31m0.9449\u001b[0m        \u001b[94m0.7594\u001b[0m        \u001b[36m0.8928\u001b[0m  0.0000  3.6612\n",
            "     23  0.5302       0.8449    0.3863        \u001b[31m0.9449\u001b[0m        0.7600        \u001b[36m0.8928\u001b[0m  0.0000  4.5473\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 1/3; 21/36] END lr=0.001, module__dropout=0.3, module__linear_size=400, module__size_emb=120;, score=-1.146 total time= 1.6min\n",
            "[CV 2/3; 21/36] START lr=0.001, module__dropout=0.3, module__linear_size=400, module__size_emb=120\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4834\u001b[0m       \u001b[32m0.8409\u001b[0m    \u001b[35m0.3392\u001b[0m        \u001b[31m0.9697\u001b[0m        \u001b[94m1.1112\u001b[0m        \u001b[36m0.9403\u001b[0m  0.0010  3.8830\n",
            "      2  0.3942       \u001b[32m0.8650\u001b[0m    0.2552        \u001b[31m0.9534\u001b[0m        \u001b[94m0.8906\u001b[0m        \u001b[36m0.9090\u001b[0m  0.0010  4.5602\n",
            "      3  0.4784       0.8493    0.3329        \u001b[31m0.9424\u001b[0m        \u001b[94m0.8622\u001b[0m        \u001b[36m0.8882\u001b[0m  0.0010  3.6946\n",
            "      4  \u001b[36m0.5038\u001b[0m       0.8446    \u001b[35m0.3590\u001b[0m        \u001b[31m0.9374\u001b[0m        \u001b[94m0.8418\u001b[0m        \u001b[36m0.8787\u001b[0m  0.0010  3.6999\n",
            "      5  \u001b[36m0.5363\u001b[0m       0.8399    \u001b[35m0.3939\u001b[0m        \u001b[31m0.9317\u001b[0m        \u001b[94m0.8286\u001b[0m        \u001b[36m0.8681\u001b[0m  0.0010  4.4521\n",
            "      6  0.5092       0.8473    0.3640        \u001b[31m0.9298\u001b[0m        \u001b[94m0.8179\u001b[0m        \u001b[36m0.8645\u001b[0m  0.0010  3.5801\n",
            "      7  \u001b[36m0.6084\u001b[0m       0.8277    \u001b[35m0.4809\u001b[0m        \u001b[31m0.9296\u001b[0m        \u001b[94m0.8069\u001b[0m        \u001b[36m0.8642\u001b[0m  0.0010  3.6990\n",
            "      8  0.5253       0.8510    0.3799        \u001b[31m0.9249\u001b[0m        \u001b[94m0.7711\u001b[0m        \u001b[36m0.8554\u001b[0m  0.0001  4.4369\n",
            "      9  0.5310       0.8494    0.3862        \u001b[31m0.9229\u001b[0m        \u001b[94m0.7659\u001b[0m        \u001b[36m0.8518\u001b[0m  0.0001  3.5754\n",
            "     10  0.5346       0.8500    0.3900        \u001b[31m0.9225\u001b[0m        \u001b[94m0.7634\u001b[0m        \u001b[36m0.8509\u001b[0m  0.0001  3.6744\n",
            "     11  0.5405       0.8491    0.3964        \u001b[31m0.9222\u001b[0m        \u001b[94m0.7611\u001b[0m        \u001b[36m0.8505\u001b[0m  0.0001  4.0066\n",
            "     12  0.5320       0.8506    0.3870        \u001b[31m0.9212\u001b[0m        \u001b[94m0.7583\u001b[0m        \u001b[36m0.8486\u001b[0m  0.0001  4.1043\n",
            "     13  0.5170       0.8550    0.3705        \u001b[31m0.9208\u001b[0m        \u001b[94m0.7568\u001b[0m        \u001b[36m0.8478\u001b[0m  0.0001  3.4262\n",
            "     14  0.5262       0.8530    0.3804        \u001b[31m0.9190\u001b[0m        \u001b[94m0.7534\u001b[0m        \u001b[36m0.8446\u001b[0m  0.0001  3.9420\n",
            "     15  0.5380       0.8493    0.3936        0.9192        \u001b[94m0.7480\u001b[0m        0.8449  0.0000  4.2153\n",
            "     16  0.5393       0.8483    0.3953        0.9194        0.7490        0.8452  0.0000  3.6411\n",
            "     17  0.5391       0.8488    0.3950        0.9193        0.7484        0.8452  0.0000  3.6368\n",
            "     18  0.5373       0.8496    0.3929        0.9193        \u001b[94m0.7473\u001b[0m        0.8451  0.0000  4.5762\n",
            "     19  0.5358       0.8506    0.3910        0.9192        \u001b[94m0.7472\u001b[0m        0.8450  0.0000  3.7153\n",
            "     20  0.5376       0.8500    0.3931        0.9192        \u001b[94m0.7471\u001b[0m        0.8449  0.0000  3.6314\n",
            "     21  0.5389       0.8501    0.3944        \u001b[31m0.9189\u001b[0m        \u001b[94m0.7466\u001b[0m        \u001b[36m0.8444\u001b[0m  0.0000  4.3686\n",
            "     22  0.5385       0.8500    0.3941        0.9189        0.7466        0.8444  0.0000  3.7098\n",
            "     23  0.5379       0.8498    0.3935        0.9189        0.7469        0.8445  0.0000  3.7003\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 2/3; 21/36] END lr=0.001, module__dropout=0.3, module__linear_size=400, module__size_emb=120;, score=-1.026 total time= 1.6min\n",
            "[CV 3/3; 21/36] START lr=0.001, module__dropout=0.3, module__linear_size=400, module__size_emb=120\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4495\u001b[0m       \u001b[32m0.8404\u001b[0m    \u001b[35m0.3068\u001b[0m        \u001b[31m0.9780\u001b[0m        \u001b[94m1.0892\u001b[0m        \u001b[36m0.9564\u001b[0m  0.0010  3.6991\n",
            "      2  \u001b[36m0.5066\u001b[0m       0.8288    \u001b[35m0.3648\u001b[0m        \u001b[31m0.9583\u001b[0m        \u001b[94m0.8754\u001b[0m        \u001b[36m0.9184\u001b[0m  0.0010  3.6881\n",
            "      3  0.4201       \u001b[32m0.8538\u001b[0m    0.2786        \u001b[31m0.9514\u001b[0m        \u001b[94m0.8450\u001b[0m        \u001b[36m0.9051\u001b[0m  0.0010  4.4927\n",
            "      4  \u001b[36m0.5139\u001b[0m       0.8352    \u001b[35m0.3712\u001b[0m        \u001b[31m0.9482\u001b[0m        \u001b[94m0.8295\u001b[0m        \u001b[36m0.8991\u001b[0m  0.0010  3.6123\n",
            "      5  0.4251       \u001b[32m0.8581\u001b[0m    0.2825        \u001b[31m0.9438\u001b[0m        \u001b[94m0.8150\u001b[0m        \u001b[36m0.8907\u001b[0m  0.0010  3.7203\n",
            "      6  0.4259       \u001b[32m0.8649\u001b[0m    0.2825        \u001b[31m0.9422\u001b[0m        \u001b[94m0.8025\u001b[0m        \u001b[36m0.8878\u001b[0m  0.0010  4.1226\n",
            "      7  0.4744       0.8494    0.3291        \u001b[31m0.9386\u001b[0m        \u001b[94m0.7947\u001b[0m        \u001b[36m0.8809\u001b[0m  0.0010  3.9240\n",
            "      8  0.5083       0.8428    0.3639        \u001b[31m0.9341\u001b[0m        \u001b[94m0.7608\u001b[0m        \u001b[36m0.8725\u001b[0m  0.0001  3.6052\n",
            "      9  0.4865       0.8491    0.3409        0.9343        \u001b[94m0.7560\u001b[0m        0.8729  0.0001  3.7703\n",
            "     10  0.5045       0.8431    0.3599        \u001b[31m0.9329\u001b[0m        \u001b[94m0.7529\u001b[0m        \u001b[36m0.8704\u001b[0m  0.0001  4.2351\n",
            "     11  0.5069       0.8425    0.3625        \u001b[31m0.9317\u001b[0m        \u001b[94m0.7509\u001b[0m        \u001b[36m0.8680\u001b[0m  0.0001  3.6477\n",
            "     12  0.4971       0.8459    0.3519        0.9320        \u001b[94m0.7484\u001b[0m        0.8687  0.0001  3.5527\n",
            "     13  0.4966       0.8468    0.3513        \u001b[31m0.9298\u001b[0m        \u001b[94m0.7462\u001b[0m        \u001b[36m0.8646\u001b[0m  0.0001  4.4760\n",
            "     14  0.4984       0.8466    0.3532        \u001b[31m0.9296\u001b[0m        \u001b[94m0.7439\u001b[0m        \u001b[36m0.8642\u001b[0m  0.0001  3.6253\n",
            "     15  0.5039       0.8444    0.3591        \u001b[31m0.9294\u001b[0m        \u001b[94m0.7378\u001b[0m        \u001b[36m0.8637\u001b[0m  0.0000  3.6753\n",
            "     16  0.5017       0.8445    0.3568        \u001b[31m0.9292\u001b[0m        0.7386        \u001b[36m0.8634\u001b[0m  0.0000  4.4839\n",
            "     17  0.5050       0.8446    0.3602        \u001b[31m0.9291\u001b[0m        0.7386        \u001b[36m0.8632\u001b[0m  0.0000  3.5969\n",
            "     18  0.5031       0.8458    0.3580        \u001b[31m0.9289\u001b[0m        0.7378        \u001b[36m0.8628\u001b[0m  0.0000  3.6226\n",
            "     19  0.5042       0.8450    0.3593        0.9290        \u001b[94m0.7373\u001b[0m        0.8630  0.0000  4.0952\n",
            "     20  0.5052       0.8446    0.3604        \u001b[31m0.9288\u001b[0m        \u001b[94m0.7367\u001b[0m        \u001b[36m0.8626\u001b[0m  0.0000  4.0410\n",
            "     21  0.5045       0.8456    0.3594        \u001b[31m0.9287\u001b[0m        \u001b[94m0.7357\u001b[0m        \u001b[36m0.8625\u001b[0m  0.0000  3.7253\n",
            "     22  0.5043       0.8456    0.3593        0.9287        0.7373        0.8625  0.0000  3.6871\n",
            "     23  0.5037       0.8455    0.3587        0.9287        0.7359        0.8625  0.0000  4.3592\n",
            "     24  0.5041       0.8457    0.3591        0.9287        0.7358        0.8625  0.0000  3.5837\n",
            "     25  0.5041       0.8457    0.3591        \u001b[31m0.9287\u001b[0m        0.7362        \u001b[36m0.8624\u001b[0m  0.0000  3.7101\n",
            "     26  0.5040       0.8460    0.3589        0.9287        \u001b[94m0.7356\u001b[0m        0.8625  0.0000  4.6106\n",
            "     27  0.5035       0.8464    0.3583        0.9287        0.7365        0.8625  0.0000  3.6859\n",
            "     28  0.5032       0.8463    0.3580        \u001b[31m0.9287\u001b[0m        0.7365        \u001b[36m0.8624\u001b[0m  0.0000  3.6508\n",
            "     29  0.5032       0.8463    0.3580        \u001b[31m0.9287\u001b[0m        \u001b[94m0.7350\u001b[0m        \u001b[36m0.8624\u001b[0m  0.0000  4.4513\n",
            "     30  0.5032       0.8463    0.3580        0.9287        0.7356        0.8624  0.0000  3.6650\n",
            "[CV 3/3; 21/36] END lr=0.001, module__dropout=0.3, module__linear_size=400, module__size_emb=120;, score=-1.112 total time= 2.0min\n",
            "[CV 1/3; 22/36] START lr=0.001, module__dropout=0.3, module__linear_size=500, module__size_emb=30\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4291\u001b[0m       \u001b[32m0.8436\u001b[0m    \u001b[35m0.2877\u001b[0m        \u001b[31m1.0015\u001b[0m        \u001b[94m1.0932\u001b[0m        \u001b[36m1.0031\u001b[0m  0.0010  3.4273\n",
            "      2  \u001b[36m0.4800\u001b[0m       0.8353    \u001b[35m0.3368\u001b[0m        \u001b[31m0.9786\u001b[0m        \u001b[94m0.8932\u001b[0m        \u001b[36m0.9576\u001b[0m  0.0010  4.5099\n",
            "      3  \u001b[36m0.5551\u001b[0m       0.8116    \u001b[35m0.4218\u001b[0m        0.9789        \u001b[94m0.8647\u001b[0m        0.9582  0.0010  3.5853\n",
            "      4  0.4920       0.8324    0.3491        \u001b[31m0.9724\u001b[0m        \u001b[94m0.8553\u001b[0m        \u001b[36m0.9456\u001b[0m  0.0010  3.5907\n",
            "      5  0.4813       0.8419    0.3370        \u001b[31m0.9652\u001b[0m        \u001b[94m0.8434\u001b[0m        \u001b[36m0.9316\u001b[0m  0.0010  4.5326\n",
            "      6  \u001b[36m0.5671\u001b[0m       0.8177    \u001b[35m0.4340\u001b[0m        0.9667        \u001b[94m0.8326\u001b[0m        0.9344  0.0010  3.6299\n",
            "      7  0.4970       0.8422    0.3526        \u001b[31m0.9594\u001b[0m        \u001b[94m0.8241\u001b[0m        \u001b[36m0.9205\u001b[0m  0.0010  3.5776\n",
            "      8  0.5262       0.8344    0.3842        \u001b[31m0.9581\u001b[0m        \u001b[94m0.7975\u001b[0m        \u001b[36m0.9179\u001b[0m  0.0001  4.0093\n",
            "      9  0.5003       0.8410    0.3560        \u001b[31m0.9567\u001b[0m        \u001b[94m0.7947\u001b[0m        \u001b[36m0.9154\u001b[0m  0.0001  4.0528\n",
            "     10  0.5230       0.8349    0.3808        \u001b[31m0.9566\u001b[0m        \u001b[94m0.7921\u001b[0m        \u001b[36m0.9151\u001b[0m  0.0001  3.6498\n",
            "     11  0.5110       0.8386    0.3675        \u001b[31m0.9560\u001b[0m        \u001b[94m0.7903\u001b[0m        \u001b[36m0.9138\u001b[0m  0.0001  3.7214\n",
            "     12  0.5263       0.8361    0.3840        \u001b[31m0.9559\u001b[0m        \u001b[94m0.7885\u001b[0m        \u001b[36m0.9138\u001b[0m  0.0001  4.4361\n",
            "     13  0.5261       0.8348    0.3841        \u001b[31m0.9558\u001b[0m        \u001b[94m0.7880\u001b[0m        \u001b[36m0.9135\u001b[0m  0.0001  3.5629\n",
            "     14  0.5382       0.8313    0.3979        0.9559        \u001b[94m0.7873\u001b[0m        0.9138  0.0001  3.5659\n",
            "     15  0.5272       0.8356    0.3850        \u001b[31m0.9554\u001b[0m        \u001b[94m0.7834\u001b[0m        \u001b[36m0.9127\u001b[0m  0.0000  4.4610\n",
            "     16  0.5255       0.8364    0.3831        \u001b[31m0.9553\u001b[0m        \u001b[94m0.7832\u001b[0m        \u001b[36m0.9125\u001b[0m  0.0000  3.6712\n",
            "     17  0.5274       0.8347    0.3855        0.9553        0.7842        0.9126  0.0000  3.6508\n",
            "     18  0.5284       0.8346    0.3866        0.9553        \u001b[94m0.7829\u001b[0m        0.9125  0.0000  4.5712\n",
            "     19  0.5284       0.8348    0.3865        \u001b[31m0.9553\u001b[0m        0.7833        \u001b[36m0.9125\u001b[0m  0.0000  3.3431\n",
            "     20  0.5277       0.8346    0.3859        \u001b[31m0.9552\u001b[0m        0.7831        \u001b[36m0.9124\u001b[0m  0.0000  3.5819\n",
            "     21  0.5253       0.8362    0.3829        \u001b[31m0.9550\u001b[0m        \u001b[94m0.7826\u001b[0m        \u001b[36m0.9121\u001b[0m  0.0000  4.0556\n",
            "     22  0.5253       0.8368    0.3828        \u001b[31m0.9550\u001b[0m        0.7828        \u001b[36m0.9120\u001b[0m  0.0000  4.1172\n",
            "     23  0.5253       0.8362    0.3829        0.9550        \u001b[94m0.7825\u001b[0m        0.9121  0.0000  3.6075\n",
            "     24  0.5258       0.8357    0.3835        0.9551        \u001b[94m0.7816\u001b[0m        0.9121  0.0000  3.6719\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 1/3; 22/36] END lr=0.001, module__dropout=0.3, module__linear_size=500, module__size_emb=30;, score=-1.156 total time= 1.7min\n",
            "[CV 2/3; 22/36] START lr=0.001, module__dropout=0.3, module__linear_size=500, module__size_emb=30\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.5399\u001b[0m       \u001b[32m0.8230\u001b[0m    \u001b[35m0.4017\u001b[0m        \u001b[31m0.9802\u001b[0m        \u001b[94m1.1559\u001b[0m        \u001b[36m0.9607\u001b[0m  0.0010  3.6470\n",
            "      2  0.4845       \u001b[32m0.8420\u001b[0m    0.3401        \u001b[31m0.9520\u001b[0m        \u001b[94m0.9044\u001b[0m        \u001b[36m0.9064\u001b[0m  0.0010  4.0471\n",
            "      3  0.4500       \u001b[32m0.8547\u001b[0m    0.3054        \u001b[31m0.9487\u001b[0m        \u001b[94m0.8696\u001b[0m        \u001b[36m0.9001\u001b[0m  0.0010  4.0421\n",
            "      4  0.4104       \u001b[32m0.8633\u001b[0m    0.2692        0.9531        \u001b[94m0.8552\u001b[0m        0.9083  0.0010  3.5673\n",
            "      5  0.4577       0.8560    0.3124        \u001b[31m0.9397\u001b[0m        \u001b[94m0.8416\u001b[0m        \u001b[36m0.8830\u001b[0m  0.0010  3.6244\n",
            "      6  \u001b[36m0.5461\u001b[0m       0.8359    \u001b[35m0.4055\u001b[0m        \u001b[31m0.9352\u001b[0m        \u001b[94m0.8314\u001b[0m        \u001b[36m0.8746\u001b[0m  0.0010  4.4547\n",
            "      7  \u001b[36m0.5655\u001b[0m       0.8320    \u001b[35m0.4283\u001b[0m        \u001b[31m0.9315\u001b[0m        \u001b[94m0.8179\u001b[0m        \u001b[36m0.8677\u001b[0m  0.0010  3.7071\n",
            "      8  0.5378       0.8428    0.3949        \u001b[31m0.9304\u001b[0m        \u001b[94m0.7882\u001b[0m        \u001b[36m0.8657\u001b[0m  0.0001  3.7256\n",
            "      9  0.5548       0.8383    0.4146        \u001b[31m0.9301\u001b[0m        \u001b[94m0.7838\u001b[0m        \u001b[36m0.8650\u001b[0m  0.0001  4.5803\n",
            "     10  0.5293       0.8459    0.3851        \u001b[31m0.9289\u001b[0m        \u001b[94m0.7817\u001b[0m        \u001b[36m0.8628\u001b[0m  0.0001  3.6742\n",
            "     11  0.5381       0.8436    0.3951        \u001b[31m0.9283\u001b[0m        \u001b[94m0.7791\u001b[0m        \u001b[36m0.8617\u001b[0m  0.0001  3.6752\n",
            "     12  0.5237       0.8465    0.3791        \u001b[31m0.9276\u001b[0m        \u001b[94m0.7781\u001b[0m        \u001b[36m0.8604\u001b[0m  0.0001  4.4106\n",
            "     13  0.5243       0.8462    0.3798        \u001b[31m0.9271\u001b[0m        \u001b[94m0.7746\u001b[0m        \u001b[36m0.8594\u001b[0m  0.0001  3.6914\n",
            "     14  0.5124       0.8503    0.3667        \u001b[31m0.9270\u001b[0m        \u001b[94m0.7740\u001b[0m        \u001b[36m0.8593\u001b[0m  0.0001  3.7526\n",
            "     15  0.5377       0.8426    0.3948        \u001b[31m0.9263\u001b[0m        \u001b[94m0.7699\u001b[0m        \u001b[36m0.8581\u001b[0m  0.0000  4.0178\n",
            "     16  0.5345       0.8439    0.3911        \u001b[31m0.9263\u001b[0m        \u001b[94m0.7686\u001b[0m        \u001b[36m0.8581\u001b[0m  0.0000  3.8687\n",
            "     17  0.5388       0.8430    0.3960        \u001b[31m0.9263\u001b[0m        \u001b[94m0.7681\u001b[0m        \u001b[36m0.8580\u001b[0m  0.0000  3.6881\n",
            "     18  0.5355       0.8428    0.3925        \u001b[31m0.9261\u001b[0m        0.7683        \u001b[36m0.8577\u001b[0m  0.0000  3.6952\n",
            "     19  0.5361       0.8428    0.3930        \u001b[31m0.9260\u001b[0m        0.7691        \u001b[36m0.8576\u001b[0m  0.0000  4.4249\n",
            "     20  0.5373       0.8422    0.3945        \u001b[31m0.9260\u001b[0m        0.7684        \u001b[36m0.8575\u001b[0m  0.0000  3.6137\n",
            "     21  0.5373       0.8424    0.3944        \u001b[31m0.9260\u001b[0m        0.7687        \u001b[36m0.8575\u001b[0m  0.0000  3.6530\n",
            "     22  0.5377       0.8426    0.3948        0.9260        \u001b[94m0.7677\u001b[0m        0.8575  0.0000  4.4564\n",
            "     23  0.5375       0.8427    0.3946        0.9260        \u001b[94m0.7673\u001b[0m        0.8575  0.0000  3.6077\n",
            "     24  0.5371       0.8425    0.3942        0.9260        0.7680        0.8575  0.0000  3.6918\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 2/3; 22/36] END lr=0.001, module__dropout=0.3, module__linear_size=500, module__size_emb=30;, score=-1.018 total time= 1.7min\n",
            "[CV 3/3; 22/36] START lr=0.001, module__dropout=0.3, module__linear_size=500, module__size_emb=30\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4444\u001b[0m       \u001b[32m0.8406\u001b[0m    \u001b[35m0.3021\u001b[0m        \u001b[31m0.9811\u001b[0m        \u001b[94m1.1189\u001b[0m        \u001b[36m0.9626\u001b[0m  0.0010  3.6777\n",
            "      2  \u001b[36m0.4451\u001b[0m       \u001b[32m0.8434\u001b[0m    \u001b[35m0.3023\u001b[0m        \u001b[31m0.9583\u001b[0m        \u001b[94m0.8805\u001b[0m        \u001b[36m0.9184\u001b[0m  0.0010  3.6483\n",
            "      3  0.4286       \u001b[32m0.8451\u001b[0m    0.2871        0.9584        \u001b[94m0.8542\u001b[0m        0.9185  0.0010  4.5288\n",
            "      4  \u001b[36m0.5657\u001b[0m       0.8155    \u001b[35m0.4330\u001b[0m        \u001b[31m0.9510\u001b[0m        \u001b[94m0.8401\u001b[0m        \u001b[36m0.9045\u001b[0m  0.0010  3.6811\n",
            "      5  \u001b[36m0.5770\u001b[0m       0.8155    \u001b[35m0.4465\u001b[0m        \u001b[31m0.9478\u001b[0m        \u001b[94m0.8269\u001b[0m        \u001b[36m0.8984\u001b[0m  0.0010  3.6230\n",
            "      6  0.5442       0.8258    0.4058        \u001b[31m0.9435\u001b[0m        \u001b[94m0.8172\u001b[0m        \u001b[36m0.8901\u001b[0m  0.0010  4.5464\n",
            "      7  0.5198       0.8351    0.3774        \u001b[31m0.9403\u001b[0m        \u001b[94m0.8063\u001b[0m        \u001b[36m0.8842\u001b[0m  0.0010  3.3500\n",
            "      8  0.5028       0.8409    0.3586        \u001b[31m0.9403\u001b[0m        \u001b[94m0.7802\u001b[0m        \u001b[36m0.8842\u001b[0m  0.0001  3.6487\n",
            "      9  0.4861       \u001b[32m0.8470\u001b[0m    0.3409        0.9409        \u001b[94m0.7765\u001b[0m        0.8853  0.0001  4.0084\n",
            "     10  0.4834       \u001b[32m0.8500\u001b[0m    0.3378        \u001b[31m0.9400\u001b[0m        \u001b[94m0.7752\u001b[0m        \u001b[36m0.8836\u001b[0m  0.0001  4.1269\n",
            "     11  0.5267       0.8371    0.3842        \u001b[31m0.9376\u001b[0m        \u001b[94m0.7742\u001b[0m        \u001b[36m0.8791\u001b[0m  0.0001  3.6882\n",
            "     12  0.5070       0.8434    0.3625        \u001b[31m0.9372\u001b[0m        \u001b[94m0.7712\u001b[0m        \u001b[36m0.8783\u001b[0m  0.0001  3.5631\n",
            "     13  0.5006       0.8478    0.3551        0.9372        \u001b[94m0.7701\u001b[0m        0.8784  0.0001  4.5541\n",
            "     14  0.5059       0.8460    0.3609        \u001b[31m0.9363\u001b[0m        \u001b[94m0.7693\u001b[0m        \u001b[36m0.8767\u001b[0m  0.0001  3.6718\n",
            "     15  0.5079       0.8445    0.3631        \u001b[31m0.9362\u001b[0m        \u001b[94m0.7648\u001b[0m        \u001b[36m0.8764\u001b[0m  0.0000  3.6288\n",
            "     16  0.5063       0.8466    0.3611        0.9362        \u001b[94m0.7640\u001b[0m        0.8765  0.0000  4.5294\n",
            "     17  0.5044       0.8475    0.3591        0.9362        0.7641        0.8765  0.0000  3.6498\n",
            "     18  0.5071       0.8455    0.3621        \u001b[31m0.9360\u001b[0m        \u001b[94m0.7640\u001b[0m        \u001b[36m0.8761\u001b[0m  0.0000  3.6523\n",
            "     19  0.5066       0.8460    0.3615        \u001b[31m0.9358\u001b[0m        \u001b[94m0.7640\u001b[0m        \u001b[36m0.8756\u001b[0m  0.0000  4.5935\n",
            "     20  0.5086       0.8444    0.3639        \u001b[31m0.9357\u001b[0m        \u001b[94m0.7638\u001b[0m        \u001b[36m0.8756\u001b[0m  0.0000  3.6994\n",
            "     21  0.5074       0.8460    0.3624        \u001b[31m0.9357\u001b[0m        \u001b[94m0.7635\u001b[0m        \u001b[36m0.8755\u001b[0m  0.0000  3.6793\n",
            "     22  0.5069       0.8463    0.3618        0.9357        \u001b[94m0.7629\u001b[0m        0.8755  0.0000  3.9720\n",
            "     23  0.5071       0.8460    0.3620        \u001b[31m0.9357\u001b[0m        \u001b[94m0.7625\u001b[0m        \u001b[36m0.8755\u001b[0m  0.0000  4.0473\n",
            "     24  0.5074       0.8455    0.3625        \u001b[31m0.9356\u001b[0m        0.7628        \u001b[36m0.8754\u001b[0m  0.0000  3.7003\n",
            "     25  0.5073       0.8456    0.3623        0.9357        0.7634        0.8755  0.0000  3.4244\n",
            "     26  0.5074       0.8458    0.3624        0.9357        0.7630        0.8755  0.0000  4.4183\n",
            "     27  0.5075       0.8455    0.3626        0.9357        0.7633        0.8755  0.0000  3.6648\n",
            "     28  0.5080       0.8455    0.3630        0.9357        0.7628        0.8755  0.0000  3.5777\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 3/3; 22/36] END lr=0.001, module__dropout=0.3, module__linear_size=500, module__size_emb=30;, score=-1.087 total time= 1.9min\n",
            "[CV 1/3; 23/36] START lr=0.001, module__dropout=0.3, module__linear_size=500, module__size_emb=60\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4446\u001b[0m       \u001b[32m0.8432\u001b[0m    \u001b[35m0.3019\u001b[0m        \u001b[31m0.9950\u001b[0m        \u001b[94m1.0674\u001b[0m        \u001b[36m0.9900\u001b[0m  0.0010  3.6314\n",
            "      2  \u001b[36m0.4813\u001b[0m       0.8371    \u001b[35m0.3378\u001b[0m        \u001b[31m0.9735\u001b[0m        \u001b[94m0.8919\u001b[0m        \u001b[36m0.9477\u001b[0m  0.0010  3.6606\n",
            "      3  \u001b[36m0.4997\u001b[0m       0.8360    \u001b[35m0.3563\u001b[0m        \u001b[31m0.9690\u001b[0m        \u001b[94m0.8636\u001b[0m        \u001b[36m0.9389\u001b[0m  0.0010  4.5464\n",
            "      4  \u001b[36m0.5351\u001b[0m       0.8248    \u001b[35m0.3961\u001b[0m        \u001b[31m0.9648\u001b[0m        \u001b[94m0.8504\u001b[0m        \u001b[36m0.9308\u001b[0m  0.0010  3.6311\n",
            "      5  0.4653       \u001b[32m0.8518\u001b[0m    0.3201        \u001b[31m0.9614\u001b[0m        \u001b[94m0.8376\u001b[0m        \u001b[36m0.9243\u001b[0m  0.0010  3.6174\n",
            "      6  \u001b[36m0.5394\u001b[0m       0.8303    \u001b[35m0.3995\u001b[0m        \u001b[31m0.9566\u001b[0m        \u001b[94m0.8257\u001b[0m        \u001b[36m0.9152\u001b[0m  0.0010  4.5329\n",
            "      7  0.5219       0.8382    0.3789        \u001b[31m0.9527\u001b[0m        \u001b[94m0.8182\u001b[0m        \u001b[36m0.9076\u001b[0m  0.0010  3.6976\n",
            "      8  0.5260       0.8407    0.3827        \u001b[31m0.9515\u001b[0m        \u001b[94m0.7891\u001b[0m        \u001b[36m0.9053\u001b[0m  0.0001  3.5600\n",
            "      9  0.5081       0.8459    0.3631        \u001b[31m0.9500\u001b[0m        \u001b[94m0.7846\u001b[0m        \u001b[36m0.9025\u001b[0m  0.0001  4.4350\n",
            "     10  0.5353       0.8367    0.3936        \u001b[31m0.9499\u001b[0m        \u001b[94m0.7838\u001b[0m        \u001b[36m0.9023\u001b[0m  0.0001  3.6621\n",
            "     11  0.5295       0.8404    0.3865        \u001b[31m0.9494\u001b[0m        \u001b[94m0.7803\u001b[0m        \u001b[36m0.9013\u001b[0m  0.0001  3.6446\n",
            "     12  0.5215       0.8424    0.3776        \u001b[31m0.9485\u001b[0m        \u001b[94m0.7799\u001b[0m        \u001b[36m0.8996\u001b[0m  0.0001  4.1348\n",
            "     13  0.5298       0.8394    0.3870        \u001b[31m0.9481\u001b[0m        \u001b[94m0.7782\u001b[0m        \u001b[36m0.8988\u001b[0m  0.0001  4.0337\n",
            "     14  0.5150       0.8422    0.3709        \u001b[31m0.9471\u001b[0m        \u001b[94m0.7760\u001b[0m        \u001b[36m0.8970\u001b[0m  0.0001  3.6603\n",
            "     15  0.5279       0.8395    0.3850        0.9473        \u001b[94m0.7731\u001b[0m        0.8974  0.0000  3.7357\n",
            "     16  0.5279       0.8394    0.3850        0.9473        0.7733        0.8973  0.0000  4.4725\n",
            "     17  0.5328       0.8363    0.3910        0.9474        \u001b[94m0.7726\u001b[0m        0.8975  0.0000  3.7409\n",
            "     18  0.5303       0.8389    0.3877        0.9473        0.7726        0.8973  0.0000  3.4488\n",
            "     19  0.5296       0.8394    0.3868        0.9472        0.7730        0.8972  0.0000  4.4412\n",
            "     20  0.5303       0.8389    0.3877        0.9472        0.7729        0.8972  0.0000  3.5806\n",
            "     21  0.5321       0.8372    0.3900        0.9472        \u001b[94m0.7724\u001b[0m        0.8972  0.0000  3.6926\n",
            "     22  0.5319       0.8376    0.3897        0.9472        \u001b[94m0.7717\u001b[0m        0.8972  0.0000  4.5777\n",
            "     23  0.5312       0.8379    0.3888        0.9472        0.7718        0.8971  0.0000  3.7314\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 1/3; 23/36] END lr=0.001, module__dropout=0.3, module__linear_size=500, module__size_emb=60;, score=-1.141 total time= 1.6min\n",
            "[CV 2/3; 23/36] START lr=0.001, module__dropout=0.3, module__linear_size=500, module__size_emb=60\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4818\u001b[0m       \u001b[32m0.8398\u001b[0m    \u001b[35m0.3378\u001b[0m        \u001b[31m0.9743\u001b[0m        \u001b[94m1.1323\u001b[0m        \u001b[36m0.9493\u001b[0m  0.0010  4.4716\n",
            "      2  \u001b[36m0.5178\u001b[0m       0.8355    \u001b[35m0.3751\u001b[0m        \u001b[31m0.9520\u001b[0m        \u001b[94m0.8991\u001b[0m        \u001b[36m0.9063\u001b[0m  0.0010  3.6763\n",
            "      3  \u001b[36m0.5412\u001b[0m       0.8290    \u001b[35m0.4018\u001b[0m        \u001b[31m0.9485\u001b[0m        \u001b[94m0.8672\u001b[0m        \u001b[36m0.8996\u001b[0m  0.0010  3.6329\n",
            "      4  0.5336       0.8294    0.3934        \u001b[31m0.9430\u001b[0m        \u001b[94m0.8534\u001b[0m        \u001b[36m0.8893\u001b[0m  0.0010  4.5598\n",
            "      5  0.4731       \u001b[32m0.8551\u001b[0m    0.3270        \u001b[31m0.9389\u001b[0m        \u001b[94m0.8416\u001b[0m        \u001b[36m0.8816\u001b[0m  0.0010  3.6002\n",
            "      6  0.4295       \u001b[32m0.8702\u001b[0m    0.2851        \u001b[31m0.9376\u001b[0m        \u001b[94m0.8264\u001b[0m        \u001b[36m0.8791\u001b[0m  0.0010  3.5905\n",
            "      7  \u001b[36m0.5529\u001b[0m       0.8345    \u001b[35m0.4134\u001b[0m        \u001b[31m0.9303\u001b[0m        \u001b[94m0.8165\u001b[0m        \u001b[36m0.8655\u001b[0m  0.0010  4.1792\n",
            "      8  0.5370       0.8424    0.3942        \u001b[31m0.9295\u001b[0m        \u001b[94m0.7808\u001b[0m        \u001b[36m0.8640\u001b[0m  0.0001  4.0081\n",
            "      9  0.5409       0.8407    0.3987        \u001b[31m0.9288\u001b[0m        \u001b[94m0.7775\u001b[0m        \u001b[36m0.8626\u001b[0m  0.0001  3.6692\n",
            "     10  0.5514       0.8392    0.4106        \u001b[31m0.9276\u001b[0m        \u001b[94m0.7758\u001b[0m        \u001b[36m0.8605\u001b[0m  0.0001  3.5239\n",
            "     11  0.5409       0.8426    0.3983        \u001b[31m0.9272\u001b[0m        \u001b[94m0.7730\u001b[0m        \u001b[36m0.8596\u001b[0m  0.0001  4.3695\n",
            "     12  0.5387       0.8434    0.3957        \u001b[31m0.9265\u001b[0m        \u001b[94m0.7711\u001b[0m        \u001b[36m0.8584\u001b[0m  0.0001  3.5704\n",
            "     13  0.5497       0.8413    0.4082        \u001b[31m0.9264\u001b[0m        \u001b[94m0.7687\u001b[0m        \u001b[36m0.8582\u001b[0m  0.0001  3.6546\n",
            "     14  0.5448       0.8445    0.4021        \u001b[31m0.9257\u001b[0m        \u001b[94m0.7681\u001b[0m        \u001b[36m0.8570\u001b[0m  0.0001  4.6364\n",
            "     15  0.5442       0.8451    0.4013        \u001b[31m0.9256\u001b[0m        \u001b[94m0.7631\u001b[0m        \u001b[36m0.8568\u001b[0m  0.0000  3.6575\n",
            "     16  0.5448       0.8448    0.4021        0.9257        0.7633        0.8569  0.0000  3.6149\n",
            "     17  0.5452       0.8442    0.4026        \u001b[31m0.9256\u001b[0m        0.7634        \u001b[36m0.8567\u001b[0m  0.0000  4.5370\n",
            "     18  0.5396       0.8456    0.3962        0.9256        0.7632        0.8567  0.0000  3.6619\n",
            "     19  0.5408       0.8456    0.3975        \u001b[31m0.9255\u001b[0m        \u001b[94m0.7625\u001b[0m        \u001b[36m0.8566\u001b[0m  0.0000  3.6927\n",
            "     20  0.5435       0.8447    0.4006        \u001b[31m0.9255\u001b[0m        0.7625        \u001b[36m0.8565\u001b[0m  0.0000  4.3490\n",
            "     21  0.5405       0.8452    0.3973        \u001b[31m0.9255\u001b[0m        \u001b[94m0.7623\u001b[0m        \u001b[36m0.8565\u001b[0m  0.0000  3.8426\n",
            "     22  0.5408       0.8451    0.3977        \u001b[31m0.9255\u001b[0m        \u001b[94m0.7622\u001b[0m        \u001b[36m0.8565\u001b[0m  0.0000  3.6560\n",
            "     23  0.5415       0.8449    0.3984        \u001b[31m0.9255\u001b[0m        \u001b[94m0.7611\u001b[0m        \u001b[36m0.8565\u001b[0m  0.0000  3.9747\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 2/3; 23/36] END lr=0.001, module__dropout=0.3, module__linear_size=500, module__size_emb=60;, score=-1.028 total time= 1.6min\n",
            "[CV 3/3; 23/36] START lr=0.001, module__dropout=0.3, module__linear_size=500, module__size_emb=60\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4226\u001b[0m       \u001b[32m0.8440\u001b[0m    \u001b[35m0.2818\u001b[0m        \u001b[31m0.9798\u001b[0m        \u001b[94m1.1069\u001b[0m        \u001b[36m0.9599\u001b[0m  0.0010  3.6261\n",
            "      2  \u001b[36m0.5120\u001b[0m       0.8267    \u001b[35m0.3708\u001b[0m        \u001b[31m0.9578\u001b[0m        \u001b[94m0.8793\u001b[0m        \u001b[36m0.9173\u001b[0m  0.0010  4.2367\n",
            "      3  0.3845       \u001b[32m0.8598\u001b[0m    0.2476        \u001b[31m0.9570\u001b[0m        \u001b[94m0.8489\u001b[0m        \u001b[36m0.9158\u001b[0m  0.0010  3.7670\n",
            "      4  0.5013       0.8320    0.3587        \u001b[31m0.9465\u001b[0m        \u001b[94m0.8318\u001b[0m        \u001b[36m0.8959\u001b[0m  0.0010  3.6190\n",
            "      5  0.4799       0.8438    0.3353        \u001b[31m0.9430\u001b[0m        \u001b[94m0.8161\u001b[0m        \u001b[36m0.8892\u001b[0m  0.0010  3.6812\n",
            "      6  0.3873       \u001b[32m0.8676\u001b[0m    0.2493        0.9461        \u001b[94m0.8062\u001b[0m        0.8951  0.0010  4.2357\n",
            "      7  0.5118       0.8374    0.3685        \u001b[31m0.9376\u001b[0m        \u001b[94m0.7973\u001b[0m        \u001b[36m0.8792\u001b[0m  0.0010  3.6057\n",
            "      8  0.4916       0.8445    0.3467        \u001b[31m0.9356\u001b[0m        \u001b[94m0.7657\u001b[0m        \u001b[36m0.8754\u001b[0m  0.0001  3.5888\n",
            "      9  0.5115       0.8388    0.3679        \u001b[31m0.9342\u001b[0m        \u001b[94m0.7590\u001b[0m        \u001b[36m0.8727\u001b[0m  0.0001  4.4638\n",
            "     10  0.5063       0.8405    0.3623        \u001b[31m0.9327\u001b[0m        \u001b[94m0.7580\u001b[0m        \u001b[36m0.8700\u001b[0m  0.0001  3.6499\n",
            "     11  0.5008       0.8437    0.3561        \u001b[31m0.9316\u001b[0m        \u001b[94m0.7549\u001b[0m        \u001b[36m0.8679\u001b[0m  0.0001  3.7765\n",
            "     12  0.5040       0.8425    0.3595        \u001b[31m0.9306\u001b[0m        \u001b[94m0.7526\u001b[0m        \u001b[36m0.8660\u001b[0m  0.0001  4.4507\n",
            "     13  0.4863       0.8504    0.3405        0.9306        \u001b[94m0.7506\u001b[0m        0.8660  0.0001  3.7636\n",
            "     14  0.4950       0.8461    0.3499        \u001b[31m0.9299\u001b[0m        \u001b[94m0.7483\u001b[0m        \u001b[36m0.8648\u001b[0m  0.0001  3.7191\n",
            "     15  0.5065       0.8422    0.3621        \u001b[31m0.9296\u001b[0m        \u001b[94m0.7429\u001b[0m        \u001b[36m0.8641\u001b[0m  0.0000  4.4109\n",
            "     16  0.5046       0.8425    0.3602        \u001b[31m0.9295\u001b[0m        \u001b[94m0.7426\u001b[0m        \u001b[36m0.8640\u001b[0m  0.0000  3.8946\n",
            "     17  0.5076       0.8423    0.3633        \u001b[31m0.9295\u001b[0m        0.7432        \u001b[36m0.8639\u001b[0m  0.0000  3.6161\n",
            "     18  0.5089       0.8424    0.3645        \u001b[31m0.9294\u001b[0m        \u001b[94m0.7418\u001b[0m        \u001b[36m0.8638\u001b[0m  0.0000  3.9852\n",
            "     19  0.5072       0.8424    0.3628        \u001b[31m0.9293\u001b[0m        \u001b[94m0.7408\u001b[0m        \u001b[36m0.8635\u001b[0m  0.0000  4.2046\n",
            "     20  0.5055       0.8420    0.3611        \u001b[31m0.9292\u001b[0m        0.7410        \u001b[36m0.8635\u001b[0m  0.0000  3.6299\n",
            "     21  0.5083       0.8428    0.3639        \u001b[31m0.9291\u001b[0m        0.7414        \u001b[36m0.8632\u001b[0m  0.0000  3.4037\n",
            "     22  0.5083       0.8428    0.3638        \u001b[31m0.9291\u001b[0m        \u001b[94m0.7404\u001b[0m        \u001b[36m0.8632\u001b[0m  0.0000  4.5082\n",
            "     23  0.5081       0.8427    0.3637        0.9291        0.7415        0.8632  0.0000  3.5697\n",
            "     24  0.5078       0.8426    0.3634        0.9291        0.7405        0.8632  0.0000  3.6892\n",
            "     25  0.5072       0.8424    0.3628        0.9291        0.7406        0.8632  0.0000  4.5100\n",
            "     26  0.5072       0.8424    0.3628        0.9291        0.7411        0.8632  0.0000  3.7269\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 3/3; 23/36] END lr=0.001, module__dropout=0.3, module__linear_size=500, module__size_emb=60;, score=-1.103 total time= 1.8min\n",
            "[CV 1/3; 24/36] START lr=0.001, module__dropout=0.3, module__linear_size=500, module__size_emb=120\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.3859\u001b[0m       \u001b[32m0.8609\u001b[0m    \u001b[35m0.2487\u001b[0m        \u001b[31m0.9992\u001b[0m        \u001b[94m1.0558\u001b[0m        \u001b[36m0.9984\u001b[0m  0.0010  4.5805\n",
            "      2  \u001b[36m0.4814\u001b[0m       0.8368    \u001b[35m0.3379\u001b[0m        \u001b[31m0.9749\u001b[0m        \u001b[94m0.8905\u001b[0m        \u001b[36m0.9505\u001b[0m  0.0010  3.6604\n",
            "      3  \u001b[36m0.5147\u001b[0m       0.8308    \u001b[35m0.3729\u001b[0m        \u001b[31m0.9691\u001b[0m        \u001b[94m0.8623\u001b[0m        \u001b[36m0.9391\u001b[0m  0.0010  3.6569\n",
            "      4  0.4827       0.8438    0.3380        \u001b[31m0.9601\u001b[0m        \u001b[94m0.8433\u001b[0m        \u001b[36m0.9217\u001b[0m  0.0010  4.5459\n",
            "      5  \u001b[36m0.5412\u001b[0m       0.8278    \u001b[35m0.4021\u001b[0m        \u001b[31m0.9576\u001b[0m        \u001b[94m0.8279\u001b[0m        \u001b[36m0.9170\u001b[0m  0.0010  3.7251\n",
            "      6  0.5287       0.8381    0.3861        \u001b[31m0.9537\u001b[0m        \u001b[94m0.8198\u001b[0m        \u001b[36m0.9096\u001b[0m  0.0010  3.7986\n",
            "      7  \u001b[36m0.5586\u001b[0m       0.8333    \u001b[35m0.4201\u001b[0m        \u001b[31m0.9514\u001b[0m        \u001b[94m0.8121\u001b[0m        \u001b[36m0.9052\u001b[0m  0.0010  4.5164\n",
            "      8  0.5120       0.8449    0.3673        \u001b[31m0.9466\u001b[0m        \u001b[94m0.7785\u001b[0m        \u001b[36m0.8960\u001b[0m  0.0001  3.7646\n",
            "      9  0.5199       0.8419    0.3761        \u001b[31m0.9455\u001b[0m        \u001b[94m0.7733\u001b[0m        \u001b[36m0.8940\u001b[0m  0.0001  3.7917\n",
            "     10  0.5167       0.8441    0.3722        \u001b[31m0.9440\u001b[0m        \u001b[94m0.7699\u001b[0m        \u001b[36m0.8912\u001b[0m  0.0001  4.2972\n",
            "     11  0.5086       0.8474    0.3634        \u001b[31m0.9429\u001b[0m        \u001b[94m0.7680\u001b[0m        \u001b[36m0.8890\u001b[0m  0.0001  3.5354\n",
            "     12  0.5197       0.8448    0.3753        \u001b[31m0.9425\u001b[0m        \u001b[94m0.7649\u001b[0m        \u001b[36m0.8883\u001b[0m  0.0001  3.6588\n",
            "     13  0.5210       0.8478    0.3760        \u001b[31m0.9415\u001b[0m        \u001b[94m0.7630\u001b[0m        \u001b[36m0.8864\u001b[0m  0.0001  3.7954\n",
            "     14  0.5158       0.8493    0.3704        \u001b[31m0.9404\u001b[0m        \u001b[94m0.7616\u001b[0m        \u001b[36m0.8844\u001b[0m  0.0001  4.3410\n",
            "     15  0.5274       0.8438    0.3835        \u001b[31m0.9404\u001b[0m        \u001b[94m0.7558\u001b[0m        \u001b[36m0.8844\u001b[0m  0.0000  3.6593\n",
            "     16  0.5328       0.8438    0.3893        \u001b[31m0.9403\u001b[0m        \u001b[94m0.7553\u001b[0m        \u001b[36m0.8842\u001b[0m  0.0000  3.6757\n",
            "     17  0.5341       0.8436    0.3908        \u001b[31m0.9402\u001b[0m        \u001b[94m0.7545\u001b[0m        \u001b[36m0.8840\u001b[0m  0.0000  4.4836\n",
            "     18  0.5341       0.8436    0.3908        \u001b[31m0.9401\u001b[0m        \u001b[94m0.7543\u001b[0m        \u001b[36m0.8838\u001b[0m  0.0000  3.7294\n",
            "     19  0.5354       0.8434    0.3921        \u001b[31m0.9400\u001b[0m        0.7543        \u001b[36m0.8837\u001b[0m  0.0000  3.7101\n",
            "     20  0.5364       0.8432    0.3933        \u001b[31m0.9400\u001b[0m        \u001b[94m0.7532\u001b[0m        \u001b[36m0.8835\u001b[0m  0.0000  4.5660\n",
            "     21  0.5352       0.8437    0.3919        \u001b[31m0.9399\u001b[0m        0.7533        \u001b[36m0.8834\u001b[0m  0.0000  3.7519\n",
            "     22  0.5339       0.8443    0.3904        \u001b[31m0.9398\u001b[0m        0.7534        \u001b[36m0.8833\u001b[0m  0.0000  3.5844\n",
            "     23  0.5339       0.8440    0.3904        \u001b[31m0.9398\u001b[0m        \u001b[94m0.7531\u001b[0m        \u001b[36m0.8833\u001b[0m  0.0000  4.3529\n",
            "     24  0.5341       0.8442    0.3906        \u001b[31m0.9398\u001b[0m        \u001b[94m0.7529\u001b[0m        \u001b[36m0.8833\u001b[0m  0.0000  3.8669\n",
            "     25  0.5338       0.8441    0.3903        \u001b[31m0.9398\u001b[0m        0.7533        \u001b[36m0.8832\u001b[0m  0.0000  3.7348\n",
            "     26  0.5339       0.8443    0.3904        \u001b[31m0.9398\u001b[0m        0.7531        \u001b[36m0.8832\u001b[0m  0.0000  4.1746\n",
            "     27  0.5340       0.8444    0.3905        \u001b[31m0.9398\u001b[0m        \u001b[94m0.7525\u001b[0m        \u001b[36m0.8832\u001b[0m  0.0000  4.0597\n",
            "     28  0.5338       0.8444    0.3902        \u001b[31m0.9398\u001b[0m        0.7528        \u001b[36m0.8832\u001b[0m  0.0000  3.7185\n",
            "     29  0.5339       0.8448    0.3902        \u001b[31m0.9398\u001b[0m        0.7526        \u001b[36m0.8832\u001b[0m  0.0000  3.7931\n",
            "     30  0.5340       0.8445    0.3904        0.9398        0.7533        0.8832  0.0000  4.7295\n",
            "[CV 1/3; 24/36] END lr=0.001, module__dropout=0.3, module__linear_size=500, module__size_emb=120;, score=-1.138 total time= 2.0min\n",
            "[CV 2/3; 24/36] START lr=0.001, module__dropout=0.3, module__linear_size=500, module__size_emb=120\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.5401\u001b[0m       \u001b[32m0.8278\u001b[0m    \u001b[35m0.4008\u001b[0m        \u001b[31m0.9716\u001b[0m        \u001b[94m1.0928\u001b[0m        \u001b[36m0.9441\u001b[0m  0.0010  3.6910\n",
            "      2  0.5284       0.8278    0.3881        \u001b[31m0.9510\u001b[0m        \u001b[94m0.8924\u001b[0m        \u001b[36m0.9043\u001b[0m  0.0010  4.4167\n",
            "      3  0.4794       \u001b[32m0.8515\u001b[0m    0.3336        \u001b[31m0.9435\u001b[0m        \u001b[94m0.8590\u001b[0m        \u001b[36m0.8903\u001b[0m  0.0010  3.7490\n",
            "      4  0.5243       0.8406    0.3809        \u001b[31m0.9393\u001b[0m        \u001b[94m0.8426\u001b[0m        \u001b[36m0.8823\u001b[0m  0.0010  3.7426\n",
            "      5  0.5286       0.8434    0.3849        \u001b[31m0.9338\u001b[0m        \u001b[94m0.8299\u001b[0m        \u001b[36m0.8720\u001b[0m  0.0010  4.2003\n",
            "      6  0.4324       \u001b[32m0.8732\u001b[0m    0.2874        \u001b[31m0.9329\u001b[0m        \u001b[94m0.8176\u001b[0m        \u001b[36m0.8703\u001b[0m  0.0010  4.1292\n",
            "      7  0.4733       0.8697    0.3252        \u001b[31m0.9263\u001b[0m        \u001b[94m0.8059\u001b[0m        \u001b[36m0.8581\u001b[0m  0.0010  3.6937\n",
            "      8  0.5384       0.8529    0.3934        \u001b[31m0.9241\u001b[0m        \u001b[94m0.7696\u001b[0m        \u001b[36m0.8539\u001b[0m  0.0001  3.9032\n",
            "      9  0.5390       0.8526    0.3941        \u001b[31m0.9228\u001b[0m        \u001b[94m0.7649\u001b[0m        \u001b[36m0.8516\u001b[0m  0.0001  4.3672\n",
            "     10  \u001b[36m0.5414\u001b[0m       0.8530    0.3965        \u001b[31m0.9217\u001b[0m        \u001b[94m0.7610\u001b[0m        \u001b[36m0.8496\u001b[0m  0.0001  3.6959\n",
            "     11  0.5325       0.8551    0.3867        \u001b[31m0.9211\u001b[0m        \u001b[94m0.7588\u001b[0m        \u001b[36m0.8485\u001b[0m  0.0001  3.5218\n",
            "     12  0.5335       0.8543    0.3878        \u001b[31m0.9200\u001b[0m        \u001b[94m0.7562\u001b[0m        \u001b[36m0.8464\u001b[0m  0.0001  4.4999\n",
            "     13  0.5294       0.8560    0.3832        \u001b[31m0.9190\u001b[0m        \u001b[94m0.7522\u001b[0m        \u001b[36m0.8445\u001b[0m  0.0001  3.6132\n",
            "     14  \u001b[36m0.5430\u001b[0m       0.8511    0.3987        \u001b[31m0.9187\u001b[0m        \u001b[94m0.7515\u001b[0m        \u001b[36m0.8441\u001b[0m  0.0001  3.6595\n",
            "     15  0.5402       0.8524    0.3954        \u001b[31m0.9186\u001b[0m        \u001b[94m0.7463\u001b[0m        \u001b[36m0.8438\u001b[0m  0.0000  4.5428\n",
            "     16  0.5405       0.8525    0.3957        \u001b[31m0.9184\u001b[0m        \u001b[94m0.7453\u001b[0m        \u001b[36m0.8435\u001b[0m  0.0000  3.7726\n",
            "     17  0.5385       0.8535    0.3934        \u001b[31m0.9183\u001b[0m        0.7454        \u001b[36m0.8434\u001b[0m  0.0000  3.6444\n",
            "     18  0.5418       0.8526    0.3970        \u001b[31m0.9182\u001b[0m        \u001b[94m0.7448\u001b[0m        \u001b[36m0.8431\u001b[0m  0.0000  4.3898\n",
            "     19  0.5390       0.8530    0.3939        0.9182        \u001b[94m0.7442\u001b[0m        0.8431  0.0000  3.7952\n",
            "     20  0.5386       0.8540    0.3933        0.9182        0.7443        0.8431  0.0000  3.6754\n",
            "     21  0.5379       0.8538    0.3927        \u001b[31m0.9181\u001b[0m        0.7445        \u001b[36m0.8429\u001b[0m  0.0000  4.0956\n",
            "     22  0.5386       0.8537    0.3934        \u001b[31m0.9181\u001b[0m        \u001b[94m0.7425\u001b[0m        \u001b[36m0.8429\u001b[0m  0.0000  4.1375\n",
            "     23  0.5386       0.8536    0.3935        \u001b[31m0.9181\u001b[0m        0.7438        \u001b[36m0.8429\u001b[0m  0.0000  3.6738\n",
            "     24  0.5392       0.8531    0.3942        \u001b[31m0.9181\u001b[0m        0.7435        \u001b[36m0.8428\u001b[0m  0.0000  3.6606\n",
            "     25  0.5398       0.8530    0.3948        \u001b[31m0.9180\u001b[0m        0.7427        \u001b[36m0.8428\u001b[0m  0.0000  4.5477\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 2/3; 24/36] END lr=0.001, module__dropout=0.3, module__linear_size=500, module__size_emb=120;, score=-1.027 total time= 1.8min\n",
            "[CV 3/3; 24/36] START lr=0.001, module__dropout=0.3, module__linear_size=500, module__size_emb=120\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4123\u001b[0m       \u001b[32m0.8481\u001b[0m    \u001b[35m0.2723\u001b[0m        \u001b[31m0.9834\u001b[0m        \u001b[94m1.0850\u001b[0m        \u001b[36m0.9671\u001b[0m  0.0010  4.1431\n",
            "      2  \u001b[36m0.5539\u001b[0m       0.8145    \u001b[35m0.4197\u001b[0m        \u001b[31m0.9570\u001b[0m        \u001b[94m0.8775\u001b[0m        \u001b[36m0.9159\u001b[0m  0.0010  3.9720\n",
            "      3  0.4367       \u001b[32m0.8509\u001b[0m    0.2937        \u001b[31m0.9508\u001b[0m        \u001b[94m0.8433\u001b[0m        \u001b[36m0.9040\u001b[0m  0.0010  3.6921\n",
            "      4  0.3986       \u001b[32m0.8626\u001b[0m    0.2592        \u001b[31m0.9491\u001b[0m        \u001b[94m0.8241\u001b[0m        \u001b[36m0.9008\u001b[0m  0.0010  3.5307\n",
            "      5  0.5147       0.8358    0.3719        \u001b[31m0.9409\u001b[0m        \u001b[94m0.8115\u001b[0m        \u001b[36m0.8852\u001b[0m  0.0010  4.5566\n",
            "      6  0.4998       0.8437    0.3551        \u001b[31m0.9351\u001b[0m        \u001b[94m0.8002\u001b[0m        \u001b[36m0.8745\u001b[0m  0.0010  3.6127\n",
            "      7  \u001b[36m0.5734\u001b[0m       0.8290    \u001b[35m0.4382\u001b[0m        \u001b[31m0.9302\u001b[0m        \u001b[94m0.7888\u001b[0m        \u001b[36m0.8653\u001b[0m  0.0010  3.6529\n",
            "      8  0.5171       0.8470    0.3722        \u001b[31m0.9280\u001b[0m        \u001b[94m0.7518\u001b[0m        \u001b[36m0.8611\u001b[0m  0.0001  4.5603\n",
            "      9  0.5145       0.8486    0.3692        \u001b[31m0.9262\u001b[0m        \u001b[94m0.7440\u001b[0m        \u001b[36m0.8578\u001b[0m  0.0001  3.6514\n",
            "     10  0.5226       0.8463    0.3780        \u001b[31m0.9249\u001b[0m        \u001b[94m0.7401\u001b[0m        \u001b[36m0.8555\u001b[0m  0.0001  3.6885\n",
            "     11  0.5047       0.8502    0.3588        \u001b[31m0.9246\u001b[0m        \u001b[94m0.7350\u001b[0m        \u001b[36m0.8549\u001b[0m  0.0001  4.6304\n",
            "     12  0.5027       0.8540    0.3562        \u001b[31m0.9237\u001b[0m        \u001b[94m0.7320\u001b[0m        \u001b[36m0.8532\u001b[0m  0.0001  3.6911\n",
            "     13  0.5057       0.8536    0.3593        \u001b[31m0.9223\u001b[0m        \u001b[94m0.7285\u001b[0m        \u001b[36m0.8507\u001b[0m  0.0001  3.6803\n",
            "     14  0.4948       0.8554    0.3481        \u001b[31m0.9221\u001b[0m        \u001b[94m0.7262\u001b[0m        \u001b[36m0.8502\u001b[0m  0.0001  4.1026\n",
            "     15  0.5104       0.8523    0.3643        \u001b[31m0.9217\u001b[0m        \u001b[94m0.7199\u001b[0m        \u001b[36m0.8495\u001b[0m  0.0000  3.7571\n",
            "     16  0.5112       0.8516    0.3652        \u001b[31m0.9216\u001b[0m        \u001b[94m0.7191\u001b[0m        \u001b[36m0.8494\u001b[0m  0.0000  3.6649\n",
            "     17  0.5144       0.8511    0.3686        \u001b[31m0.9213\u001b[0m        \u001b[94m0.7191\u001b[0m        \u001b[36m0.8488\u001b[0m  0.0000  4.5483\n",
            "     18  0.5134       0.8510    0.3676        \u001b[31m0.9212\u001b[0m        0.7200        \u001b[36m0.8487\u001b[0m  0.0000  3.6874\n",
            "     19  0.5120       0.8510    0.3662        \u001b[31m0.9212\u001b[0m        \u001b[94m0.7185\u001b[0m        \u001b[36m0.8486\u001b[0m  0.0000  3.6596\n",
            "     20  0.5109       0.8511    0.3650        \u001b[31m0.9211\u001b[0m        0.7186        \u001b[36m0.8485\u001b[0m  0.0000  3.9178\n",
            "     21  0.5097       0.8516    0.3637        \u001b[31m0.9209\u001b[0m        \u001b[94m0.7179\u001b[0m        \u001b[36m0.8481\u001b[0m  0.0000  4.1561\n",
            "     22  0.5110       0.8510    0.3651        \u001b[31m0.9209\u001b[0m        \u001b[94m0.7175\u001b[0m        \u001b[36m0.8481\u001b[0m  0.0000  3.6783\n",
            "     23  0.5116       0.8509    0.3658        \u001b[31m0.9209\u001b[0m        0.7178        \u001b[36m0.8480\u001b[0m  0.0000  3.7177\n",
            "     24  0.5111       0.8509    0.3653        0.9209        \u001b[94m0.7172\u001b[0m        0.8480  0.0000  4.4528\n",
            "     25  0.5115       0.8508    0.3656        \u001b[31m0.9209\u001b[0m        0.7178        \u001b[36m0.8480\u001b[0m  0.0000  3.5963\n",
            "     26  0.5116       0.8509    0.3657        0.9209        0.7173        0.8480  0.0000  3.7152\n",
            "     27  0.5112       0.8511    0.3653        0.9209        \u001b[94m0.7162\u001b[0m        0.8480  0.0000  4.6232\n",
            "     28  0.5111       0.8510    0.3652        0.9209        0.7167        0.8480  0.0000  3.6890\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 3/3; 24/36] END lr=0.001, module__dropout=0.3, module__linear_size=500, module__size_emb=120;, score=-1.113 total time= 1.9min\n",
            "[CV 1/3; 25/36] START lr=0.001, module__dropout=0.3, module__linear_size=600, module__size_emb=30\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.5391\u001b[0m       \u001b[32m0.8176\u001b[0m    \u001b[35m0.4021\u001b[0m        \u001b[31m1.0030\u001b[0m        \u001b[94m1.0729\u001b[0m        \u001b[36m1.0060\u001b[0m  0.0010  4.2812\n",
            "      2  \u001b[36m0.5462\u001b[0m       \u001b[32m0.8212\u001b[0m    \u001b[35m0.4092\u001b[0m        \u001b[31m0.9815\u001b[0m        \u001b[94m0.8937\u001b[0m        \u001b[36m0.9633\u001b[0m  0.0010  3.7403\n",
            "      3  \u001b[36m0.5768\u001b[0m       0.8097    \u001b[35m0.4480\u001b[0m        \u001b[31m0.9782\u001b[0m        \u001b[94m0.8645\u001b[0m        \u001b[36m0.9568\u001b[0m  0.0010  3.6775\n",
            "      4  0.4456       \u001b[32m0.8494\u001b[0m    0.3021        \u001b[31m0.9672\u001b[0m        \u001b[94m0.8533\u001b[0m        \u001b[36m0.9355\u001b[0m  0.0010  4.5514\n",
            "      5  0.4718       0.8462    0.3271        \u001b[31m0.9643\u001b[0m        \u001b[94m0.8420\u001b[0m        \u001b[36m0.9298\u001b[0m  0.0010  3.6347\n",
            "      6  0.5034       0.8385    0.3597        \u001b[31m0.9625\u001b[0m        \u001b[94m0.8314\u001b[0m        \u001b[36m0.9265\u001b[0m  0.0010  4.4327\n",
            "      7  0.4895       0.8490    0.3439        \u001b[31m0.9585\u001b[0m        \u001b[94m0.8244\u001b[0m        \u001b[36m0.9188\u001b[0m  0.0010  5.2247\n",
            "      8  0.5267       0.8403    0.3835        \u001b[31m0.9573\u001b[0m        \u001b[94m0.7975\u001b[0m        \u001b[36m0.9164\u001b[0m  0.0001  4.0560\n",
            "      9  0.5131       0.8411    0.3691        \u001b[31m0.9559\u001b[0m        \u001b[94m0.7941\u001b[0m        \u001b[36m0.9138\u001b[0m  0.0001  3.8263\n",
            "     10  0.5170       0.8420    0.3731        \u001b[31m0.9555\u001b[0m        \u001b[94m0.7918\u001b[0m        \u001b[36m0.9129\u001b[0m  0.0001  4.8005\n",
            "     11  0.5265       0.8392    0.3836        \u001b[31m0.9550\u001b[0m        \u001b[94m0.7916\u001b[0m        \u001b[36m0.9121\u001b[0m  0.0001  3.9501\n",
            "     12  0.5337       0.8366    0.3919        0.9551        \u001b[94m0.7900\u001b[0m        0.9123  0.0001  3.8773\n",
            "     13  0.5210       0.8380    0.3780        \u001b[31m0.9540\u001b[0m        \u001b[94m0.7878\u001b[0m        \u001b[36m0.9101\u001b[0m  0.0001  4.9536\n",
            "     14  0.5139       0.8417    0.3698        \u001b[31m0.9535\u001b[0m        \u001b[94m0.7876\u001b[0m        \u001b[36m0.9091\u001b[0m  0.0001  4.0818\n",
            "     15  0.5198       0.8385    0.3766        0.9536        \u001b[94m0.7840\u001b[0m        0.9094  0.0000  3.7925\n",
            "     16  0.5193       0.8385    0.3761        0.9536        0.7851        0.9093  0.0000  4.5653\n",
            "     17  0.5208       0.8378    0.3778        0.9536        \u001b[94m0.7833\u001b[0m        0.9094  0.0000  3.7247\n",
            "     18  0.5194       0.8387    0.3762        0.9535        0.7833        0.9092  0.0000  3.6603\n",
            "     19  0.5192       0.8404    0.3756        \u001b[31m0.9534\u001b[0m        \u001b[94m0.7827\u001b[0m        \u001b[36m0.9090\u001b[0m  0.0000  4.5839\n",
            "     20  0.5198       0.8387    0.3766        \u001b[31m0.9534\u001b[0m        0.7833        \u001b[36m0.9090\u001b[0m  0.0000  4.3079\n",
            "     21  0.5195       0.8400    0.3760        \u001b[31m0.9533\u001b[0m        \u001b[94m0.7823\u001b[0m        \u001b[36m0.9089\u001b[0m  0.0000  3.7961\n",
            "     22  0.5197       0.8387    0.3765        0.9534        \u001b[94m0.7818\u001b[0m        0.9089  0.0000  4.3009\n",
            "     23  0.5198       0.8387    0.3766        0.9534        0.7827        0.9089  0.0000  4.2387\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 1/3; 25/36] END lr=0.001, module__dropout=0.3, module__linear_size=600, module__size_emb=30;, score=-1.143 total time= 1.7min\n",
            "[CV 2/3; 25/36] START lr=0.001, module__dropout=0.3, module__linear_size=600, module__size_emb=30\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.5513\u001b[0m       \u001b[32m0.8262\u001b[0m    \u001b[35m0.4137\u001b[0m        \u001b[31m0.9728\u001b[0m        \u001b[94m1.1274\u001b[0m        \u001b[36m0.9464\u001b[0m  0.0010  4.6336\n",
            "      2  0.4959       \u001b[32m0.8426\u001b[0m    0.3513        \u001b[31m0.9522\u001b[0m        \u001b[94m0.8983\u001b[0m        \u001b[36m0.9066\u001b[0m  0.0010  3.8296\n",
            "      3  0.4971       0.8412    0.3528        \u001b[31m0.9472\u001b[0m        \u001b[94m0.8688\u001b[0m        \u001b[36m0.8972\u001b[0m  0.0010  3.8252\n",
            "      4  0.5341       0.8311    0.3935        \u001b[31m0.9450\u001b[0m        \u001b[94m0.8568\u001b[0m        \u001b[36m0.8931\u001b[0m  0.0010  4.3858\n",
            "      5  0.5091       0.8395    0.3654        \u001b[31m0.9399\u001b[0m        \u001b[94m0.8469\u001b[0m        \u001b[36m0.8834\u001b[0m  0.0010  3.9121\n",
            "      6  0.4948       \u001b[32m0.8458\u001b[0m    0.3497        \u001b[31m0.9365\u001b[0m        \u001b[94m0.8354\u001b[0m        \u001b[36m0.8771\u001b[0m  0.0010  3.6850\n",
            "      7  0.4701       \u001b[32m0.8567\u001b[0m    0.3239        \u001b[31m0.9347\u001b[0m        \u001b[94m0.8266\u001b[0m        \u001b[36m0.8737\u001b[0m  0.0010  4.0405\n",
            "      8  0.5294       0.8406    0.3863        \u001b[31m0.9331\u001b[0m        \u001b[94m0.7991\u001b[0m        \u001b[36m0.8707\u001b[0m  0.0001  4.5319\n",
            "      9  0.5412       0.8380    0.3996        \u001b[31m0.9329\u001b[0m        \u001b[94m0.7941\u001b[0m        \u001b[36m0.8703\u001b[0m  0.0001  3.8267\n",
            "     10  0.5318       0.8407    0.3889        \u001b[31m0.9316\u001b[0m        \u001b[94m0.7924\u001b[0m        \u001b[36m0.8680\u001b[0m  0.0001  4.1326\n",
            "     11  0.5259       0.8453    0.3816        \u001b[31m0.9309\u001b[0m        \u001b[94m0.7905\u001b[0m        \u001b[36m0.8665\u001b[0m  0.0001  4.5338\n",
            "     12  0.5281       0.8433    0.3844        \u001b[31m0.9308\u001b[0m        \u001b[94m0.7881\u001b[0m        \u001b[36m0.8663\u001b[0m  0.0001  3.6544\n",
            "     13  0.5354       0.8415    0.3926        \u001b[31m0.9297\u001b[0m        \u001b[94m0.7864\u001b[0m        \u001b[36m0.8644\u001b[0m  0.0001  3.8477\n",
            "     14  \u001b[36m0.5535\u001b[0m       0.8365    0.4136        \u001b[31m0.9293\u001b[0m        \u001b[94m0.7837\u001b[0m        \u001b[36m0.8635\u001b[0m  0.0001  4.8852\n",
            "     15  0.5324       0.8431    0.3891        \u001b[31m0.9291\u001b[0m        \u001b[94m0.7799\u001b[0m        \u001b[36m0.8633\u001b[0m  0.0000  3.9466\n",
            "     16  0.5327       0.8430    0.3893        \u001b[31m0.9291\u001b[0m        \u001b[94m0.7796\u001b[0m        \u001b[36m0.8633\u001b[0m  0.0000  3.9408\n",
            "     17  0.5346       0.8421    0.3916        0.9291        0.7803        0.8633  0.0000  4.6679\n",
            "     18  0.5303       0.8437    0.3867        0.9292        0.7799        0.8633  0.0000  4.0140\n",
            "     19  0.5307       0.8438    0.3870        \u001b[31m0.9291\u001b[0m        0.7798        \u001b[36m0.8632\u001b[0m  0.0000  3.8973\n",
            "     20  0.5304       0.8438    0.3868        0.9291        \u001b[94m0.7794\u001b[0m        0.8632  0.0000  4.8632\n",
            "     21  0.5356       0.8419    0.3927        \u001b[31m0.9289\u001b[0m        \u001b[94m0.7788\u001b[0m        \u001b[36m0.8629\u001b[0m  0.0000  3.7319\n",
            "     22  0.5348       0.8421    0.3919        \u001b[31m0.9289\u001b[0m        0.7795        \u001b[36m0.8629\u001b[0m  0.0000  3.6796\n",
            "     23  0.5347       0.8424    0.3916        \u001b[31m0.9289\u001b[0m        \u001b[94m0.7782\u001b[0m        \u001b[36m0.8629\u001b[0m  0.0000  5.0586\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 2/3; 25/36] END lr=0.001, module__dropout=0.3, module__linear_size=600, module__size_emb=30;, score=-1.012 total time= 1.7min\n",
            "[CV 3/3; 25/36] START lr=0.001, module__dropout=0.3, module__linear_size=600, module__size_emb=30\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4547\u001b[0m       \u001b[32m0.8336\u001b[0m    \u001b[35m0.3126\u001b[0m        \u001b[31m0.9790\u001b[0m        \u001b[94m1.1137\u001b[0m        \u001b[36m0.9584\u001b[0m  0.0010  3.8746\n",
            "      2  \u001b[36m0.4803\u001b[0m       0.8300    \u001b[35m0.3380\u001b[0m        \u001b[31m0.9599\u001b[0m        \u001b[94m0.8823\u001b[0m        \u001b[36m0.9214\u001b[0m  0.0010  4.2035\n",
            "      3  0.4750       \u001b[32m0.8371\u001b[0m    0.3316        \u001b[31m0.9545\u001b[0m        \u001b[94m0.8523\u001b[0m        \u001b[36m0.9110\u001b[0m  0.0010  3.6821\n",
            "      4  \u001b[36m0.5945\u001b[0m       0.8094    \u001b[35m0.4698\u001b[0m        \u001b[31m0.9529\u001b[0m        \u001b[94m0.8399\u001b[0m        \u001b[36m0.9081\u001b[0m  0.0010  3.9065\n",
            "      5  0.4918       \u001b[32m0.8388\u001b[0m    0.3479        \u001b[31m0.9447\u001b[0m        \u001b[94m0.8279\u001b[0m        \u001b[36m0.8924\u001b[0m  0.0010  4.6683\n",
            "      6  0.4989       0.8329    0.3561        \u001b[31m0.9409\u001b[0m        \u001b[94m0.8145\u001b[0m        \u001b[36m0.8853\u001b[0m  0.0010  3.9730\n",
            "      7  0.5152       0.8366    0.3722        0.9414        \u001b[94m0.8072\u001b[0m        0.8863  0.0010  3.8972\n",
            "      8  0.5084       0.8375    0.3650        \u001b[31m0.9393\u001b[0m        \u001b[94m0.7768\u001b[0m        \u001b[36m0.8823\u001b[0m  0.0001  4.6022\n",
            "      9  0.5137       0.8368    0.3705        \u001b[31m0.9391\u001b[0m        \u001b[94m0.7742\u001b[0m        \u001b[36m0.8819\u001b[0m  0.0001  3.7430\n",
            "     10  0.5275       0.8363    0.3852        \u001b[31m0.9374\u001b[0m        \u001b[94m0.7716\u001b[0m        \u001b[36m0.8787\u001b[0m  0.0001  3.7811\n",
            "     11  0.4891       \u001b[32m0.8445\u001b[0m    0.3442        \u001b[31m0.9370\u001b[0m        \u001b[94m0.7700\u001b[0m        \u001b[36m0.8779\u001b[0m  0.0001  5.0276\n",
            "     12  0.5008       0.8415    0.3565        \u001b[31m0.9363\u001b[0m        \u001b[94m0.7681\u001b[0m        \u001b[36m0.8766\u001b[0m  0.0001  3.8417\n",
            "     13  0.5027       0.8427    0.3582        \u001b[31m0.9358\u001b[0m        \u001b[94m0.7668\u001b[0m        \u001b[36m0.8757\u001b[0m  0.0001  3.8256\n",
            "     14  0.5008       0.8427    0.3563        0.9361        \u001b[94m0.7651\u001b[0m        0.8763  0.0001  4.7137\n",
            "     15  0.5034       0.8418    0.3591        0.9358        \u001b[94m0.7619\u001b[0m        0.8758  0.0000  3.8636\n",
            "     16  0.5048       0.8413    0.3606        0.9358        \u001b[94m0.7617\u001b[0m        0.8757  0.0000  3.8420\n",
            "     17  0.5060       0.8418    0.3617        \u001b[31m0.9356\u001b[0m        \u001b[94m0.7609\u001b[0m        \u001b[36m0.8754\u001b[0m  0.0000  4.6104\n",
            "     18  0.5034       0.8413    0.3592        \u001b[31m0.9356\u001b[0m        0.7609        \u001b[36m0.8753\u001b[0m  0.0000  3.8293\n",
            "     19  0.5064       0.8418    0.3621        \u001b[31m0.9355\u001b[0m        0.7611        \u001b[36m0.8751\u001b[0m  0.0000  3.8620\n",
            "     20  0.5081       0.8418    0.3638        0.9355        0.7609        0.8752  0.0000  4.0324\n",
            "     21  0.5076       0.8419    0.3633        \u001b[31m0.9354\u001b[0m        0.7610        \u001b[36m0.8750\u001b[0m  0.0000  4.1558\n",
            "     22  0.5075       0.8419    0.3632        \u001b[31m0.9354\u001b[0m        \u001b[94m0.7592\u001b[0m        \u001b[36m0.8750\u001b[0m  0.0000  3.7210\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 3/3; 25/36] END lr=0.001, module__dropout=0.3, module__linear_size=600, module__size_emb=30;, score=-1.092 total time= 1.6min\n",
            "[CV 1/3; 26/36] START lr=0.001, module__dropout=0.3, module__linear_size=600, module__size_emb=60\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.5218\u001b[0m       \u001b[32m0.8275\u001b[0m    \u001b[35m0.3810\u001b[0m        \u001b[31m0.9999\u001b[0m        \u001b[94m1.0621\u001b[0m        \u001b[36m0.9999\u001b[0m  0.0010  3.7473\n",
            "      2  0.4198       \u001b[32m0.8488\u001b[0m    0.2789        \u001b[31m0.9775\u001b[0m        \u001b[94m0.8923\u001b[0m        \u001b[36m0.9555\u001b[0m  0.0010  3.9159\n",
            "      3  0.4525       \u001b[32m0.8491\u001b[0m    0.3084        \u001b[31m0.9708\u001b[0m        \u001b[94m0.8666\u001b[0m        \u001b[36m0.9425\u001b[0m  0.0010  4.6207\n",
            "      4  \u001b[36m0.5397\u001b[0m       0.8242    \u001b[35m0.4013\u001b[0m        \u001b[31m0.9655\u001b[0m        \u001b[94m0.8502\u001b[0m        \u001b[36m0.9322\u001b[0m  0.0010  3.7421\n",
            "      5  \u001b[36m0.5466\u001b[0m       0.8255    \u001b[35m0.4086\u001b[0m        \u001b[31m0.9602\u001b[0m        \u001b[94m0.8367\u001b[0m        \u001b[36m0.9220\u001b[0m  0.0010  3.6508\n",
            "      6  0.4057       \u001b[32m0.8660\u001b[0m    0.2649        \u001b[31m0.9601\u001b[0m        \u001b[94m0.8264\u001b[0m        \u001b[36m0.9218\u001b[0m  0.0010  4.0875\n",
            "      7  0.5281       0.8345    0.3863        \u001b[31m0.9576\u001b[0m        \u001b[94m0.8194\u001b[0m        \u001b[36m0.9170\u001b[0m  0.0010  4.3035\n",
            "      8  0.5139       0.8406    0.3701        \u001b[31m0.9538\u001b[0m        \u001b[94m0.7887\u001b[0m        \u001b[36m0.9097\u001b[0m  0.0001  4.0264\n",
            "      9  0.5213       0.8372    0.3785        \u001b[31m0.9527\u001b[0m        \u001b[94m0.7856\u001b[0m        \u001b[36m0.9076\u001b[0m  0.0001  4.1542\n",
            "     10  0.5293       0.8347    0.3875        \u001b[31m0.9520\u001b[0m        \u001b[94m0.7829\u001b[0m        \u001b[36m0.9062\u001b[0m  0.0001  4.2503\n",
            "     11  0.5186       0.8403    0.3750        \u001b[31m0.9511\u001b[0m        \u001b[94m0.7805\u001b[0m        \u001b[36m0.9046\u001b[0m  0.0001  3.7752\n",
            "     12  0.5416       0.8314    0.4016        0.9518        \u001b[94m0.7793\u001b[0m        0.9060  0.0001  4.0092\n",
            "     13  0.5420       0.8319    0.4019        0.9513        \u001b[94m0.7778\u001b[0m        0.9049  0.0001  4.4778\n",
            "     14  0.5367       0.8332    0.3959        \u001b[31m0.9505\u001b[0m        \u001b[94m0.7768\u001b[0m        \u001b[36m0.9034\u001b[0m  0.0001  3.9475\n",
            "     15  0.5271       0.8373    0.3846        \u001b[31m0.9499\u001b[0m        \u001b[94m0.7724\u001b[0m        \u001b[36m0.9022\u001b[0m  0.0000  3.7982\n",
            "     16  0.5268       0.8378    0.3842        \u001b[31m0.9497\u001b[0m        0.7727        \u001b[36m0.9020\u001b[0m  0.0000  4.6044\n",
            "     17  0.5283       0.8375    0.3859        \u001b[31m0.9497\u001b[0m        \u001b[94m0.7712\u001b[0m        \u001b[36m0.9019\u001b[0m  0.0000  3.9169\n",
            "     18  0.5286       0.8376    0.3861        \u001b[31m0.9496\u001b[0m        0.7719        \u001b[36m0.9018\u001b[0m  0.0000  3.9773\n",
            "     19  0.5288       0.8377    0.3863        \u001b[31m0.9495\u001b[0m        0.7718        \u001b[36m0.9016\u001b[0m  0.0000  4.2836\n",
            "     20  0.5268       0.8378    0.3842        \u001b[31m0.9494\u001b[0m        0.7719        \u001b[36m0.9015\u001b[0m  0.0000  4.0116\n",
            "     21  0.5301       0.8376    0.3877        0.9495        0.7715        0.9016  0.0000  3.4851\n",
            "     22  0.5297       0.8374    0.3874        0.9495        0.7713        0.9015  0.0000  4.6530\n",
            "     23  0.5291       0.8372    0.3868        0.9495        \u001b[94m0.7708\u001b[0m        0.9015  0.0000  3.8847\n",
            "     24  0.5286       0.8375    0.3862        \u001b[31m0.9494\u001b[0m        0.7709        \u001b[36m0.9014\u001b[0m  0.0000  3.9185\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 1/3; 26/36] END lr=0.001, module__dropout=0.3, module__linear_size=600, module__size_emb=60;, score=-1.147 total time= 1.7min\n",
            "[CV 2/3; 26/36] START lr=0.001, module__dropout=0.3, module__linear_size=600, module__size_emb=60\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.5128\u001b[0m       \u001b[32m0.8228\u001b[0m    \u001b[35m0.3725\u001b[0m        \u001b[31m0.9702\u001b[0m        \u001b[94m1.1075\u001b[0m        \u001b[36m0.9412\u001b[0m  0.0010  3.9271\n",
            "      2  0.4513       \u001b[32m0.8509\u001b[0m    0.3071        \u001b[31m0.9526\u001b[0m        \u001b[94m0.8961\u001b[0m        \u001b[36m0.9075\u001b[0m  0.0010  4.0271\n",
            "      3  0.4974       0.8409    0.3531        \u001b[31m0.9461\u001b[0m        \u001b[94m0.8670\u001b[0m        \u001b[36m0.8950\u001b[0m  0.0010  4.4798\n",
            "      4  \u001b[36m0.5435\u001b[0m       0.8268    \u001b[35m0.4048\u001b[0m        \u001b[31m0.9425\u001b[0m        \u001b[94m0.8516\u001b[0m        \u001b[36m0.8883\u001b[0m  0.0010  4.0119\n",
            "      5  0.5353       0.8380    0.3932        \u001b[31m0.9375\u001b[0m        \u001b[94m0.8385\u001b[0m        \u001b[36m0.8790\u001b[0m  0.0010  3.8472\n",
            "      6  0.4779       \u001b[32m0.8577\u001b[0m    0.3312        \u001b[31m0.9349\u001b[0m        \u001b[94m0.8276\u001b[0m        \u001b[36m0.8740\u001b[0m  0.0010  4.7327\n",
            "      7  \u001b[36m0.5797\u001b[0m       0.8301    \u001b[35m0.4454\u001b[0m        \u001b[31m0.9301\u001b[0m        \u001b[94m0.8143\u001b[0m        \u001b[36m0.8650\u001b[0m  0.0010  3.6839\n",
            "      8  0.5477       0.8421    0.4058        \u001b[31m0.9279\u001b[0m        \u001b[94m0.7796\u001b[0m        \u001b[36m0.8610\u001b[0m  0.0001  3.9270\n",
            "      9  0.5346       0.8467    0.3906        \u001b[31m0.9274\u001b[0m        \u001b[94m0.7747\u001b[0m        \u001b[36m0.8601\u001b[0m  0.0001  4.6784\n",
            "     10  0.5514       0.8423    0.4098        \u001b[31m0.9261\u001b[0m        \u001b[94m0.7728\u001b[0m        \u001b[36m0.8576\u001b[0m  0.0001  4.1074\n",
            "     11  0.5504       0.8446    0.4082        \u001b[31m0.9248\u001b[0m        \u001b[94m0.7708\u001b[0m        \u001b[36m0.8553\u001b[0m  0.0001  4.0044\n",
            "     12  0.5170       0.8515    0.3712        0.9249        \u001b[94m0.7685\u001b[0m        0.8554  0.0001  4.6172\n",
            "     13  0.5673       0.8384    0.4286        \u001b[31m0.9241\u001b[0m        \u001b[94m0.7663\u001b[0m        \u001b[36m0.8539\u001b[0m  0.0001  3.4573\n",
            "     14  0.5444       0.8461    0.4013        \u001b[31m0.9236\u001b[0m        \u001b[94m0.7639\u001b[0m        \u001b[36m0.8531\u001b[0m  0.0001  3.7209\n",
            "     15  0.5361       0.8469    0.3922        \u001b[31m0.9235\u001b[0m        \u001b[94m0.7593\u001b[0m        \u001b[36m0.8529\u001b[0m  0.0000  4.4594\n",
            "     16  0.5401       0.8479    0.3962        \u001b[31m0.9234\u001b[0m        0.7595        \u001b[36m0.8526\u001b[0m  0.0000  3.9037\n",
            "     17  0.5346       0.8482    0.3902        \u001b[31m0.9233\u001b[0m        \u001b[94m0.7584\u001b[0m        \u001b[36m0.8524\u001b[0m  0.0000  3.7719\n",
            "     18  0.5405       0.8480    0.3967        \u001b[31m0.9232\u001b[0m        \u001b[94m0.7579\u001b[0m        \u001b[36m0.8522\u001b[0m  0.0000  4.8199\n",
            "     19  0.5352       0.8488    0.3909        \u001b[31m0.9231\u001b[0m        0.7587        \u001b[36m0.8521\u001b[0m  0.0000  3.8669\n",
            "     20  0.5394       0.8480    0.3955        0.9231        0.7580        0.8521  0.0000  3.8078\n",
            "     21  0.5358       0.8491    0.3914        \u001b[31m0.9230\u001b[0m        \u001b[94m0.7561\u001b[0m        \u001b[36m0.8520\u001b[0m  0.0000  4.5894\n",
            "     22  0.5358       0.8488    0.3914        \u001b[31m0.9230\u001b[0m        0.7580        \u001b[36m0.8519\u001b[0m  0.0000  3.9134\n",
            "     23  0.5371       0.8480    0.3930        \u001b[31m0.9230\u001b[0m        0.7573        \u001b[36m0.8519\u001b[0m  0.0000  3.8873\n",
            "     24  0.5372       0.8479    0.3932        \u001b[31m0.9230\u001b[0m        0.7583        \u001b[36m0.8519\u001b[0m  0.0000  4.5713\n",
            "     25  0.5369       0.8479    0.3928        \u001b[31m0.9230\u001b[0m        0.7572        \u001b[36m0.8519\u001b[0m  0.0000  4.0203\n",
            "     26  0.5376       0.8480    0.3936        \u001b[31m0.9230\u001b[0m        0.7564        \u001b[36m0.8518\u001b[0m  0.0000  3.6393\n",
            "     27  0.5375       0.8480    0.3935        \u001b[31m0.9229\u001b[0m        0.7569        \u001b[36m0.8518\u001b[0m  0.0000  3.8702\n",
            "     28  0.5370       0.8480    0.3929        \u001b[31m0.9229\u001b[0m        0.7581        \u001b[36m0.8518\u001b[0m  0.0000  4.4021\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 2/3; 26/36] END lr=0.001, module__dropout=0.3, module__linear_size=600, module__size_emb=60;, score=-1.022 total time= 2.0min\n",
            "[CV 3/3; 26/36] START lr=0.001, module__dropout=0.3, module__linear_size=600, module__size_emb=60\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.3940\u001b[0m       \u001b[32m0.8504\u001b[0m    \u001b[35m0.2564\u001b[0m        \u001b[31m0.9821\u001b[0m        \u001b[94m1.0982\u001b[0m        \u001b[36m0.9645\u001b[0m  0.0010  4.2855\n",
            "      2  \u001b[36m0.4618\u001b[0m       0.8369    \u001b[35m0.3189\u001b[0m        \u001b[31m0.9587\u001b[0m        \u001b[94m0.8829\u001b[0m        \u001b[36m0.9192\u001b[0m  0.0010  3.8798\n",
            "      3  \u001b[36m0.4668\u001b[0m       0.8390    \u001b[35m0.3234\u001b[0m        \u001b[31m0.9533\u001b[0m        \u001b[94m0.8521\u001b[0m        \u001b[36m0.9088\u001b[0m  0.0010  3.6814\n",
            "      4  \u001b[36m0.5223\u001b[0m       0.8290    \u001b[35m0.3812\u001b[0m        \u001b[31m0.9470\u001b[0m        \u001b[94m0.8328\u001b[0m        \u001b[36m0.8969\u001b[0m  0.0010  3.7887\n",
            "      5  0.4159       \u001b[32m0.8572\u001b[0m    0.2746        0.9495        \u001b[94m0.8189\u001b[0m        0.9016  0.0010  4.2812\n",
            "      6  0.4792       0.8456    0.3344        \u001b[31m0.9401\u001b[0m        \u001b[94m0.8093\u001b[0m        \u001b[36m0.8838\u001b[0m  0.0010  3.5969\n",
            "      7  \u001b[36m0.5343\u001b[0m       0.8338    \u001b[35m0.3931\u001b[0m        \u001b[31m0.9378\u001b[0m        \u001b[94m0.7987\u001b[0m        \u001b[36m0.8794\u001b[0m  0.0010  3.7136\n",
            "      8  0.5132       0.8411    0.3692        \u001b[31m0.9363\u001b[0m        \u001b[94m0.7675\u001b[0m        \u001b[36m0.8766\u001b[0m  0.0001  4.5554\n",
            "      9  0.4943       0.8451    0.3493        \u001b[31m0.9358\u001b[0m        \u001b[94m0.7636\u001b[0m        \u001b[36m0.8757\u001b[0m  0.0001  3.6827\n",
            "     10  0.5079       0.8427    0.3635        \u001b[31m0.9344\u001b[0m        \u001b[94m0.7607\u001b[0m        \u001b[36m0.8730\u001b[0m  0.0001  3.6906\n",
            "     11  0.5256       0.8392    0.3826        \u001b[31m0.9329\u001b[0m        \u001b[94m0.7584\u001b[0m        \u001b[36m0.8704\u001b[0m  0.0001  4.5949\n",
            "     12  0.5048       0.8431    0.3603        0.9335        \u001b[94m0.7574\u001b[0m        0.8715  0.0001  3.6858\n",
            "     13  0.5023       0.8440    0.3576        0.9331        \u001b[94m0.7545\u001b[0m        0.8708  0.0001  3.7068\n",
            "     14  0.5173       0.8402    0.3737        \u001b[31m0.9324\u001b[0m        \u001b[94m0.7533\u001b[0m        \u001b[36m0.8693\u001b[0m  0.0001  4.5214\n",
            "     15  0.5094       0.8437    0.3648        0.9327        \u001b[94m0.7504\u001b[0m        0.8699  0.0000  3.8134\n",
            "     16  0.5107       0.8435    0.3662        0.9326        \u001b[94m0.7498\u001b[0m        0.8698  0.0000  3.6135\n",
            "     17  0.5114       0.8436    0.3669        0.9326        \u001b[94m0.7492\u001b[0m        0.8697  0.0000  4.1824\n",
            "     18  0.5104       0.8431    0.3660        0.9325        \u001b[94m0.7487\u001b[0m        0.8696  0.0000  3.9684\n",
            "     19  0.5079       0.8435    0.3633        0.9325        \u001b[94m0.7480\u001b[0m        0.8696  0.0000  3.7122\n",
            "     20  0.5111       0.8431    0.3667        \u001b[31m0.9323\u001b[0m        0.7490        \u001b[36m0.8692\u001b[0m  0.0000  3.9649\n",
            "     21  0.5095       0.8434    0.3650        0.9323        0.7485        0.8692  0.0000  4.3553\n",
            "     22  0.5092       0.8434    0.3646        \u001b[31m0.9323\u001b[0m        0.7482        \u001b[36m0.8692\u001b[0m  0.0000  3.6927\n",
            "     23  0.5096       0.8433    0.3651        \u001b[31m0.9323\u001b[0m        \u001b[94m0.7473\u001b[0m        \u001b[36m0.8692\u001b[0m  0.0000  3.6351\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 3/3; 26/36] END lr=0.001, module__dropout=0.3, module__linear_size=600, module__size_emb=60;, score=-1.104 total time= 1.6min\n",
            "[CV 1/3; 27/36] START lr=0.001, module__dropout=0.3, module__linear_size=600, module__size_emb=120\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4171\u001b[0m       \u001b[32m0.8556\u001b[0m    \u001b[35m0.2757\u001b[0m        \u001b[31m0.9933\u001b[0m        \u001b[94m1.0466\u001b[0m        \u001b[36m0.9867\u001b[0m  0.0010  3.8087\n",
            "      2  \u001b[36m0.4626\u001b[0m       0.8432    \u001b[35m0.3187\u001b[0m        \u001b[31m0.9740\u001b[0m        \u001b[94m0.8884\u001b[0m        \u001b[36m0.9487\u001b[0m  0.0010  4.2418\n",
            "      3  \u001b[36m0.5133\u001b[0m       0.8336    \u001b[35m0.3708\u001b[0m        \u001b[31m0.9641\u001b[0m        \u001b[94m0.8583\u001b[0m        \u001b[36m0.9294\u001b[0m  0.0010  4.3172\n",
            "      4  0.4987       0.8379    0.3550        \u001b[31m0.9622\u001b[0m        \u001b[94m0.8424\u001b[0m        \u001b[36m0.9259\u001b[0m  0.0010  3.7605\n",
            "      5  0.4881       0.8450    0.3432        \u001b[31m0.9590\u001b[0m        \u001b[94m0.8318\u001b[0m        \u001b[36m0.9196\u001b[0m  0.0010  4.0082\n",
            "      6  \u001b[36m0.5290\u001b[0m       0.8364    \u001b[35m0.3868\u001b[0m        \u001b[31m0.9547\u001b[0m        \u001b[94m0.8200\u001b[0m        \u001b[36m0.9115\u001b[0m  0.0010  4.4499\n",
            "      7  0.3295       \u001b[32m0.8873\u001b[0m    0.2023        0.9592        \u001b[94m0.8139\u001b[0m        0.9200  0.0010  3.7903\n",
            "      8  0.5212       0.8404    0.3777        \u001b[31m0.9491\u001b[0m        \u001b[94m0.7817\u001b[0m        \u001b[36m0.9008\u001b[0m  0.0001  3.7037\n",
            "      9  0.5158       0.8409    0.3720        \u001b[31m0.9475\u001b[0m        \u001b[94m0.7765\u001b[0m        \u001b[36m0.8978\u001b[0m  0.0001  4.5425\n",
            "     10  0.5210       0.8411    0.3774        \u001b[31m0.9464\u001b[0m        \u001b[94m0.7736\u001b[0m        \u001b[36m0.8957\u001b[0m  0.0001  3.7956\n",
            "     11  0.5125       0.8454    0.3677        \u001b[31m0.9455\u001b[0m        \u001b[94m0.7715\u001b[0m        \u001b[36m0.8940\u001b[0m  0.0001  3.7634\n",
            "     12  0.4956       0.8541    0.3491        \u001b[31m0.9446\u001b[0m        \u001b[94m0.7693\u001b[0m        \u001b[36m0.8923\u001b[0m  0.0001  4.6228\n",
            "     13  \u001b[36m0.5475\u001b[0m       0.8366    \u001b[35m0.4069\u001b[0m        0.9449        \u001b[94m0.7659\u001b[0m        0.8928  0.0001  3.7666\n",
            "     14  0.5203       0.8453    0.3758        \u001b[31m0.9431\u001b[0m        \u001b[94m0.7630\u001b[0m        \u001b[36m0.8894\u001b[0m  0.0001  3.6862\n",
            "     15  0.5258       0.8434    0.3819        \u001b[31m0.9430\u001b[0m        \u001b[94m0.7569\u001b[0m        \u001b[36m0.8892\u001b[0m  0.0000  4.6140\n",
            "     16  0.5279       0.8407    0.3847        \u001b[31m0.9430\u001b[0m        0.7571        \u001b[36m0.8892\u001b[0m  0.0000  3.7337\n",
            "     17  0.5251       0.8444    0.3810        \u001b[31m0.9428\u001b[0m        0.7576        \u001b[36m0.8889\u001b[0m  0.0000  3.8068\n",
            "     18  0.5280       0.8420    0.3846        \u001b[31m0.9428\u001b[0m        \u001b[94m0.7565\u001b[0m        \u001b[36m0.8888\u001b[0m  0.0000  4.5714\n",
            "     19  0.5277       0.8422    0.3842        \u001b[31m0.9426\u001b[0m        0.7570        \u001b[36m0.8886\u001b[0m  0.0000  3.7348\n",
            "     20  0.5270       0.8430    0.3833        \u001b[31m0.9425\u001b[0m        0.7568        \u001b[36m0.8883\u001b[0m  0.0000  3.6975\n",
            "     21  0.5286       0.8414    0.3853        0.9426        \u001b[94m0.7558\u001b[0m        0.8884  0.0000  4.2962\n",
            "     22  0.5286       0.8417    0.3852        0.9426        \u001b[94m0.7557\u001b[0m        0.8884  0.0000  4.0579\n",
            "     23  0.5286       0.8419    0.3852        0.9425        \u001b[94m0.7549\u001b[0m        0.8884  0.0000  3.7944\n",
            "     24  0.5286       0.8417    0.3852        0.9425        0.7553        0.8884  0.0000  4.2918\n",
            "     25  0.5290       0.8413    0.3858        0.9425        0.7558        0.8884  0.0000  4.2755\n",
            "     26  0.5288       0.8412    0.3856        0.9425        0.7554        0.8884  0.0000  3.9123\n",
            "     27  0.5289       0.8412    0.3857        \u001b[31m0.9425\u001b[0m        0.7558        \u001b[36m0.8883\u001b[0m  0.0000  4.1539\n",
            "     28  0.5289       0.8414    0.3857        \u001b[31m0.9425\u001b[0m        \u001b[94m0.7545\u001b[0m        \u001b[36m0.8883\u001b[0m  0.0000  4.6203\n",
            "     29  0.5287       0.8413    0.3855        \u001b[31m0.9425\u001b[0m        0.7551        \u001b[36m0.8883\u001b[0m  0.0000  3.4558\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 1/3; 27/36] END lr=0.001, module__dropout=0.3, module__linear_size=600, module__size_emb=120;, score=-1.139 total time= 2.1min\n",
            "[CV 2/3; 27/36] START lr=0.001, module__dropout=0.3, module__linear_size=600, module__size_emb=120\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.3703\u001b[0m       \u001b[32m0.8668\u001b[0m    \u001b[35m0.2355\u001b[0m        \u001b[31m0.9685\u001b[0m        \u001b[94m1.0941\u001b[0m        \u001b[36m0.9379\u001b[0m  0.0010  4.1283\n",
            "      2  \u001b[36m0.4457\u001b[0m       0.8554    \u001b[35m0.3013\u001b[0m        \u001b[31m0.9517\u001b[0m        \u001b[94m0.8898\u001b[0m        \u001b[36m0.9057\u001b[0m  0.0010  3.8559\n",
            "      3  0.3981       0.8659    0.2585        \u001b[31m0.9466\u001b[0m        \u001b[94m0.8607\u001b[0m        \u001b[36m0.8960\u001b[0m  0.0010  4.2605\n",
            "      4  0.3658       \u001b[32m0.8825\u001b[0m    0.2307        \u001b[31m0.9448\u001b[0m        \u001b[94m0.8407\u001b[0m        \u001b[36m0.8927\u001b[0m  0.0010  3.9731\n",
            "      5  \u001b[36m0.4800\u001b[0m       0.8570    \u001b[35m0.3333\u001b[0m        \u001b[31m0.9332\u001b[0m        \u001b[94m0.8270\u001b[0m        \u001b[36m0.8708\u001b[0m  0.0010  3.8796\n",
            "      6  \u001b[36m0.5183\u001b[0m       0.8500    \u001b[35m0.3728\u001b[0m        \u001b[31m0.9275\u001b[0m        \u001b[94m0.8136\u001b[0m        \u001b[36m0.8603\u001b[0m  0.0010  4.2030\n",
            "      7  0.4389       0.8731    0.2931        0.9292        \u001b[94m0.8030\u001b[0m        0.8634  0.0010  4.1461\n",
            "      8  0.5132       0.8576    0.3662        \u001b[31m0.9232\u001b[0m        \u001b[94m0.7668\u001b[0m        \u001b[36m0.8522\u001b[0m  0.0001  3.7660\n",
            "      9  \u001b[36m0.5232\u001b[0m       0.8549    \u001b[35m0.3770\u001b[0m        \u001b[31m0.9215\u001b[0m        \u001b[94m0.7611\u001b[0m        \u001b[36m0.8492\u001b[0m  0.0001  4.1044\n",
            "     10  0.5183       0.8582    0.3713        \u001b[31m0.9200\u001b[0m        \u001b[94m0.7576\u001b[0m        \u001b[36m0.8465\u001b[0m  0.0001  4.3083\n",
            "     11  \u001b[36m0.5521\u001b[0m       0.8478    \u001b[35m0.4093\u001b[0m        \u001b[31m0.9184\u001b[0m        \u001b[94m0.7550\u001b[0m        \u001b[36m0.8435\u001b[0m  0.0001  3.7323\n",
            "     12  0.5387       0.8524    0.3938        0.9185        \u001b[94m0.7505\u001b[0m        0.8436  0.0001  3.6723\n",
            "     13  \u001b[36m0.5539\u001b[0m       0.8491    \u001b[35m0.4110\u001b[0m        \u001b[31m0.9168\u001b[0m        \u001b[94m0.7481\u001b[0m        \u001b[36m0.8406\u001b[0m  0.0001  4.5788\n",
            "     14  0.5282       0.8548    0.3822        \u001b[31m0.9164\u001b[0m        \u001b[94m0.7447\u001b[0m        \u001b[36m0.8398\u001b[0m  0.0001  3.7642\n",
            "     15  0.5366       0.8543    0.3911        \u001b[31m0.9161\u001b[0m        \u001b[94m0.7394\u001b[0m        \u001b[36m0.8393\u001b[0m  0.0000  3.7272\n",
            "     16  0.5384       0.8538    0.3932        \u001b[31m0.9160\u001b[0m        0.7394        \u001b[36m0.8390\u001b[0m  0.0000  4.6260\n",
            "     17  0.5369       0.8543    0.3915        \u001b[31m0.9158\u001b[0m        0.7398        \u001b[36m0.8388\u001b[0m  0.0000  3.7660\n",
            "     18  0.5387       0.8536    0.3936        \u001b[31m0.9158\u001b[0m        \u001b[94m0.7391\u001b[0m        \u001b[36m0.8386\u001b[0m  0.0000  3.7210\n",
            "     19  0.5394       0.8541    0.3942        \u001b[31m0.9156\u001b[0m        0.7396        \u001b[36m0.8383\u001b[0m  0.0000  4.5944\n",
            "     20  0.5392       0.8545    0.3938        \u001b[31m0.9155\u001b[0m        \u001b[94m0.7374\u001b[0m        \u001b[36m0.8382\u001b[0m  0.0000  3.7851\n",
            "     21  0.5420       0.8520    0.3974        \u001b[31m0.9155\u001b[0m        \u001b[94m0.7373\u001b[0m        \u001b[36m0.8381\u001b[0m  0.0000  3.6450\n",
            "     22  0.5418       0.8521    0.3971        \u001b[31m0.9155\u001b[0m        0.7375        \u001b[36m0.8381\u001b[0m  0.0000  4.4902\n",
            "     23  0.5414       0.8538    0.3963        \u001b[31m0.9155\u001b[0m        0.7386        \u001b[36m0.8381\u001b[0m  0.0000  3.8373\n",
            "     24  0.5411       0.8540    0.3960        0.9155        0.7376        0.8381  0.0000  3.7061\n",
            "     25  0.5411       0.8540    0.3960        \u001b[31m0.9155\u001b[0m        \u001b[94m0.7371\u001b[0m        \u001b[36m0.8381\u001b[0m  0.0000  4.2786\n",
            "     26  0.5405       0.8542    0.3953        \u001b[31m0.9155\u001b[0m        \u001b[94m0.7367\u001b[0m        \u001b[36m0.8381\u001b[0m  0.0000  4.1424\n",
            "     27  0.5403       0.8542    0.3951        \u001b[31m0.9154\u001b[0m        \u001b[94m0.7364\u001b[0m        \u001b[36m0.8380\u001b[0m  0.0000  3.6545\n",
            "     28  0.5404       0.8541    0.3953        \u001b[31m0.9154\u001b[0m        0.7373        \u001b[36m0.8380\u001b[0m  0.0000  3.8348\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 2/3; 27/36] END lr=0.001, module__dropout=0.3, module__linear_size=600, module__size_emb=120;, score=-1.028 total time= 2.0min\n",
            "[CV 3/3; 27/36] START lr=0.001, module__dropout=0.3, module__linear_size=600, module__size_emb=120\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4992\u001b[0m       \u001b[32m0.8244\u001b[0m    \u001b[35m0.3580\u001b[0m        \u001b[31m0.9702\u001b[0m        \u001b[94m1.0768\u001b[0m        \u001b[36m0.9412\u001b[0m  0.0010  3.7633\n",
            "      2  0.4246       \u001b[32m0.8484\u001b[0m    0.2832        \u001b[31m0.9590\u001b[0m        \u001b[94m0.8719\u001b[0m        \u001b[36m0.9197\u001b[0m  0.0010  4.5631\n",
            "      3  0.4861       0.8345    0.3430        \u001b[31m0.9515\u001b[0m        \u001b[94m0.8418\u001b[0m        \u001b[36m0.9054\u001b[0m  0.0010  3.8309\n",
            "      4  0.4674       0.8480    0.3226        \u001b[31m0.9438\u001b[0m        \u001b[94m0.8236\u001b[0m        \u001b[36m0.8907\u001b[0m  0.0010  3.7195\n",
            "      5  \u001b[36m0.5369\u001b[0m       0.8336    \u001b[35m0.3960\u001b[0m        \u001b[31m0.9399\u001b[0m        \u001b[94m0.8096\u001b[0m        \u001b[36m0.8835\u001b[0m  0.0010  4.2683\n",
            "      6  0.4716       \u001b[32m0.8506\u001b[0m    0.3262        0.9461        \u001b[94m0.7995\u001b[0m        0.8951  0.0010  4.1416\n",
            "      7  0.4803       0.8476    0.3351        \u001b[31m0.9322\u001b[0m        \u001b[94m0.7907\u001b[0m        \u001b[36m0.8690\u001b[0m  0.0010  3.6023\n",
            "      8  0.4877       0.8496    0.3420        \u001b[31m0.9319\u001b[0m        \u001b[94m0.7549\u001b[0m        \u001b[36m0.8685\u001b[0m  0.0001  4.2202\n",
            "      9  0.5115       0.8447    0.3668        \u001b[31m0.9297\u001b[0m        \u001b[94m0.7508\u001b[0m        \u001b[36m0.8643\u001b[0m  0.0001  4.3354\n",
            "     10  0.4928       \u001b[32m0.8509\u001b[0m    0.3468        0.9299        \u001b[94m0.7469\u001b[0m        0.8647  0.0001  3.7292\n",
            "     11  0.5127       0.8481    0.3674        \u001b[31m0.9279\u001b[0m        \u001b[94m0.7427\u001b[0m        \u001b[36m0.8610\u001b[0m  0.0001  3.9710\n",
            "     12  0.5037       0.8497    0.3579        \u001b[31m0.9269\u001b[0m        \u001b[94m0.7403\u001b[0m        \u001b[36m0.8591\u001b[0m  0.0001  4.5120\n",
            "     13  0.5006       0.8504    0.3547        0.9278        \u001b[94m0.7369\u001b[0m        0.8608  0.0001  3.6994\n",
            "     14  0.4994       0.8505    0.3534        \u001b[31m0.9259\u001b[0m        \u001b[94m0.7349\u001b[0m        \u001b[36m0.8573\u001b[0m  0.0001  3.7100\n",
            "     15  0.5065       0.8502    0.3607        \u001b[31m0.9253\u001b[0m        \u001b[94m0.7288\u001b[0m        \u001b[36m0.8562\u001b[0m  0.0000  4.5805\n",
            "     16  0.5118       0.8501    0.3661        \u001b[31m0.9250\u001b[0m        \u001b[94m0.7279\u001b[0m        \u001b[36m0.8556\u001b[0m  0.0000  3.7655\n",
            "     17  0.5144       0.8500    0.3688        \u001b[31m0.9249\u001b[0m        \u001b[94m0.7270\u001b[0m        \u001b[36m0.8554\u001b[0m  0.0000  3.8312\n",
            "     18  0.5138       0.8503    0.3681        \u001b[31m0.9249\u001b[0m        \u001b[94m0.7264\u001b[0m        \u001b[36m0.8554\u001b[0m  0.0000  4.5305\n",
            "     19  0.5094       0.8498    0.3637        \u001b[31m0.9247\u001b[0m        \u001b[94m0.7263\u001b[0m        \u001b[36m0.8552\u001b[0m  0.0000  3.6936\n",
            "     20  0.5131       0.8505    0.3673        \u001b[31m0.9245\u001b[0m        \u001b[94m0.7262\u001b[0m        \u001b[36m0.8547\u001b[0m  0.0000  3.6551\n",
            "     21  0.5082       0.8502    0.3624        \u001b[31m0.9245\u001b[0m        \u001b[94m0.7262\u001b[0m        \u001b[36m0.8546\u001b[0m  0.0000  4.5742\n",
            "     22  0.5086       0.8502    0.3628        \u001b[31m0.9244\u001b[0m        0.7264        \u001b[36m0.8546\u001b[0m  0.0000  3.7080\n",
            "     23  0.5092       0.8504    0.3634        \u001b[31m0.9244\u001b[0m        \u001b[94m0.7257\u001b[0m        \u001b[36m0.8545\u001b[0m  0.0000  3.6941\n",
            "     24  0.5089       0.8503    0.3631        0.9244        \u001b[94m0.7254\u001b[0m        0.8545  0.0000  4.0459\n",
            "     25  0.5083       0.8503    0.3625        0.9244        \u001b[94m0.7243\u001b[0m        0.8546  0.0000  4.0134\n",
            "     26  0.5087       0.8504    0.3628        0.9244        0.7248        0.8546  0.0000  3.8564\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 3/3; 27/36] END lr=0.001, module__dropout=0.3, module__linear_size=600, module__size_emb=120;, score=-1.111 total time= 1.9min\n",
            "[CV 1/3; 28/36] START lr=0.001, module__dropout=0.5, module__linear_size=400, module__size_emb=30\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4419\u001b[0m       \u001b[32m0.8398\u001b[0m    \u001b[35m0.2998\u001b[0m        \u001b[31m1.0049\u001b[0m        \u001b[94m1.1071\u001b[0m        \u001b[36m1.0098\u001b[0m  0.0010  3.6376\n",
            "      2  \u001b[36m0.4865\u001b[0m       0.8346    \u001b[35m0.3433\u001b[0m        \u001b[31m0.9805\u001b[0m        \u001b[94m0.9082\u001b[0m        \u001b[36m0.9614\u001b[0m  0.0010  3.5584\n",
            "      3  \u001b[36m0.5725\u001b[0m       0.8140    \u001b[35m0.4415\u001b[0m        \u001b[31m0.9774\u001b[0m        \u001b[94m0.8746\u001b[0m        \u001b[36m0.9553\u001b[0m  0.0010  4.3501\n",
            "      4  0.4976       0.8352    0.3543        \u001b[31m0.9686\u001b[0m        \u001b[94m0.8592\u001b[0m        \u001b[36m0.9381\u001b[0m  0.0010  3.7118\n",
            "      5  0.5172       0.8307    0.3755        \u001b[31m0.9652\u001b[0m        \u001b[94m0.8452\u001b[0m        \u001b[36m0.9315\u001b[0m  0.0010  3.6874\n",
            "      6  0.5602       0.8242    0.4243        \u001b[31m0.9635\u001b[0m        \u001b[94m0.8375\u001b[0m        \u001b[36m0.9283\u001b[0m  0.0010  4.0150\n",
            "      7  0.5060       0.8388    0.3623        \u001b[31m0.9603\u001b[0m        \u001b[94m0.8299\u001b[0m        \u001b[36m0.9221\u001b[0m  0.0010  4.0528\n",
            "      8  0.5174       0.8372    0.3744        \u001b[31m0.9601\u001b[0m        \u001b[94m0.8069\u001b[0m        \u001b[36m0.9218\u001b[0m  0.0001  3.6125\n",
            "      9  0.5160       0.8380    0.3728        \u001b[31m0.9596\u001b[0m        \u001b[94m0.8052\u001b[0m        \u001b[36m0.9208\u001b[0m  0.0001  3.6563\n",
            "     10  0.5111       0.8388    0.3675        \u001b[31m0.9588\u001b[0m        \u001b[94m0.8039\u001b[0m        \u001b[36m0.9192\u001b[0m  0.0001  4.5060\n",
            "     11  0.5258       0.8376    0.3832        0.9590        \u001b[94m0.8033\u001b[0m        0.9197  0.0001  3.5441\n",
            "     12  0.5069       \u001b[32m0.8418\u001b[0m    0.3626        \u001b[31m0.9581\u001b[0m        \u001b[94m0.8016\u001b[0m        \u001b[36m0.9179\u001b[0m  0.0001  3.5775\n",
            "     13  0.5230       0.8400    0.3797        \u001b[31m0.9579\u001b[0m        \u001b[94m0.8005\u001b[0m        \u001b[36m0.9176\u001b[0m  0.0001  4.4529\n",
            "     14  0.5072       0.8413    0.3630        \u001b[31m0.9571\u001b[0m        \u001b[94m0.7986\u001b[0m        \u001b[36m0.9161\u001b[0m  0.0001  3.5814\n",
            "     15  0.5202       0.8388    0.3770        0.9575        \u001b[94m0.7960\u001b[0m        0.9167  0.0000  3.6384\n",
            "     16  0.5223       0.8388    0.3792        0.9576        \u001b[94m0.7960\u001b[0m        0.9170  0.0000  4.0517\n",
            "     17  0.5218       0.8391    0.3786        0.9576        \u001b[94m0.7957\u001b[0m        0.9169  0.0000  3.7485\n",
            "     18  0.5208       0.8394    0.3775        0.9575        0.7964        0.9168  0.0000  3.6310\n",
            "     19  0.5204       0.8392    0.3772        0.9574        0.7962        0.9167  0.0000  3.8696\n",
            "     20  0.5227       0.8388    0.3797        0.9575        \u001b[94m0.7950\u001b[0m        0.9168  0.0000  4.3887\n",
            "     21  0.5217       0.8389    0.3786        0.9574        0.7962        0.9167  0.0000  3.6244\n",
            "     22  0.5216       0.8390    0.3784        0.9574        \u001b[94m0.7948\u001b[0m        0.9167  0.0000  3.6260\n",
            "     23  0.5215       0.8393    0.3782        0.9574        \u001b[94m0.7937\u001b[0m        0.9167  0.0000  4.3629\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 1/3; 28/36] END lr=0.001, module__dropout=0.5, module__linear_size=400, module__size_emb=30;, score=-1.149 total time= 1.6min\n",
            "[CV 2/3; 28/36] START lr=0.001, module__dropout=0.5, module__linear_size=400, module__size_emb=30\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.5192\u001b[0m       \u001b[32m0.8337\u001b[0m    \u001b[35m0.3770\u001b[0m        \u001b[31m0.9852\u001b[0m        \u001b[94m1.1607\u001b[0m        \u001b[36m0.9706\u001b[0m  0.0010  3.9571\n",
            "      2  0.4157       \u001b[32m0.8636\u001b[0m    0.2738        \u001b[31m0.9563\u001b[0m        \u001b[94m0.9191\u001b[0m        \u001b[36m0.9145\u001b[0m  0.0010  4.2973\n",
            "      3  0.5082       0.8350    0.3653        \u001b[31m0.9469\u001b[0m        \u001b[94m0.8791\u001b[0m        \u001b[36m0.8967\u001b[0m  0.0010  3.6165\n",
            "      4  0.4166       0.8607    0.2748        \u001b[31m0.9462\u001b[0m        \u001b[94m0.8614\u001b[0m        \u001b[36m0.8953\u001b[0m  0.0010  3.6700\n",
            "      5  \u001b[36m0.5400\u001b[0m       0.8331    \u001b[35m0.3995\u001b[0m        \u001b[31m0.9391\u001b[0m        \u001b[94m0.8468\u001b[0m        \u001b[36m0.8819\u001b[0m  0.0010  4.5140\n",
            "      6  0.5153       0.8440    0.3709        \u001b[31m0.9362\u001b[0m        \u001b[94m0.8341\u001b[0m        \u001b[36m0.8764\u001b[0m  0.0010  3.5998\n",
            "      7  0.5369       0.8413    0.3943        \u001b[31m0.9316\u001b[0m        \u001b[94m0.8240\u001b[0m        \u001b[36m0.8679\u001b[0m  0.0010  3.6691\n",
            "      8  0.5295       0.8444    0.3857        0.9319        \u001b[94m0.7969\u001b[0m        0.8685  0.0001  4.2256\n",
            "      9  \u001b[36m0.5485\u001b[0m       0.8395    \u001b[35m0.4073\u001b[0m        \u001b[31m0.9308\u001b[0m        \u001b[94m0.7929\u001b[0m        \u001b[36m0.8665\u001b[0m  0.0001  3.5938\n",
            "     10  0.5280       0.8434    0.3842        \u001b[31m0.9297\u001b[0m        \u001b[94m0.7889\u001b[0m        \u001b[36m0.8644\u001b[0m  0.0001  3.5731\n",
            "     11  0.5291       0.8419    0.3858        \u001b[31m0.9289\u001b[0m        0.7894        \u001b[36m0.8629\u001b[0m  0.0001  4.2245\n",
            "     12  0.5258       0.8423    0.3822        0.9290        \u001b[94m0.7867\u001b[0m        0.8630  0.0001  3.8350\n",
            "     13  0.5381       0.8408    0.3957        \u001b[31m0.9287\u001b[0m        \u001b[94m0.7848\u001b[0m        \u001b[36m0.8624\u001b[0m  0.0001  3.7410\n",
            "     14  0.5431       0.8397    0.4013        \u001b[31m0.9278\u001b[0m        \u001b[94m0.7834\u001b[0m        \u001b[36m0.8609\u001b[0m  0.0001  3.8145\n",
            "     15  0.5350       0.8415    0.3921        \u001b[31m0.9278\u001b[0m        \u001b[94m0.7809\u001b[0m        \u001b[36m0.8608\u001b[0m  0.0000  4.2222\n",
            "     16  0.5311       0.8424    0.3877        0.9279        \u001b[94m0.7803\u001b[0m        0.8609  0.0000  3.5877\n",
            "     17  0.5315       0.8421    0.3883        \u001b[31m0.9278\u001b[0m        \u001b[94m0.7798\u001b[0m        \u001b[36m0.8608\u001b[0m  0.0000  3.6102\n",
            "     18  0.5309       0.8424    0.3876        0.9278        0.7801        0.8608  0.0000  4.5054\n",
            "     19  0.5294       0.8425    0.3859        \u001b[31m0.9278\u001b[0m        \u001b[94m0.7798\u001b[0m        \u001b[36m0.8608\u001b[0m  0.0000  3.5737\n",
            "     20  0.5332       0.8417    0.3902        \u001b[31m0.9277\u001b[0m        \u001b[94m0.7790\u001b[0m        \u001b[36m0.8606\u001b[0m  0.0000  3.6505\n",
            "     21  0.5333       0.8420    0.3902        \u001b[31m0.9276\u001b[0m        0.7792        \u001b[36m0.8604\u001b[0m  0.0000  4.3986\n",
            "     22  0.5329       0.8422    0.3898        0.9276        \u001b[94m0.7785\u001b[0m        0.8605  0.0000  3.6444\n",
            "     23  0.5324       0.8424    0.3892        \u001b[31m0.9276\u001b[0m        0.7796        \u001b[36m0.8604\u001b[0m  0.0000  3.6418\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 2/3; 28/36] END lr=0.001, module__dropout=0.5, module__linear_size=400, module__size_emb=30;, score=-1.016 total time= 1.6min\n",
            "[CV 3/3; 28/36] START lr=0.001, module__dropout=0.5, module__linear_size=400, module__size_emb=30\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4448\u001b[0m       \u001b[32m0.8380\u001b[0m    \u001b[35m0.3028\u001b[0m        \u001b[31m0.9911\u001b[0m        \u001b[94m1.1528\u001b[0m        \u001b[36m0.9823\u001b[0m  0.0010  3.3395\n",
            "      2  \u001b[36m0.4635\u001b[0m       \u001b[32m0.8387\u001b[0m    \u001b[35m0.3202\u001b[0m        \u001b[31m0.9618\u001b[0m        \u001b[94m0.9061\u001b[0m        \u001b[36m0.9251\u001b[0m  0.0010  3.6773\n",
            "      3  \u001b[36m0.4987\u001b[0m       0.8280    \u001b[35m0.3568\u001b[0m        \u001b[31m0.9518\u001b[0m        \u001b[94m0.8607\u001b[0m        \u001b[36m0.9059\u001b[0m  0.0010  4.5326\n",
            "      4  0.4492       \u001b[32m0.8460\u001b[0m    0.3058        0.9535        \u001b[94m0.8412\u001b[0m        0.9091  0.0010  3.7146\n",
            "      5  0.4441       \u001b[32m0.8481\u001b[0m    0.3008        \u001b[31m0.9473\u001b[0m        \u001b[94m0.8275\u001b[0m        \u001b[36m0.8975\u001b[0m  0.0010  3.7921\n",
            "      6  \u001b[36m0.5505\u001b[0m       0.8255    \u001b[35m0.4130\u001b[0m        \u001b[31m0.9452\u001b[0m        \u001b[94m0.8195\u001b[0m        \u001b[36m0.8934\u001b[0m  0.0010  4.3482\n",
            "      7  0.5051       0.8384    0.3614        \u001b[31m0.9391\u001b[0m        \u001b[94m0.8104\u001b[0m        \u001b[36m0.8820\u001b[0m  0.0010  3.8536\n",
            "      8  0.4997       0.8419    0.3552        0.9413        \u001b[94m0.7878\u001b[0m        0.8861  0.0001  3.6729\n",
            "      9  0.4746       0.8469    0.3297        0.9413        \u001b[94m0.7828\u001b[0m        0.8861  0.0001  3.8679\n",
            "     10  0.4915       0.8435    0.3468        0.9403        \u001b[94m0.7823\u001b[0m        0.8842  0.0001  4.3800\n",
            "     11  0.4918       0.8439    0.3470        0.9398        \u001b[94m0.7810\u001b[0m        0.8833  0.0001  3.7463\n",
            "     12  0.5029       0.8424    0.3585        0.9393        \u001b[94m0.7784\u001b[0m        0.8823  0.0001  3.6199\n",
            "     13  0.4789       0.8455    0.3340        0.9395        \u001b[94m0.7773\u001b[0m        0.8827  0.0001  4.5243\n",
            "     14  0.5081       0.8418    0.3638        \u001b[31m0.9387\u001b[0m        \u001b[94m0.7761\u001b[0m        \u001b[36m0.8811\u001b[0m  0.0001  3.7138\n",
            "     15  0.5030       0.8421    0.3586        \u001b[31m0.9386\u001b[0m        \u001b[94m0.7735\u001b[0m        \u001b[36m0.8809\u001b[0m  0.0000  3.7441\n",
            "     16  0.5042       0.8419    0.3599        \u001b[31m0.9385\u001b[0m        \u001b[94m0.7710\u001b[0m        \u001b[36m0.8809\u001b[0m  0.0000  4.5351\n",
            "     17  0.5027       0.8417    0.3584        \u001b[31m0.9385\u001b[0m        0.7726        \u001b[36m0.8808\u001b[0m  0.0000  3.6882\n",
            "     18  0.5035       0.8416    0.3592        0.9385        0.7723        0.8808  0.0000  3.5818\n",
            "     19  0.5031       0.8420    0.3587        \u001b[31m0.9384\u001b[0m        0.7718        \u001b[36m0.8805\u001b[0m  0.0000  4.4999\n",
            "     20  0.5027       0.8420    0.3583        \u001b[31m0.9383\u001b[0m        0.7720        \u001b[36m0.8805\u001b[0m  0.0000  3.4830\n",
            "     21  0.5021       0.8423    0.3577        \u001b[31m0.9383\u001b[0m        0.7711        \u001b[36m0.8804\u001b[0m  0.0000  3.6859\n",
            "     22  0.5017       0.8422    0.3573        \u001b[31m0.9383\u001b[0m        0.7726        \u001b[36m0.8804\u001b[0m  0.0000  3.9439\n",
            "     23  0.5016       0.8423    0.3571        \u001b[31m0.9383\u001b[0m        0.7713        \u001b[36m0.8804\u001b[0m  0.0000  4.1035\n",
            "     24  0.5013       0.8423    0.3568        0.9383        0.7721        0.8804  0.0000  3.6605\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 3/3; 28/36] END lr=0.001, module__dropout=0.5, module__linear_size=400, module__size_emb=30;, score=-1.089 total time= 1.7min\n",
            "[CV 1/3; 29/36] START lr=0.001, module__dropout=0.5, module__linear_size=400, module__size_emb=60\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4235\u001b[0m       \u001b[32m0.8559\u001b[0m    \u001b[35m0.2814\u001b[0m        \u001b[31m1.0031\u001b[0m        \u001b[94m1.0881\u001b[0m        \u001b[36m1.0062\u001b[0m  0.0010  3.6621\n",
            "      2  \u001b[36m0.5056\u001b[0m       0.8290    \u001b[35m0.3637\u001b[0m        \u001b[31m0.9796\u001b[0m        \u001b[94m0.9083\u001b[0m        \u001b[36m0.9596\u001b[0m  0.0010  3.5609\n",
            "      3  0.4994       0.8279    0.3576        \u001b[31m0.9728\u001b[0m        \u001b[94m0.8770\u001b[0m        \u001b[36m0.9463\u001b[0m  0.0010  3.8737\n",
            "      4  0.4748       0.8411    0.3308        \u001b[31m0.9650\u001b[0m        \u001b[94m0.8577\u001b[0m        \u001b[36m0.9313\u001b[0m  0.0010  4.2072\n",
            "      5  \u001b[36m0.5120\u001b[0m       0.8347    \u001b[35m0.3693\u001b[0m        \u001b[31m0.9617\u001b[0m        \u001b[94m0.8466\u001b[0m        \u001b[36m0.9249\u001b[0m  0.0010  3.6549\n",
            "      6  0.4886       0.8384    0.3448        \u001b[31m0.9599\u001b[0m        \u001b[94m0.8368\u001b[0m        \u001b[36m0.9213\u001b[0m  0.0010  3.6653\n",
            "      7  0.5002       0.8426    0.3557        \u001b[31m0.9574\u001b[0m        \u001b[94m0.8290\u001b[0m        \u001b[36m0.9167\u001b[0m  0.0010  4.4906\n",
            "      8  \u001b[36m0.5412\u001b[0m       0.8329    \u001b[35m0.4008\u001b[0m        \u001b[31m0.9570\u001b[0m        \u001b[94m0.8009\u001b[0m        \u001b[36m0.9158\u001b[0m  0.0001  3.6363\n",
            "      9  0.5237       0.8379    0.3808        \u001b[31m0.9553\u001b[0m        \u001b[94m0.7971\u001b[0m        \u001b[36m0.9126\u001b[0m  0.0001  3.6632\n",
            "     10  0.5216       0.8392    0.3784        \u001b[31m0.9545\u001b[0m        \u001b[94m0.7964\u001b[0m        \u001b[36m0.9111\u001b[0m  0.0001  4.4303\n",
            "     11  0.5198       0.8393    0.3765        \u001b[31m0.9541\u001b[0m        \u001b[94m0.7941\u001b[0m        \u001b[36m0.9102\u001b[0m  0.0001  3.6670\n",
            "     12  0.5279       0.8374    0.3854        \u001b[31m0.9539\u001b[0m        \u001b[94m0.7932\u001b[0m        \u001b[36m0.9099\u001b[0m  0.0001  3.2881\n",
            "     13  0.5150       0.8424    0.3709        \u001b[31m0.9529\u001b[0m        \u001b[94m0.7918\u001b[0m        \u001b[36m0.9080\u001b[0m  0.0001  4.2086\n",
            "     14  0.5173       0.8424    0.3732        \u001b[31m0.9526\u001b[0m        \u001b[94m0.7902\u001b[0m        \u001b[36m0.9074\u001b[0m  0.0001  3.8050\n",
            "     15  0.5181       0.8425    0.3740        \u001b[31m0.9525\u001b[0m        \u001b[94m0.7867\u001b[0m        \u001b[36m0.9073\u001b[0m  0.0000  3.6822\n",
            "     16  0.5193       0.8409    0.3756        \u001b[31m0.9525\u001b[0m        \u001b[94m0.7854\u001b[0m        \u001b[36m0.9073\u001b[0m  0.0000  3.9567\n",
            "     17  0.5209       0.8396    0.3776        0.9525        0.7857        0.9073  0.0000  4.3201\n",
            "     18  0.5217       0.8394    0.3784        \u001b[31m0.9524\u001b[0m        \u001b[94m0.7844\u001b[0m        \u001b[36m0.9072\u001b[0m  0.0000  3.7184\n",
            "     19  0.5195       0.8405    0.3759        \u001b[31m0.9523\u001b[0m        0.7866        \u001b[36m0.9069\u001b[0m  0.0000  3.8138\n",
            "     20  0.5189       0.8401    0.3754        \u001b[31m0.9523\u001b[0m        0.7854        \u001b[36m0.9069\u001b[0m  0.0000  4.6096\n",
            "     21  0.5200       0.8397    0.3766        0.9523        \u001b[94m0.7836\u001b[0m        0.9069  0.0000  3.7202\n",
            "     22  0.5204       0.8398    0.3770        0.9523        0.7844        0.9069  0.0000  3.7172\n",
            "     23  0.5201       0.8399    0.3766        \u001b[31m0.9523\u001b[0m        0.7837        \u001b[36m0.9069\u001b[0m  0.0000  4.5116\n",
            "     24  0.5202       0.8400    0.3768        0.9523        0.7846        0.9069  0.0000  3.6998\n",
            "     25  0.5199       0.8400    0.3765        \u001b[31m0.9523\u001b[0m        \u001b[94m0.7836\u001b[0m        \u001b[36m0.9068\u001b[0m  0.0000  3.6880\n",
            "     26  0.5203       0.8398    0.3769        0.9523        0.7853        0.9069  0.0000  4.7450\n",
            "     27  0.5202       0.8400    0.3768        \u001b[31m0.9523\u001b[0m        0.7848        \u001b[36m0.9068\u001b[0m  0.0000  3.7688\n",
            "     28  0.5202       0.8400    0.3768        \u001b[31m0.9523\u001b[0m        0.7842        \u001b[36m0.9068\u001b[0m  0.0000  3.7368\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 1/3; 29/36] END lr=0.001, module__dropout=0.5, module__linear_size=400, module__size_emb=60;, score=-1.145 total time= 1.9min\n",
            "[CV 2/3; 29/36] START lr=0.001, module__dropout=0.5, module__linear_size=400, module__size_emb=60\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4275\u001b[0m       \u001b[32m0.8521\u001b[0m    \u001b[35m0.2853\u001b[0m        \u001b[31m0.9829\u001b[0m        \u001b[94m1.1403\u001b[0m        \u001b[36m0.9660\u001b[0m  0.0010  3.6988\n",
            "      2  \u001b[36m0.5580\u001b[0m       0.8203    \u001b[35m0.4228\u001b[0m        \u001b[31m0.9536\u001b[0m        \u001b[94m0.9133\u001b[0m        \u001b[36m0.9094\u001b[0m  0.0010  3.6552\n",
            "      3  0.4782       0.8463    0.3332        \u001b[31m0.9464\u001b[0m        \u001b[94m0.8742\u001b[0m        \u001b[36m0.8956\u001b[0m  0.0010  4.5338\n",
            "      4  0.4524       \u001b[32m0.8592\u001b[0m    0.3071        \u001b[31m0.9446\u001b[0m        \u001b[94m0.8569\u001b[0m        \u001b[36m0.8924\u001b[0m  0.0010  3.3624\n",
            "      5  0.4473       \u001b[32m0.8633\u001b[0m    0.3019        \u001b[31m0.9403\u001b[0m        \u001b[94m0.8428\u001b[0m        \u001b[36m0.8842\u001b[0m  0.0010  3.5996\n",
            "      6  0.5262       0.8406    0.3830        \u001b[31m0.9335\u001b[0m        \u001b[94m0.8333\u001b[0m        \u001b[36m0.8714\u001b[0m  0.0010  4.4993\n",
            "      7  0.5308       0.8470    0.3865        \u001b[31m0.9293\u001b[0m        \u001b[94m0.8212\u001b[0m        \u001b[36m0.8635\u001b[0m  0.0010  3.7313\n",
            "      8  0.5366       0.8458    0.3929        \u001b[31m0.9287\u001b[0m        \u001b[94m0.7888\u001b[0m        \u001b[36m0.8624\u001b[0m  0.0001  3.6355\n",
            "      9  0.5294       0.8471    0.3850        \u001b[31m0.9273\u001b[0m        \u001b[94m0.7838\u001b[0m        \u001b[36m0.8598\u001b[0m  0.0001  4.2249\n",
            "     10  0.5267       0.8476    0.3820        \u001b[31m0.9266\u001b[0m        \u001b[94m0.7802\u001b[0m        \u001b[36m0.8587\u001b[0m  0.0001  3.8860\n",
            "     11  0.5216       0.8501    0.3762        0.9266        \u001b[94m0.7787\u001b[0m        0.8587  0.0001  3.5175\n",
            "     12  0.5237       0.8495    0.3785        \u001b[31m0.9261\u001b[0m        \u001b[94m0.7752\u001b[0m        \u001b[36m0.8576\u001b[0m  0.0001  3.7802\n",
            "     13  0.5405       0.8467    0.3970        \u001b[31m0.9246\u001b[0m        \u001b[94m0.7750\u001b[0m        \u001b[36m0.8548\u001b[0m  0.0001  4.4599\n",
            "     14  0.5282       0.8489    0.3833        \u001b[31m0.9240\u001b[0m        \u001b[94m0.7746\u001b[0m        \u001b[36m0.8538\u001b[0m  0.0001  3.6157\n",
            "     15  0.5350       0.8458    0.3912        0.9241        \u001b[94m0.7687\u001b[0m        0.8539  0.0000  3.6980\n",
            "     16  0.5345       0.8462    0.3906        0.9242        0.7690        0.8541  0.0000  4.4465\n",
            "     17  0.5330       0.8469    0.3889        0.9242        0.7692        0.8541  0.0000  3.7229\n",
            "     18  0.5363       0.8460    0.3926        0.9242        \u001b[94m0.7675\u001b[0m        0.8541  0.0000  3.7310\n",
            "     19  0.5365       0.8464    0.3927        0.9241        0.7684        0.8539  0.0000  4.5801\n",
            "     20  0.5345       0.8471    0.3904        0.9240        0.7680        0.8538  0.0000  3.6465\n",
            "     21  0.5370       0.8464    0.3932        0.9240        0.7679        0.8538  0.0000  3.6055\n",
            "     22  0.5367       0.8466    0.3928        \u001b[31m0.9240\u001b[0m        0.7680        \u001b[36m0.8538\u001b[0m  0.0000  3.8576\n",
            "     23  0.5365       0.8469    0.3927        \u001b[31m0.9240\u001b[0m        \u001b[94m0.7675\u001b[0m        \u001b[36m0.8538\u001b[0m  0.0000  4.0364\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 2/3; 29/36] END lr=0.001, module__dropout=0.5, module__linear_size=400, module__size_emb=60;, score=-1.019 total time= 1.6min\n",
            "[CV 3/3; 29/36] START lr=0.001, module__dropout=0.5, module__linear_size=400, module__size_emb=60\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4398\u001b[0m       \u001b[32m0.8424\u001b[0m    \u001b[35m0.2976\u001b[0m        \u001b[31m0.9887\u001b[0m        \u001b[94m1.1211\u001b[0m        \u001b[36m0.9775\u001b[0m  0.0010  4.4341\n",
            "      2  \u001b[36m0.4976\u001b[0m       0.8257    \u001b[35m0.3561\u001b[0m        \u001b[31m0.9593\u001b[0m        \u001b[94m0.9005\u001b[0m        \u001b[36m0.9202\u001b[0m  0.0010  3.6059\n",
            "      3  0.4520       0.8423    0.3089        \u001b[31m0.9551\u001b[0m        \u001b[94m0.8586\u001b[0m        \u001b[36m0.9123\u001b[0m  0.0010  3.6406\n",
            "      4  \u001b[36m0.5069\u001b[0m       0.8319    \u001b[35m0.3645\u001b[0m        \u001b[31m0.9460\u001b[0m        \u001b[94m0.8400\u001b[0m        \u001b[36m0.8949\u001b[0m  0.0010  4.2614\n",
            "      5  \u001b[36m0.5692\u001b[0m       0.8196    \u001b[35m0.4360\u001b[0m        \u001b[31m0.9456\u001b[0m        \u001b[94m0.8248\u001b[0m        \u001b[36m0.8942\u001b[0m  0.0010  3.9185\n",
            "      6  0.4588       \u001b[32m0.8508\u001b[0m    0.3141        \u001b[31m0.9419\u001b[0m        \u001b[94m0.8160\u001b[0m        \u001b[36m0.8871\u001b[0m  0.0010  3.5823\n",
            "      7  0.4623       0.8482    0.3177        \u001b[31m0.9415\u001b[0m        \u001b[94m0.8046\u001b[0m        \u001b[36m0.8864\u001b[0m  0.0010  3.7763\n",
            "      8  0.5034       0.8439    0.3586        \u001b[31m0.9388\u001b[0m        \u001b[94m0.7762\u001b[0m        \u001b[36m0.8813\u001b[0m  0.0001  4.3202\n",
            "      9  0.4810       0.8459    0.3361        0.9389        \u001b[94m0.7739\u001b[0m        0.8816  0.0001  3.6612\n",
            "     10  0.5185       0.8373    0.3756        \u001b[31m0.9371\u001b[0m        \u001b[94m0.7706\u001b[0m        \u001b[36m0.8781\u001b[0m  0.0001  3.7172\n",
            "     11  0.4951       0.8451    0.3501        \u001b[31m0.9368\u001b[0m        \u001b[94m0.7687\u001b[0m        \u001b[36m0.8775\u001b[0m  0.0001  4.5413\n",
            "     12  0.5256       0.8352    0.3834        \u001b[31m0.9361\u001b[0m        \u001b[94m0.7678\u001b[0m        \u001b[36m0.8762\u001b[0m  0.0001  3.5572\n",
            "     13  0.4878       0.8435    0.3432        \u001b[31m0.9357\u001b[0m        \u001b[94m0.7665\u001b[0m        \u001b[36m0.8756\u001b[0m  0.0001  3.6262\n",
            "     14  0.5128       0.8388    0.3693        \u001b[31m0.9349\u001b[0m        \u001b[94m0.7648\u001b[0m        \u001b[36m0.8740\u001b[0m  0.0001  4.5271\n",
            "     15  0.5071       0.8395    0.3633        \u001b[31m0.9348\u001b[0m        \u001b[94m0.7608\u001b[0m        \u001b[36m0.8739\u001b[0m  0.0000  3.7429\n",
            "     16  0.5024       0.8402    0.3583        0.9349        \u001b[94m0.7600\u001b[0m        0.8740  0.0000  3.6878\n",
            "     17  0.5026       0.8402    0.3585        \u001b[31m0.9348\u001b[0m        0.7606        \u001b[36m0.8738\u001b[0m  0.0000  4.1667\n",
            "     18  0.5033       0.8401    0.3593        \u001b[31m0.9347\u001b[0m        0.7609        \u001b[36m0.8737\u001b[0m  0.0000  3.8386\n",
            "     19  0.5025       0.8403    0.3585        \u001b[31m0.9346\u001b[0m        0.7609        \u001b[36m0.8736\u001b[0m  0.0000  3.3784\n",
            "     20  0.5050       0.8400    0.3611        \u001b[31m0.9346\u001b[0m        \u001b[94m0.7590\u001b[0m        \u001b[36m0.8734\u001b[0m  0.0000  3.8843\n",
            "     21  0.5010       0.8409    0.3568        \u001b[31m0.9346\u001b[0m        0.7606        \u001b[36m0.8734\u001b[0m  0.0000  4.4067\n",
            "     22  0.5013       0.8410    0.3570        0.9346        0.7601        0.8734  0.0000  3.6523\n",
            "     23  0.5019       0.8409    0.3577        \u001b[31m0.9346\u001b[0m        0.7600        \u001b[36m0.8734\u001b[0m  0.0000  3.6076\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 3/3; 29/36] END lr=0.001, module__dropout=0.5, module__linear_size=400, module__size_emb=60;, score=-1.099 total time= 1.6min\n",
            "[CV 1/3; 30/36] START lr=0.001, module__dropout=0.5, module__linear_size=400, module__size_emb=120\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.4765\u001b[0m       \u001b[32m0.8457\u001b[0m    \u001b[35m0.3317\u001b[0m        \u001b[31m0.9988\u001b[0m        \u001b[94m1.0752\u001b[0m        \u001b[36m0.9975\u001b[0m  0.0010  3.6666\n",
            "      2  0.4339       \u001b[32m0.8539\u001b[0m    0.2909        \u001b[31m0.9753\u001b[0m        \u001b[94m0.9030\u001b[0m        \u001b[36m0.9513\u001b[0m  0.0010  3.9596\n",
            "      3  0.4047       \u001b[32m0.8596\u001b[0m    0.2646        \u001b[31m0.9708\u001b[0m        \u001b[94m0.8708\u001b[0m        \u001b[36m0.9425\u001b[0m  0.0010  4.2136\n",
            "      4  \u001b[36m0.5545\u001b[0m       0.8220    \u001b[35m0.4184\u001b[0m        \u001b[31m0.9645\u001b[0m        \u001b[94m0.8537\u001b[0m        \u001b[36m0.9303\u001b[0m  0.0010  3.6763\n",
            "      5  0.4827       0.8449    0.3379        \u001b[31m0.9573\u001b[0m        \u001b[94m0.8395\u001b[0m        \u001b[36m0.9164\u001b[0m  0.0010  3.5923\n",
            "      6  0.5298       0.8321    0.3886        \u001b[31m0.9564\u001b[0m        \u001b[94m0.8309\u001b[0m        \u001b[36m0.9146\u001b[0m  0.0010  4.4849\n",
            "      7  0.5263       0.8373    0.3838        \u001b[31m0.9525\u001b[0m        \u001b[94m0.8212\u001b[0m        \u001b[36m0.9073\u001b[0m  0.0010  3.6748\n",
            "      8  0.5122       0.8432    0.3679        \u001b[31m0.9506\u001b[0m        \u001b[94m0.7918\u001b[0m        \u001b[36m0.9037\u001b[0m  0.0001  3.6167\n",
            "      9  0.5243       0.8381    0.3815        \u001b[31m0.9499\u001b[0m        \u001b[94m0.7877\u001b[0m        \u001b[36m0.9023\u001b[0m  0.0001  4.4725\n",
            "     10  0.5189       0.8406    0.3753        \u001b[31m0.9489\u001b[0m        \u001b[94m0.7864\u001b[0m        \u001b[36m0.9004\u001b[0m  0.0001  3.6610\n",
            "     11  0.5160       0.8424    0.3719        \u001b[31m0.9478\u001b[0m        \u001b[94m0.7833\u001b[0m        \u001b[36m0.8984\u001b[0m  0.0001  3.6418\n",
            "     12  0.5259       0.8407    0.3826        \u001b[31m0.9474\u001b[0m        0.7835        \u001b[36m0.8976\u001b[0m  0.0001  4.4527\n",
            "     13  0.5154       0.8429    0.3712        \u001b[31m0.9469\u001b[0m        \u001b[94m0.7809\u001b[0m        \u001b[36m0.8967\u001b[0m  0.0001  3.7442\n",
            "     14  0.5164       0.8440    0.3720        \u001b[31m0.9465\u001b[0m        \u001b[94m0.7793\u001b[0m        \u001b[36m0.8958\u001b[0m  0.0001  3.5647\n",
            "     15  0.5242       0.8423    0.3805        0.9465        \u001b[94m0.7744\u001b[0m        0.8959  0.0000  3.9305\n",
            "     16  0.5239       0.8419    0.3803        \u001b[31m0.9464\u001b[0m        0.7749        \u001b[36m0.8957\u001b[0m  0.0000  4.1680\n",
            "     17  0.5228       0.8418    0.3791        \u001b[31m0.9463\u001b[0m        0.7744        \u001b[36m0.8955\u001b[0m  0.0000  3.4210\n",
            "     18  0.5268       0.8410    0.3835        0.9464        \u001b[94m0.7737\u001b[0m        0.8956  0.0000  3.6566\n",
            "     19  0.5260       0.8413    0.3826        \u001b[31m0.9463\u001b[0m        \u001b[94m0.7729\u001b[0m        \u001b[36m0.8955\u001b[0m  0.0000  4.5964\n",
            "     20  0.5262       0.8412    0.3828        \u001b[31m0.9462\u001b[0m        0.7729        \u001b[36m0.8953\u001b[0m  0.0000  3.5677\n",
            "     21  0.5260       0.8410    0.3826        \u001b[31m0.9462\u001b[0m        0.7730        \u001b[36m0.8952\u001b[0m  0.0000  3.5894\n",
            "     22  0.5258       0.8411    0.3825        \u001b[31m0.9462\u001b[0m        \u001b[94m0.7720\u001b[0m        \u001b[36m0.8952\u001b[0m  0.0000  4.5642\n",
            "     23  0.5260       0.8412    0.3826        \u001b[31m0.9462\u001b[0m        0.7723        \u001b[36m0.8952\u001b[0m  0.0000  3.6874\n",
            "     24  0.5258       0.8413    0.3825        \u001b[31m0.9461\u001b[0m        0.7729        \u001b[36m0.8952\u001b[0m  0.0000  3.6027\n",
            "     25  0.5256       0.8412    0.3822        \u001b[31m0.9461\u001b[0m        \u001b[94m0.7718\u001b[0m        \u001b[36m0.8952\u001b[0m  0.0000  4.3763\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 1/3; 30/36] END lr=0.001, module__dropout=0.5, module__linear_size=400, module__size_emb=120;, score=-1.138 total time= 1.7min\n",
            "[CV 2/3; 30/36] START lr=0.001, module__dropout=0.5, module__linear_size=400, module__size_emb=120\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.3765\u001b[0m       \u001b[32m0.8678\u001b[0m    \u001b[35m0.2404\u001b[0m        \u001b[31m0.9873\u001b[0m        \u001b[94m1.1250\u001b[0m        \u001b[36m0.9748\u001b[0m  0.0010  3.7102\n",
            "      2  \u001b[36m0.5282\u001b[0m       0.8312    \u001b[35m0.3871\u001b[0m        \u001b[31m0.9513\u001b[0m        \u001b[94m0.9132\u001b[0m        \u001b[36m0.9050\u001b[0m  0.0010  4.5135\n",
            "      3  \u001b[36m0.5308\u001b[0m       0.8336    \u001b[35m0.3893\u001b[0m        \u001b[31m0.9452\u001b[0m        \u001b[94m0.8738\u001b[0m        \u001b[36m0.8934\u001b[0m  0.0010  3.7136\n",
            "      4  0.4548       0.8572    0.3095        \u001b[31m0.9429\u001b[0m        \u001b[94m0.8553\u001b[0m        \u001b[36m0.8891\u001b[0m  0.0010  3.7283\n",
            "      5  \u001b[36m0.5454\u001b[0m       0.8371    \u001b[35m0.4045\u001b[0m        \u001b[31m0.9341\u001b[0m        \u001b[94m0.8408\u001b[0m        \u001b[36m0.8725\u001b[0m  0.0010  4.5586\n",
            "      6  \u001b[36m0.5550\u001b[0m       0.8387    \u001b[35m0.4147\u001b[0m        \u001b[31m0.9304\u001b[0m        \u001b[94m0.8287\u001b[0m        \u001b[36m0.8657\u001b[0m  0.0010  3.6913\n",
            "      7  \u001b[36m0.5644\u001b[0m       0.8384    \u001b[35m0.4253\u001b[0m        \u001b[31m0.9257\u001b[0m        \u001b[94m0.8152\u001b[0m        \u001b[36m0.8569\u001b[0m  0.0010  3.6689\n",
            "      8  0.5368       0.8484    0.3927        \u001b[31m0.9249\u001b[0m        \u001b[94m0.7822\u001b[0m        \u001b[36m0.8554\u001b[0m  0.0001  4.2082\n",
            "      9  0.5356       0.8490    0.3912        \u001b[31m0.9239\u001b[0m        \u001b[94m0.7759\u001b[0m        \u001b[36m0.8536\u001b[0m  0.0001  3.8287\n",
            "     10  0.5341       0.8490    0.3896        \u001b[31m0.9233\u001b[0m        \u001b[94m0.7725\u001b[0m        \u001b[36m0.8525\u001b[0m  0.0001  3.3854\n",
            "     11  0.5185       0.8546    0.3722        \u001b[31m0.9226\u001b[0m        \u001b[94m0.7709\u001b[0m        \u001b[36m0.8512\u001b[0m  0.0001  3.7361\n",
            "     12  0.5349       0.8500    0.3902        \u001b[31m0.9212\u001b[0m        \u001b[94m0.7685\u001b[0m        \u001b[36m0.8487\u001b[0m  0.0001  4.4273\n",
            "     13  0.5375       0.8494    0.3932        \u001b[31m0.9210\u001b[0m        \u001b[94m0.7649\u001b[0m        \u001b[36m0.8482\u001b[0m  0.0001  3.6235\n",
            "     14  0.5341       0.8491    0.3895        \u001b[31m0.9203\u001b[0m        \u001b[94m0.7628\u001b[0m        \u001b[36m0.8469\u001b[0m  0.0001  3.6342\n"
          ]
        }
      ],
      "source": [
        "# params = {\n",
        "#     'lr': [0.001, 0.01],\n",
        "#     'module__size_emb': [30, 60, 120],\n",
        "#     'module__dropout': [0.5],\n",
        "#     'module__linear_size': [400, 500, 600]\n",
        "# }\n",
        "params = {\n",
        "    'lr': [0.01, 0.001],\n",
        "    'module__size_emb': [30, 60, 120],\n",
        "    'module__dropout': [0.3, 0.5],\n",
        "    'module__linear_size': [400, 500, 600]\n",
        "}\n",
        "gs = GridSearchCV(deepnwidenet,\n",
        "                  params,\n",
        "                  verbose=50,\n",
        "                  refit=False,\n",
        "                  #pre_dispatch=8,\n",
        "                  n_jobs=1,\n",
        "                  cv=3,\n",
        "                  scoring='neg_mean_squared_error')\n",
        "\n",
        "X_ds = SliceDataset(train, idx=0)\n",
        "y_ds = SliceDataset(train, idx=1)\n",
        "gs.fit(X_ds, y_ds)\n",
        "\n",
        "print(gs.best_score_, gs.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Y-Z1LF7NgJH"
      },
      "source": [
        "### Two embeddings - Basic matrix factorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgHRNPN3NgJH"
      },
      "source": [
        "#### Manually specify hyperparamers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Y_9-m9v_3q03"
      },
      "outputs": [],
      "source": [
        "twoembedsnet = NeuralNet(\n",
        "    twoembeds,\n",
        "    module__size_emb=128,\n",
        "    module__y_range=train.y_range,\n",
        "    max_epochs=30,\n",
        "    lr=0.001,\n",
        "    optimizer=torch.optim.Adam,\n",
        "    criterion=torch.nn.MSELoss,\n",
        "    device=device,\n",
        "    iterator_train__batch_size=4096,\n",
        "    iterator_train__num_workers=0,\n",
        "    iterator_train__shuffle=True,\n",
        "    iterator_valid__batch_size=4096,\n",
        "    train_split=predefined_split(valid_dataset),\n",
        "    callbacks=[earlystopping,\n",
        "               epoch_rmse,\n",
        "               epoch_precision,\n",
        "               epoch_recall,\n",
        "               epoch_f1,\n",
        "               #checkpoint,\n",
        "               lr_scheduler]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9B_Z4LA6CPL",
        "outputId": "06275c20-9183-4adf-a8bc-3d9419214285"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.0000\u001b[0m       \u001b[32m0.0000\u001b[0m    \u001b[35m0.0000\u001b[0m        \u001b[31m1.2246\u001b[0m        \u001b[94m1.5324\u001b[0m        \u001b[36m1.4996\u001b[0m  0.0010  3.9334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      2  0.0000       0.0000    0.0000        \u001b[31m1.1950\u001b[0m        \u001b[94m1.4785\u001b[0m        \u001b[36m1.4281\u001b[0m  0.0010  6.3667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      3  0.0000       0.0000    0.0000        \u001b[31m1.1286\u001b[0m        \u001b[94m1.3628\u001b[0m        \u001b[36m1.2738\u001b[0m  0.0010  6.8263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Exception ignored in: <function _xla_gc_callback at 0x7e2ca9ea5900>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/lib/__init__.py\", line 97, in _xla_gc_callback\n",
            "    def _xla_gc_callback(*args):\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      4  0.0000       0.0000    0.0000        \u001b[31m1.0460\u001b[0m        \u001b[94m1.1728\u001b[0m        \u001b[36m1.0941\u001b[0m  0.0010  4.0623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5  \u001b[36m0.1819\u001b[0m       \u001b[32m0.8932\u001b[0m    \u001b[35m0.1013\u001b[0m        \u001b[31m0.9964\u001b[0m        \u001b[94m1.0072\u001b[0m        \u001b[36m0.9928\u001b[0m  0.0010  4.8329\n",
            "      6  \u001b[36m0.3995\u001b[0m       0.8504    \u001b[35m0.2611\u001b[0m        \u001b[31m0.9762\u001b[0m        \u001b[94m0.9211\u001b[0m        \u001b[36m0.9530\u001b[0m  0.0010  6.7228\n",
            "      7  \u001b[36m0.4543\u001b[0m       0.8476    \u001b[35m0.3103\u001b[0m        \u001b[31m0.9645\u001b[0m        \u001b[94m0.8750\u001b[0m        \u001b[36m0.9302\u001b[0m  0.0010  4.0794\n",
            "      8  \u001b[36m0.4569\u001b[0m       0.8480    \u001b[35m0.3126\u001b[0m        \u001b[31m0.9635\u001b[0m        \u001b[94m0.8496\u001b[0m        \u001b[36m0.9283\u001b[0m  0.0001  4.5573\n",
            "      9  \u001b[36m0.4577\u001b[0m       0.8487    \u001b[35m0.3134\u001b[0m        \u001b[31m0.9625\u001b[0m        \u001b[94m0.8464\u001b[0m        \u001b[36m0.9265\u001b[0m  0.0001  3.8373\n",
            "     10  \u001b[36m0.4592\u001b[0m       0.8484    \u001b[35m0.3148\u001b[0m        \u001b[31m0.9616\u001b[0m        \u001b[94m0.8434\u001b[0m        \u001b[36m0.9247\u001b[0m  0.0001  3.8165\n",
            "     11  \u001b[36m0.4598\u001b[0m       0.8479    \u001b[35m0.3154\u001b[0m        \u001b[31m0.9608\u001b[0m        \u001b[94m0.8404\u001b[0m        \u001b[36m0.9231\u001b[0m  0.0001  4.7428\n",
            "     12  \u001b[36m0.4611\u001b[0m       0.8474    \u001b[35m0.3167\u001b[0m        \u001b[31m0.9600\u001b[0m        \u001b[94m0.8375\u001b[0m        \u001b[36m0.9215\u001b[0m  0.0001  3.7458\n",
            "     13  \u001b[36m0.4631\u001b[0m       0.8472    \u001b[35m0.3187\u001b[0m        \u001b[31m0.9592\u001b[0m        \u001b[94m0.8348\u001b[0m        \u001b[36m0.9200\u001b[0m  0.0001  3.9336\n",
            "     14  \u001b[36m0.4649\u001b[0m       0.8467    \u001b[35m0.3204\u001b[0m        \u001b[31m0.9584\u001b[0m        \u001b[94m0.8320\u001b[0m        \u001b[36m0.9185\u001b[0m  0.0001  4.9544\n",
            "     15  0.4649       0.8465    0.3204        \u001b[31m0.9583\u001b[0m        \u001b[94m0.8301\u001b[0m        \u001b[36m0.9183\u001b[0m  0.0000  3.7956\n",
            "     16  \u001b[36m0.4650\u001b[0m       0.8463    \u001b[35m0.3205\u001b[0m        \u001b[31m0.9582\u001b[0m        \u001b[94m0.8298\u001b[0m        \u001b[36m0.9182\u001b[0m  0.0000  3.7759\n",
            "     17  \u001b[36m0.4652\u001b[0m       0.8462    \u001b[35m0.3208\u001b[0m        \u001b[31m0.9581\u001b[0m        \u001b[94m0.8295\u001b[0m        \u001b[36m0.9180\u001b[0m  0.0000  4.7780\n",
            "     18  \u001b[36m0.4655\u001b[0m       0.8463    \u001b[35m0.3211\u001b[0m        \u001b[31m0.9581\u001b[0m        \u001b[94m0.8293\u001b[0m        \u001b[36m0.9179\u001b[0m  0.0000  3.8151\n",
            "     19  \u001b[36m0.4656\u001b[0m       0.8464    \u001b[35m0.3212\u001b[0m        \u001b[31m0.9580\u001b[0m        \u001b[94m0.8290\u001b[0m        \u001b[36m0.9177\u001b[0m  0.0000  3.9693\n",
            "     20  \u001b[36m0.4658\u001b[0m       0.8464    \u001b[35m0.3213\u001b[0m        \u001b[31m0.9579\u001b[0m        \u001b[94m0.8287\u001b[0m        \u001b[36m0.9176\u001b[0m  0.0000  4.7926\n",
            "     21  0.4657       0.8464    0.3213        \u001b[31m0.9578\u001b[0m        \u001b[94m0.8285\u001b[0m        \u001b[36m0.9174\u001b[0m  0.0000  3.7950\n",
            "     22  0.4658       0.8464    0.3213        \u001b[31m0.9578\u001b[0m        \u001b[94m0.8283\u001b[0m        \u001b[36m0.9174\u001b[0m  0.0000  3.8222\n",
            "     23  \u001b[36m0.4659\u001b[0m       0.8463    \u001b[35m0.3214\u001b[0m        \u001b[31m0.9578\u001b[0m        \u001b[94m0.8283\u001b[0m        \u001b[36m0.9174\u001b[0m  0.0000  4.8962\n",
            "     24  \u001b[36m0.4660\u001b[0m       0.8463    \u001b[35m0.3215\u001b[0m        \u001b[31m0.9578\u001b[0m        \u001b[94m0.8282\u001b[0m        \u001b[36m0.9174\u001b[0m  0.0000  3.9541\n",
            "     25  0.4660       0.8463    0.3215        \u001b[31m0.9578\u001b[0m        \u001b[94m0.8282\u001b[0m        \u001b[36m0.9174\u001b[0m  0.0000  4.3550\n",
            "     26  0.4660       0.8463    0.3215        \u001b[31m0.9578\u001b[0m        \u001b[94m0.8282\u001b[0m        \u001b[36m0.9174\u001b[0m  0.0000  4.8228\n",
            "     27  0.4660       0.8463    0.3215        \u001b[31m0.9578\u001b[0m        \u001b[94m0.8282\u001b[0m        \u001b[36m0.9174\u001b[0m  0.0000  3.7901\n",
            "     28  0.4660       0.8463    0.3215        \u001b[31m0.9578\u001b[0m        \u001b[94m0.8281\u001b[0m        \u001b[36m0.9173\u001b[0m  0.0000  3.8803\n",
            "     29  0.4660       0.8463    0.3215        \u001b[31m0.9578\u001b[0m        \u001b[94m0.8281\u001b[0m        \u001b[36m0.9173\u001b[0m  0.0000  4.7463\n",
            "     30  0.4660       0.8463    0.3215        \u001b[31m0.9578\u001b[0m        \u001b[94m0.8281\u001b[0m        \u001b[36m0.9173\u001b[0m  0.0000  3.5497\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<class 'skorch.net.NeuralNet'>[initialized](\n",
              "  module_=twoembeds(\n",
              "    (emb_UserID): Embedding(943, 128)\n",
              "    (emb_MovieID): Embedding(1682, 128)\n",
              "    (emb_UserID_b): Embedding(943, 1)\n",
              "    (emb_MovieID_b): Embedding(1682, 1)\n",
              "  ),\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "twoembedsnet.fit(train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhyfVP2lNgJI"
      },
      "source": [
        "#### GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2QlpxyRTx7b",
        "outputId": "e04ac7f3-440d-4fa6-fd4d-7090c38ceef5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
            "[CV 1/3; 1/5] START lr=0.01, module__size_emb=600...............................\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr      dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  -------\n",
            "      1  \u001b[36m0.5491\u001b[0m       \u001b[32m0.8208\u001b[0m    \u001b[35m0.4125\u001b[0m        \u001b[31m1.0011\u001b[0m        \u001b[94m1.2399\u001b[0m        \u001b[36m1.0023\u001b[0m  0.0100  10.2454\n",
            "      2  0.5246       \u001b[32m0.9604\u001b[0m    0.3608        \u001b[31m0.8418\u001b[0m        \u001b[94m0.6454\u001b[0m        \u001b[36m0.7087\u001b[0m  0.0100  7.5932\n",
            "      3  \u001b[36m0.6880\u001b[0m       \u001b[32m0.9882\u001b[0m    \u001b[35m0.5277\u001b[0m        \u001b[31m0.7527\u001b[0m        \u001b[94m0.2842\u001b[0m        \u001b[36m0.5665\u001b[0m  0.0100  7.1685\n",
            "      4  \u001b[36m0.7137\u001b[0m       \u001b[32m0.9945\u001b[0m    \u001b[35m0.5566\u001b[0m        \u001b[31m0.7242\u001b[0m        \u001b[94m0.1335\u001b[0m        \u001b[36m0.5245\u001b[0m  0.0100  7.5094\n",
            "      5  0.7032       \u001b[32m0.9965\u001b[0m    0.5432        \u001b[31m0.7112\u001b[0m        \u001b[94m0.0844\u001b[0m        \u001b[36m0.5058\u001b[0m  0.0100  6.5538\n",
            "      6  0.6918       0.9958    0.5300        \u001b[31m0.7068\u001b[0m        \u001b[94m0.0638\u001b[0m        \u001b[36m0.4996\u001b[0m  0.0100  7.9443\n",
            "      7  0.6831       0.9953    0.5199        \u001b[31m0.7052\u001b[0m        \u001b[94m0.0549\u001b[0m        \u001b[36m0.4973\u001b[0m  0.0100  6.7776\n",
            "      8  0.6878       \u001b[32m0.9967\u001b[0m    0.5250        \u001b[31m0.7018\u001b[0m        \u001b[94m0.0410\u001b[0m        \u001b[36m0.4925\u001b[0m  0.0010  7.7987\n",
            "      9  0.6952       \u001b[32m0.9976\u001b[0m    0.5335        \u001b[31m0.6978\u001b[0m        \u001b[94m0.0312\u001b[0m        \u001b[36m0.4869\u001b[0m  0.0010  7.9405\n",
            "     10  0.6982       \u001b[32m0.9982\u001b[0m    0.5369        \u001b[31m0.6957\u001b[0m        \u001b[94m0.0244\u001b[0m        \u001b[36m0.4839\u001b[0m  0.0010  6.6216\n",
            "     11  0.7061       \u001b[32m0.9984\u001b[0m    0.5462        \u001b[31m0.6944\u001b[0m        \u001b[94m0.0208\u001b[0m        \u001b[36m0.4822\u001b[0m  0.0010  7.5715\n",
            "     12  0.7125       \u001b[32m0.9985\u001b[0m    0.5538        \u001b[31m0.6936\u001b[0m        \u001b[94m0.0186\u001b[0m        \u001b[36m0.4810\u001b[0m  0.0010  6.6451\n",
            "     13  \u001b[36m0.7177\u001b[0m       \u001b[32m0.9987\u001b[0m    \u001b[35m0.5601\u001b[0m        \u001b[31m0.6929\u001b[0m        \u001b[94m0.0171\u001b[0m        \u001b[36m0.4800\u001b[0m  0.0010  7.9728\n",
            "     14  \u001b[36m0.7235\u001b[0m       \u001b[32m0.9987\u001b[0m    \u001b[35m0.5671\u001b[0m        \u001b[31m0.6923\u001b[0m        \u001b[94m0.0158\u001b[0m        \u001b[36m0.4793\u001b[0m  0.0010  6.6293\n",
            "     15  \u001b[36m0.7237\u001b[0m       \u001b[32m0.9987\u001b[0m    \u001b[35m0.5674\u001b[0m        \u001b[31m0.6923\u001b[0m        \u001b[94m0.0149\u001b[0m        \u001b[36m0.4792\u001b[0m  0.0001  7.4803\n",
            "     16  \u001b[36m0.7246\u001b[0m       \u001b[32m0.9987\u001b[0m    \u001b[35m0.5686\u001b[0m        \u001b[31m0.6922\u001b[0m        \u001b[94m0.0148\u001b[0m        \u001b[36m0.4791\u001b[0m  0.0001  6.4916\n",
            "     17  \u001b[36m0.7254\u001b[0m       \u001b[32m0.9987\u001b[0m    \u001b[35m0.5696\u001b[0m        \u001b[31m0.6922\u001b[0m        \u001b[94m0.0147\u001b[0m        \u001b[36m0.4791\u001b[0m  0.0001  7.9430\n",
            "     18  \u001b[36m0.7260\u001b[0m       \u001b[32m0.9987\u001b[0m    \u001b[35m0.5702\u001b[0m        \u001b[31m0.6921\u001b[0m        \u001b[94m0.0146\u001b[0m        \u001b[36m0.4790\u001b[0m  0.0001  6.8789\n",
            "     19  \u001b[36m0.7268\u001b[0m       \u001b[32m0.9987\u001b[0m    \u001b[35m0.5712\u001b[0m        \u001b[31m0.6920\u001b[0m        \u001b[94m0.0145\u001b[0m        \u001b[36m0.4789\u001b[0m  0.0001  7.0906\n",
            "     20  \u001b[36m0.7273\u001b[0m       \u001b[32m0.9987\u001b[0m    \u001b[35m0.5719\u001b[0m        \u001b[31m0.6920\u001b[0m        \u001b[94m0.0144\u001b[0m        \u001b[36m0.4788\u001b[0m  0.0001  8.0226\n",
            "     21  \u001b[36m0.7278\u001b[0m       \u001b[32m0.9987\u001b[0m    \u001b[35m0.5725\u001b[0m        \u001b[31m0.6919\u001b[0m        \u001b[94m0.0143\u001b[0m        \u001b[36m0.4788\u001b[0m  0.0001  6.7759\n",
            "     22  \u001b[36m0.7279\u001b[0m       \u001b[32m0.9987\u001b[0m    \u001b[35m0.5727\u001b[0m        \u001b[31m0.6919\u001b[0m        \u001b[94m0.0142\u001b[0m        \u001b[36m0.4788\u001b[0m  0.0000  7.5350\n",
            "     23  \u001b[36m0.7281\u001b[0m       \u001b[32m0.9987\u001b[0m    \u001b[35m0.5729\u001b[0m        \u001b[31m0.6919\u001b[0m        \u001b[94m0.0142\u001b[0m        \u001b[36m0.4788\u001b[0m  0.0000  7.0125\n",
            "     24  0.7281       0.9987    0.5728        \u001b[31m0.6919\u001b[0m        \u001b[94m0.0142\u001b[0m        \u001b[36m0.4787\u001b[0m  0.0000  7.5221\n",
            "     25  0.7281       0.9987    0.5728        \u001b[31m0.6919\u001b[0m        \u001b[94m0.0142\u001b[0m        \u001b[36m0.4787\u001b[0m  0.0000  6.5551\n",
            "     26  0.7281       0.9987    0.5729        \u001b[31m0.6919\u001b[0m        \u001b[94m0.0142\u001b[0m        \u001b[36m0.4787\u001b[0m  0.0000  7.6224\n",
            "     27  0.7281       0.9987    0.5729        \u001b[31m0.6919\u001b[0m        \u001b[94m0.0142\u001b[0m        \u001b[36m0.4787\u001b[0m  0.0000  7.2014\n",
            "     28  0.7281       0.9987    0.5729        \u001b[31m0.6919\u001b[0m        \u001b[94m0.0141\u001b[0m        \u001b[36m0.4787\u001b[0m  0.0000  7.4360\n",
            "     29  0.7281       0.9987    0.5729        \u001b[31m0.6919\u001b[0m        \u001b[94m0.0141\u001b[0m        \u001b[36m0.4787\u001b[0m  0.0000  6.9085\n",
            "     30  0.7281       0.9987    0.5729        \u001b[31m0.6919\u001b[0m        \u001b[94m0.0141\u001b[0m        \u001b[36m0.4787\u001b[0m  0.0000  7.6074\n",
            "[CV 1/3; 1/5] END lr=0.01, module__size_emb=600;, score=-1.389 total time= 3.8min\n",
            "[CV 2/3; 1/5] START lr=0.01, module__size_emb=600...............................\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.5250\u001b[0m       \u001b[32m0.8412\u001b[0m    \u001b[35m0.3816\u001b[0m        \u001b[31m0.9830\u001b[0m        \u001b[94m1.2693\u001b[0m        \u001b[36m0.9663\u001b[0m  0.0100  7.6793\n",
            "      2  0.5151       \u001b[32m0.9646\u001b[0m    0.3514        \u001b[31m0.8216\u001b[0m        \u001b[94m0.6350\u001b[0m        \u001b[36m0.6751\u001b[0m  0.0100  6.6922\n",
            "      3  \u001b[36m0.6605\u001b[0m       \u001b[32m0.9887\u001b[0m    \u001b[35m0.4959\u001b[0m        \u001b[31m0.7348\u001b[0m        \u001b[94m0.2739\u001b[0m        \u001b[36m0.5399\u001b[0m  0.0100  8.0081\n",
            "      4  \u001b[36m0.6915\u001b[0m       \u001b[32m0.9958\u001b[0m    \u001b[35m0.5297\u001b[0m        \u001b[31m0.7064\u001b[0m        \u001b[94m0.1297\u001b[0m        \u001b[36m0.4991\u001b[0m  0.0100  7.1895\n",
            "      5  0.6810       \u001b[32m0.9972\u001b[0m    0.5171        \u001b[31m0.6953\u001b[0m        \u001b[94m0.0823\u001b[0m        \u001b[36m0.4835\u001b[0m  0.0100  6.9802\n",
            "      6  0.6662       \u001b[32m0.9977\u001b[0m    0.5001        \u001b[31m0.6909\u001b[0m        \u001b[94m0.0637\u001b[0m        \u001b[36m0.4773\u001b[0m  0.0100  8.0666\n",
            "      7  0.6587       0.9962    0.4921        \u001b[31m0.6892\u001b[0m        \u001b[94m0.0562\u001b[0m        \u001b[36m0.4749\u001b[0m  0.0100  6.5247\n",
            "      8  0.6652       0.9973    0.4990        \u001b[31m0.6855\u001b[0m        \u001b[94m0.0418\u001b[0m        \u001b[36m0.4699\u001b[0m  0.0010  7.5190\n",
            "      9  0.6689       \u001b[32m0.9977\u001b[0m    0.5032        \u001b[31m0.6814\u001b[0m        \u001b[94m0.0317\u001b[0m        \u001b[36m0.4644\u001b[0m  0.0010  7.0052\n",
            "     10  0.6730       \u001b[32m0.9979\u001b[0m    0.5077        \u001b[31m0.6793\u001b[0m        \u001b[94m0.0248\u001b[0m        \u001b[36m0.4614\u001b[0m  0.0010  7.6388\n",
            "     11  0.6809       \u001b[32m0.9981\u001b[0m    0.5167        \u001b[31m0.6781\u001b[0m        \u001b[94m0.0211\u001b[0m        \u001b[36m0.4598\u001b[0m  0.0010  6.7094\n",
            "     12  0.6889       \u001b[32m0.9983\u001b[0m    0.5259        \u001b[31m0.6771\u001b[0m        \u001b[94m0.0188\u001b[0m        \u001b[36m0.4585\u001b[0m  0.0010  7.4706\n",
            "     13  \u001b[36m0.6941\u001b[0m       \u001b[32m0.9986\u001b[0m    \u001b[35m0.5319\u001b[0m        \u001b[31m0.6764\u001b[0m        \u001b[94m0.0171\u001b[0m        \u001b[36m0.4576\u001b[0m  0.0010  7.0569\n",
            "     14  \u001b[36m0.6966\u001b[0m       \u001b[32m0.9990\u001b[0m    \u001b[35m0.5347\u001b[0m        \u001b[31m0.6758\u001b[0m        \u001b[94m0.0158\u001b[0m        \u001b[36m0.4568\u001b[0m  0.0010  7.3803\n",
            "     15  \u001b[36m0.6972\u001b[0m       \u001b[32m0.9990\u001b[0m    \u001b[35m0.5355\u001b[0m        \u001b[31m0.6758\u001b[0m        \u001b[94m0.0149\u001b[0m        \u001b[36m0.4567\u001b[0m  0.0001  7.0383\n",
            "     16  \u001b[36m0.6981\u001b[0m       \u001b[32m0.9990\u001b[0m    \u001b[35m0.5366\u001b[0m        \u001b[31m0.6757\u001b[0m        \u001b[94m0.0148\u001b[0m        \u001b[36m0.4566\u001b[0m  0.0001  7.4530\n",
            "     17  \u001b[36m0.6988\u001b[0m       \u001b[32m0.9990\u001b[0m    \u001b[35m0.5374\u001b[0m        \u001b[31m0.6757\u001b[0m        \u001b[94m0.0147\u001b[0m        \u001b[36m0.4565\u001b[0m  0.0001  7.5410\n",
            "     18  \u001b[36m0.6994\u001b[0m       \u001b[32m0.9990\u001b[0m    \u001b[35m0.5381\u001b[0m        \u001b[31m0.6756\u001b[0m        \u001b[94m0.0146\u001b[0m        \u001b[36m0.4564\u001b[0m  0.0001  6.4882\n",
            "     19  \u001b[36m0.6999\u001b[0m       \u001b[32m0.9992\u001b[0m    \u001b[35m0.5385\u001b[0m        \u001b[31m0.6756\u001b[0m        \u001b[94m0.0144\u001b[0m        \u001b[36m0.4564\u001b[0m  0.0001  7.3632\n",
            "     20  \u001b[36m0.7005\u001b[0m       \u001b[32m0.9992\u001b[0m    \u001b[35m0.5393\u001b[0m        \u001b[31m0.6755\u001b[0m        \u001b[94m0.0143\u001b[0m        \u001b[36m0.4563\u001b[0m  0.0001  7.0877\n",
            "     21  \u001b[36m0.7019\u001b[0m       \u001b[32m0.9993\u001b[0m    \u001b[35m0.5409\u001b[0m        \u001b[31m0.6754\u001b[0m        \u001b[94m0.0142\u001b[0m        \u001b[36m0.4562\u001b[0m  0.0001  7.7445\n",
            "     22  0.7018       \u001b[32m0.9995\u001b[0m    0.5408        \u001b[31m0.6754\u001b[0m        \u001b[94m0.0141\u001b[0m        \u001b[36m0.4562\u001b[0m  0.0000  6.5882\n",
            "     23  0.7018       0.9995    0.5407        \u001b[31m0.6754\u001b[0m        \u001b[94m0.0141\u001b[0m        \u001b[36m0.4562\u001b[0m  0.0000  7.6599\n",
            "     24  \u001b[36m0.7020\u001b[0m       \u001b[32m0.9995\u001b[0m    \u001b[35m0.5410\u001b[0m        \u001b[31m0.6754\u001b[0m        \u001b[94m0.0141\u001b[0m        \u001b[36m0.4562\u001b[0m  0.0000  7.1612\n",
            "     25  \u001b[36m0.7021\u001b[0m       \u001b[32m0.9995\u001b[0m    \u001b[35m0.5411\u001b[0m        \u001b[31m0.6754\u001b[0m        \u001b[94m0.0141\u001b[0m        \u001b[36m0.4562\u001b[0m  0.0000  7.5417\n",
            "     26  0.7020       0.9995    0.5410        \u001b[31m0.6754\u001b[0m        \u001b[94m0.0141\u001b[0m        \u001b[36m0.4562\u001b[0m  0.0000  6.8889\n",
            "     27  0.7021       0.9995    0.5411        \u001b[31m0.6754\u001b[0m        \u001b[94m0.0141\u001b[0m        \u001b[36m0.4562\u001b[0m  0.0000  7.3189\n",
            "     28  \u001b[36m0.7023\u001b[0m       \u001b[32m0.9995\u001b[0m    \u001b[35m0.5413\u001b[0m        \u001b[31m0.6754\u001b[0m        \u001b[94m0.0141\u001b[0m        \u001b[36m0.4562\u001b[0m  0.0000  7.5831\n",
            "     29  0.7023       0.9995    0.5413        \u001b[31m0.6754\u001b[0m        \u001b[94m0.0141\u001b[0m        \u001b[36m0.4562\u001b[0m  0.0000  6.7669\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 2/3; 1/5] END lr=0.01, module__size_emb=600;, score=-1.334 total time= 3.7min\n",
            "[CV 3/3; 1/5] START lr=0.01, module__size_emb=600...............................\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.5326\u001b[0m       \u001b[32m0.8353\u001b[0m    \u001b[35m0.3909\u001b[0m        \u001b[31m0.9872\u001b[0m        \u001b[94m1.2401\u001b[0m        \u001b[36m0.9746\u001b[0m  0.0100  7.3436\n",
            "      2  0.5088       \u001b[32m0.9595\u001b[0m    0.3462        \u001b[31m0.8313\u001b[0m        \u001b[94m0.6279\u001b[0m        \u001b[36m0.6910\u001b[0m  0.0100  6.9966\n",
            "      3  \u001b[36m0.6487\u001b[0m       \u001b[32m0.9867\u001b[0m    \u001b[35m0.4831\u001b[0m        \u001b[31m0.7443\u001b[0m        \u001b[94m0.2839\u001b[0m        \u001b[36m0.5540\u001b[0m  0.0100  7.3738\n",
            "      4  \u001b[36m0.6862\u001b[0m       \u001b[32m0.9962\u001b[0m    \u001b[35m0.5233\u001b[0m        \u001b[31m0.7140\u001b[0m        \u001b[94m0.1366\u001b[0m        \u001b[36m0.5098\u001b[0m  0.0100  7.2319\n",
            "      5  0.6630       0.9955    0.4970        \u001b[31m0.7034\u001b[0m        \u001b[94m0.0856\u001b[0m        \u001b[36m0.4947\u001b[0m  0.0100  7.6601\n",
            "      6  0.6680       0.9955    0.5026        \u001b[31m0.6985\u001b[0m        \u001b[94m0.0651\u001b[0m        \u001b[36m0.4879\u001b[0m  0.0100  6.6187\n",
            "      7  0.6550       0.9956    0.4881        \u001b[31m0.6962\u001b[0m        \u001b[94m0.0562\u001b[0m        \u001b[36m0.4847\u001b[0m  0.0100  7.9908\n",
            "      8  0.6639       \u001b[32m0.9964\u001b[0m    0.4978        \u001b[31m0.6926\u001b[0m        \u001b[94m0.0416\u001b[0m        \u001b[36m0.4798\u001b[0m  0.0010  6.4492\n",
            "      9  0.6728       \u001b[32m0.9972\u001b[0m    0.5077        \u001b[31m0.6888\u001b[0m        \u001b[94m0.0318\u001b[0m        \u001b[36m0.4744\u001b[0m  0.0010  7.4477\n",
            "     10  0.6761       \u001b[32m0.9972\u001b[0m    0.5114        \u001b[31m0.6868\u001b[0m        \u001b[94m0.0254\u001b[0m        \u001b[36m0.4718\u001b[0m  0.0010  6.9806\n",
            "     11  0.6804       \u001b[32m0.9974\u001b[0m    0.5163        \u001b[31m0.6856\u001b[0m        \u001b[94m0.0220\u001b[0m        \u001b[36m0.4701\u001b[0m  0.0010  7.5450\n",
            "     12  0.6837       \u001b[32m0.9974\u001b[0m    0.5201        \u001b[31m0.6848\u001b[0m        \u001b[94m0.0198\u001b[0m        \u001b[36m0.4690\u001b[0m  0.0010  6.7910\n",
            "     13  \u001b[36m0.6877\u001b[0m       \u001b[32m0.9974\u001b[0m    \u001b[35m0.5247\u001b[0m        \u001b[31m0.6841\u001b[0m        \u001b[94m0.0182\u001b[0m        \u001b[36m0.4680\u001b[0m  0.0010  7.2021\n",
            "     14  \u001b[36m0.6917\u001b[0m       \u001b[32m0.9980\u001b[0m    \u001b[35m0.5292\u001b[0m        \u001b[31m0.6835\u001b[0m        \u001b[94m0.0169\u001b[0m        \u001b[36m0.4672\u001b[0m  0.0010  7.8540\n",
            "     15  \u001b[36m0.6924\u001b[0m       \u001b[32m0.9980\u001b[0m    \u001b[35m0.5301\u001b[0m        \u001b[31m0.6834\u001b[0m        \u001b[94m0.0159\u001b[0m        \u001b[36m0.4671\u001b[0m  0.0001  6.8114\n",
            "     16  \u001b[36m0.6928\u001b[0m       \u001b[32m0.9980\u001b[0m    \u001b[35m0.5306\u001b[0m        \u001b[31m0.6834\u001b[0m        \u001b[94m0.0158\u001b[0m        \u001b[36m0.4670\u001b[0m  0.0001  7.7108\n",
            "     17  \u001b[36m0.6938\u001b[0m       \u001b[32m0.9980\u001b[0m    \u001b[35m0.5317\u001b[0m        \u001b[31m0.6833\u001b[0m        \u001b[94m0.0157\u001b[0m        \u001b[36m0.4669\u001b[0m  0.0001  7.1281\n",
            "     18  0.6937       0.9980    0.5316        \u001b[31m0.6833\u001b[0m        \u001b[94m0.0156\u001b[0m        \u001b[36m0.4668\u001b[0m  0.0001  7.6008\n",
            "     19  \u001b[36m0.6941\u001b[0m       \u001b[32m0.9980\u001b[0m    \u001b[35m0.5321\u001b[0m        \u001b[31m0.6832\u001b[0m        \u001b[94m0.0155\u001b[0m        \u001b[36m0.4668\u001b[0m  0.0001  6.6514\n",
            "     20  \u001b[36m0.6948\u001b[0m       \u001b[32m0.9980\u001b[0m    \u001b[35m0.5329\u001b[0m        \u001b[31m0.6831\u001b[0m        \u001b[94m0.0154\u001b[0m        \u001b[36m0.4667\u001b[0m  0.0001  8.1266\n",
            "     21  \u001b[36m0.6954\u001b[0m       \u001b[32m0.9980\u001b[0m    \u001b[35m0.5337\u001b[0m        \u001b[31m0.6831\u001b[0m        \u001b[94m0.0153\u001b[0m        \u001b[36m0.4666\u001b[0m  0.0001  6.7385\n",
            "     22  \u001b[36m0.6955\u001b[0m       \u001b[32m0.9980\u001b[0m    \u001b[35m0.5338\u001b[0m        \u001b[31m0.6831\u001b[0m        \u001b[94m0.0152\u001b[0m        \u001b[36m0.4666\u001b[0m  0.0000  7.5205\n",
            "     23  \u001b[36m0.6956\u001b[0m       \u001b[32m0.9980\u001b[0m    \u001b[35m0.5338\u001b[0m        \u001b[31m0.6831\u001b[0m        \u001b[94m0.0152\u001b[0m        \u001b[36m0.4666\u001b[0m  0.0000  7.3463\n",
            "     24  \u001b[36m0.6958\u001b[0m       \u001b[32m0.9980\u001b[0m    \u001b[35m0.5341\u001b[0m        \u001b[31m0.6831\u001b[0m        \u001b[94m0.0151\u001b[0m        \u001b[36m0.4666\u001b[0m  0.0000  7.3931\n",
            "     25  0.6958       0.9980    0.5341        \u001b[31m0.6831\u001b[0m        \u001b[94m0.0151\u001b[0m        \u001b[36m0.4666\u001b[0m  0.0000  7.6409\n",
            "     26  \u001b[36m0.6959\u001b[0m       \u001b[32m0.9980\u001b[0m    \u001b[35m0.5342\u001b[0m        \u001b[31m0.6831\u001b[0m        \u001b[94m0.0151\u001b[0m        \u001b[36m0.4666\u001b[0m  0.0000  6.7994\n",
            "     27  0.6959       0.9980    0.5342        \u001b[31m0.6830\u001b[0m        \u001b[94m0.0151\u001b[0m        \u001b[36m0.4665\u001b[0m  0.0000  7.6663\n",
            "     28  0.6959       0.9980    0.5342        \u001b[31m0.6830\u001b[0m        \u001b[94m0.0151\u001b[0m        \u001b[36m0.4665\u001b[0m  0.0000  6.8814\n",
            "     29  0.6959       0.9980    0.5342        \u001b[31m0.6830\u001b[0m        \u001b[94m0.0151\u001b[0m        \u001b[36m0.4665\u001b[0m  0.0000  7.5019\n",
            "Stopping since valid_loss has not improved in the last 10 epochs.\n",
            "[CV 3/3; 1/5] END lr=0.01, module__size_emb=600;, score=-1.385 total time= 3.7min\n",
            "[CV 1/3; 2/5] START lr=0.01, module__size_emb=700...............................\n",
            "  epoch      f1    precision    recall    rmse_score    train_loss    valid_loss      lr     dur\n",
            "-------  ------  -----------  --------  ------------  ------------  ------------  ------  ------\n",
            "      1  \u001b[36m0.5645\u001b[0m       \u001b[32m0.8332\u001b[0m    \u001b[35m0.4269\u001b[0m        \u001b[31m0.9907\u001b[0m        \u001b[94m1.2330\u001b[0m        \u001b[36m0.9815\u001b[0m  0.0100  7.3106\n",
            "      2  0.5492       \u001b[32m0.9709\u001b[0m    0.3829        \u001b[31m0.8249\u001b[0m        \u001b[94m0.6108\u001b[0m        \u001b[36m0.6804\u001b[0m  0.0100  12.4789\n",
            "      3  \u001b[36m0.6999\u001b[0m       \u001b[32m0.9912\u001b[0m    \u001b[35m0.5410\u001b[0m        \u001b[31m0.7454\u001b[0m        \u001b[94m0.2545\u001b[0m        \u001b[36m0.5556\u001b[0m  0.0100  13.3047\n",
            "      4  \u001b[36m0.7180\u001b[0m       \u001b[32m0.9952\u001b[0m    \u001b[35m0.5616\u001b[0m        \u001b[31m0.7227\u001b[0m        \u001b[94m0.1255\u001b[0m        \u001b[36m0.5223\u001b[0m  0.0100  9.5619\n",
            "      5  0.6949       \u001b[32m0.9970\u001b[0m    0.5333        \u001b[31m0.7132\u001b[0m        \u001b[94m0.0854\u001b[0m        \u001b[36m0.5086\u001b[0m  0.0100  13.1003\n",
            "      6  0.6817       0.9962    0.5181        \u001b[31m0.7096\u001b[0m        \u001b[94m0.0708\u001b[0m        \u001b[36m0.5036\u001b[0m  0.0100  9.4424\n",
            "      7  0.6840       0.9964    0.5208        \u001b[31m0.7086\u001b[0m        \u001b[94m0.0649\u001b[0m        \u001b[36m0.5022\u001b[0m  0.0100  7.7976\n",
            "      8  0.6873       \u001b[32m0.9976\u001b[0m    0.5242        \u001b[31m0.7038\u001b[0m        \u001b[94m0.0496\u001b[0m        \u001b[36m0.4953\u001b[0m  0.0010  14.0289\n",
            "      9  0.6923       \u001b[32m0.9986\u001b[0m    0.5298        \u001b[31m0.6985\u001b[0m        \u001b[94m0.0362\u001b[0m        \u001b[36m0.4878\u001b[0m  0.0010  13.7833\n",
            "     10  0.6971       \u001b[32m0.9990\u001b[0m    0.5353        \u001b[31m0.6957\u001b[0m        \u001b[94m0.0273\u001b[0m        \u001b[36m0.4841\u001b[0m  0.0010  11.8880\n",
            "     11  0.7018       \u001b[32m0.9993\u001b[0m    0.5408        \u001b[31m0.6943\u001b[0m        \u001b[94m0.0228\u001b[0m        \u001b[36m0.4820\u001b[0m  0.0010  6.9435\n",
            "     12  0.7070       \u001b[32m0.9993\u001b[0m    0.5469        \u001b[31m0.6933\u001b[0m        \u001b[94m0.0201\u001b[0m        \u001b[36m0.4806\u001b[0m  0.0010  8.4791\n",
            "     13  0.7143       \u001b[32m0.9995\u001b[0m    0.5557        \u001b[31m0.6925\u001b[0m        \u001b[94m0.0182\u001b[0m        \u001b[36m0.4796\u001b[0m  0.0010  9.8865\n",
            "     14  \u001b[36m0.7206\u001b[0m       \u001b[32m0.9998\u001b[0m    \u001b[35m0.5633\u001b[0m        \u001b[31m0.6919\u001b[0m        \u001b[94m0.0167\u001b[0m        \u001b[36m0.4788\u001b[0m  0.0010  10.2059\n",
            "     15  \u001b[36m0.7209\u001b[0m       \u001b[32m0.9998\u001b[0m    \u001b[35m0.5636\u001b[0m        \u001b[31m0.6919\u001b[0m        \u001b[94m0.0157\u001b[0m        \u001b[36m0.4787\u001b[0m  0.0001  8.6646\n",
            "     16  \u001b[36m0.7218\u001b[0m       \u001b[32m0.9998\u001b[0m    \u001b[35m0.5647\u001b[0m        \u001b[31m0.6918\u001b[0m        \u001b[94m0.0156\u001b[0m        \u001b[36m0.4786\u001b[0m  0.0001  10.6003\n",
            "     17  \u001b[36m0.7221\u001b[0m       \u001b[32m0.9998\u001b[0m    \u001b[35m0.5652\u001b[0m        \u001b[31m0.6918\u001b[0m        \u001b[94m0.0155\u001b[0m        \u001b[36m0.4785\u001b[0m  0.0001  6.5638\n",
            "     18  \u001b[36m0.7223\u001b[0m       \u001b[32m0.9998\u001b[0m    \u001b[35m0.5653\u001b[0m        \u001b[31m0.6917\u001b[0m        \u001b[94m0.0154\u001b[0m        \u001b[36m0.4785\u001b[0m  0.0001  7.4118\n",
            "     19  \u001b[36m0.7227\u001b[0m       \u001b[32m0.9998\u001b[0m    \u001b[35m0.5658\u001b[0m        \u001b[31m0.6917\u001b[0m        \u001b[94m0.0152\u001b[0m        \u001b[36m0.4784\u001b[0m  0.0001  6.9699\n",
            "     20  \u001b[36m0.7235\u001b[0m       \u001b[32m0.9998\u001b[0m    \u001b[35m0.5668\u001b[0m        \u001b[31m0.6916\u001b[0m        \u001b[94m0.0151\u001b[0m        \u001b[36m0.4783\u001b[0m  0.0001  7.5174\n",
            "     21  \u001b[36m0.7236\u001b[0m       \u001b[32m0.9998\u001b[0m    \u001b[35m0.5670\u001b[0m        \u001b[31m0.6915\u001b[0m        \u001b[94m0.0150\u001b[0m        \u001b[36m0.4782\u001b[0m  0.0001  6.6173\n",
            "     22  \u001b[36m0.7239\u001b[0m       \u001b[32m0.9998\u001b[0m    \u001b[35m0.5673\u001b[0m        \u001b[31m0.6915\u001b[0m        \u001b[94m0.0149\u001b[0m        \u001b[36m0.4782\u001b[0m  0.0000  8.5877\n",
            "     23  0.7238       0.9998    0.5672        \u001b[31m0.6915\u001b[0m        \u001b[94m0.0149\u001b[0m        \u001b[36m0.4782\u001b[0m  0.0000  8.2572\n",
            "     24  \u001b[36m0.7240\u001b[0m       \u001b[32m0.9998\u001b[0m    \u001b[35m0.5674\u001b[0m        \u001b[31m0.6915\u001b[0m        \u001b[94m0.0149\u001b[0m        \u001b[36m0.4782\u001b[0m  0.0000  9.4430\n",
            "     25  \u001b[36m0.7240\u001b[0m       \u001b[32m0.9998\u001b[0m    \u001b[35m0.5675\u001b[0m        \u001b[31m0.6915\u001b[0m        \u001b[94m0.0149\u001b[0m        \u001b[36m0.4782\u001b[0m  0.0000  18.6290\n"
          ]
        }
      ],
      "source": [
        "params = {\n",
        "    'lr': [0.01],\n",
        "    'module__size_emb': [600, 700, 800, 900, 1000]\n",
        "}\n",
        "gs = GridSearchCV(twoembedsnet,\n",
        "                  params,\n",
        "                  verbose=50,\n",
        "                  refit=False,\n",
        "                  #pre_dispatch=8,\n",
        "                  #n_jobs=8,\n",
        "                  cv=3,\n",
        "                  scoring='neg_mean_squared_error')\n",
        "\n",
        "X_ds = SliceDataset(train, idx=0)\n",
        "y_ds = SliceDataset(train, idx=1)\n",
        "gs.fit(X_ds, y_ds)\n",
        "\n",
        "print(gs.best_score_, gs.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_WP8cIqZQWN"
      },
      "source": [
        "### Benchmark with scikit-surprise SVD algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pjhfBV6ZRUH",
        "outputId": "c6f0e9d6-6e55-4bda-e320-3353699467c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting surprise\n",
            "  Downloading surprise-0.1-py2.py3-none-any.whl (1.8 kB)\n",
            "Collecting scikit-surprise (from surprise)\n",
            "  Downloading scikit-surprise-1.1.3.tar.gz (771 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/772.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/772.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m772.0/772.0 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise->surprise) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise->surprise) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise->surprise) (1.11.4)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.3-cp310-cp310-linux_x86_64.whl size=3163765 sha256=d9f3d8dac7c135ab0e4772500fbfaa880c694df7ce5e0e381c317e20daad7ff2\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/ca/a8/4e28def53797fdc4363ca4af740db15a9c2f1595ebc51fb445\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise, surprise\n",
            "Successfully installed scikit-surprise-1.1.3 surprise-0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install surprise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsJ69IyMZTcZ"
      },
      "outputs": [],
      "source": [
        "from surprise import NormalPredictor\n",
        "from surprise import SVD\n",
        "from surprise import Dataset\n",
        "from surprise import Reader\n",
        "from surprise import accuracy\n",
        "from surprise.model_selection import cross_validate, train_test_split, KFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k34DMC7XNgJI",
        "outputId": "c613abc9-72ef-4c89-ba16-67f3268da287"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([4., 4., 4.,  ..., 2., 2., 2.])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train[dataloaders['train'].dataset.indices][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UG7J_vXTZjcf"
      },
      "outputs": [],
      "source": [
        "user = train[dataloaders['train'].dataset.indices][0][0][:, 0].data.numpy()\n",
        "movie = train[dataloaders['train'].dataset.indices][0][2][:, 0].data.numpy()\n",
        "y = train[dataloaders['train'].dataset.indices][1].data.numpy()\n",
        "df = pd.DataFrame({'user': user, 'movie': movie, 'y': y})\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "data = Dataset.load_from_df(df[['user', 'movie', 'y']], reader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Khqq2oRQwyZ3"
      },
      "outputs": [],
      "source": [
        "data = Dataset.load_from_df(train.ratings.loc[dataloaders['train'].dataset.indices, ['UserID', 'MovieID', 'Rating']], reader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZULF4SZexU81"
      },
      "outputs": [],
      "source": [
        "a = train.ratings.loc[dataloaders['train'].dataset.indices, ['UserID', 'MovieID', 'Rating']]\n",
        "b = pd.DataFrame({'UserID': user, 'MovieID': movie, 'Rating': y})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePIcLN-Vawop",
        "outputId": "996ea4b1-d4d7-40e5-b21a-d41ed4723cf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RMSE: 0.9532\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9532214288290356"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainset, testset = train_test_split(data, test_size=.25)\n",
        "\n",
        "algo = SVD()\n",
        "\n",
        "algo.fit(trainset)\n",
        "predictions = algo.test(testset)\n",
        "\n",
        "#RMSE\n",
        "accuracy.rmse(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHY401zJgx8I"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
